<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KO">
<meta name="dcterms.date" content="2017-01-01">
<meta name="description" content="my notes on part 2 of the deeplearning.ai course">

<title>khalido.org - deeplearning.ai: Improving Deep Neural Networks</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">khalido.org</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/khalido" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/KO" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">deeplearning.ai: Improving Deep Neural Networks</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          my notes on part 2 of the deeplearning.ai course
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">courses</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>KO </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">January 1, 2017</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#deeplearning.ai-improving-deep-neural-networks" id="toc-deeplearning.ai-improving-deep-neural-networks" class="nav-link active" data-scroll-target="#deeplearning.ai-improving-deep-neural-networks">deeplearning.ai: Improving Deep Neural Networks</a>
  <ul class="collapse">
  <li><a href="#week-1-practical-aspects-of-deep-learning" id="toc-week-1-practical-aspects-of-deep-learning" class="nav-link" data-scroll-target="#week-1-practical-aspects-of-deep-learning">Week 1: Practical aspects of Deep Learning</a></li>
  <li><a href="#week-2optimizatoni-algorithims" id="toc-week-2optimizatoni-algorithims" class="nav-link" data-scroll-target="#week-2optimizatoni-algorithims">Week 2:Optimizatoni algorithims</a></li>
  <li><a href="#week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks" id="toc-week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks" class="nav-link" data-scroll-target="#week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks">Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks</a>
  <ul class="collapse">
  <li><a href="#hyperparameter-tuning" id="toc-hyperparameter-tuning" class="nav-link" data-scroll-target="#hyperparameter-tuning">Hyperparameter tuning</a></li>
  <li><a href="#batch-norm" id="toc-batch-norm" class="nav-link" data-scroll-target="#batch-norm">Batch Norm</a></li>
  <li><a href="#multi-class-classification" id="toc-multi-class-classification" class="nav-link" data-scroll-target="#multi-class-classification">Multi-class Classification</a></li>
  <li><a href="#intro-to-programming-frameworks" id="toc-intro-to-programming-frameworks" class="nav-link" data-scroll-target="#intro-to-programming-frameworks">Intro to programming frameworks</a></li>
  <li><a href="#yuanqing-lin-interview" id="toc-yuanqing-lin-interview" class="nav-link" data-scroll-target="#yuanqing-lin-interview">Yuanqing Lin interview</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="deeplearning.ai-improving-deep-neural-networks" class="level1">
<h1>deeplearning.ai: Improving Deep Neural Networks</h1>
<p><a href="https://www.coursera.org/learn/deep-neural-network/home/welcome">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</a></p>
<p><strong>Course Resources</strong></p>
<ul>
<li><a href="https://www.coursera.org/learn/deep-neural-network/discussions">Discussion forum</a></li>
<li><a href="https://www.youtube.com/watch?v=1waHlpKiNyY&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc">YouTube playlist for Course 1</a></li>
<li>course reviews: <a href="https://towardsdatascience.com/review-of-deeplearning-ai-courses-aed1328e4ffe">1</a></li>
</ul>
<section id="week-1-practical-aspects-of-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="week-1-practical-aspects-of-deep-learning">Week 1: Practical aspects of Deep Learning</h2>
<p>Train / Dev / Test sets</p>
<ul>
<li>training is a highly iterative process as you test ideas and parameters</li>
<li>even highly skilled practicioners find it impossible to predict the best parameters for a problem, so going through</li>
<li>split data into train/dev/test sets
<ul>
<li>training set - the actual dataset used to train the model</li>
<li>cross validation or dev set - used to evaluate the model</li>
<li>test set - only used once the model is finally trained to get a unbiased estimate of the models performance</li>
</ul></li>
<li>depending on the dataset size, use different splits, eg:
<ul>
<li>100 to 1M ==&gt; 60/20/20</li>
<li>1M to INF ==&gt; 98/1/1 or 99.5/0.25/0.25</li>
</ul></li>
<li>you can mismatch train/test distribution, so make sure they come from the same distribution</li>
</ul>
<p>Bias / Variance</p>
<p><img src="../../img/bias-variance.png" class="img-fluid"></p>
<ul>
<li>high bias - model is under fitting, or not even fitting the training set</li>
<li>high variance - model is over fitting the training and dev set</li>
<li>you can have both high bias AND high variance</li>
<li>balance the bias and variance, and also consider the underlying problems tractibility, as are we aiming for 99% accuracy or 70%? Compare with other models and human performance to get a sense of a baseline error</li>
</ul>
<p>Basic recipe for machine learning</p>
<ul>
<li>for high bias errors: bigger NN,more layers, different model, try different activations</li>
<li>high variance: more data, regularize appropirately, try a different model</li>
<li>training a bigger NN almost never hurts</li>
</ul>
<p>Regularization</p>
<p><img src="https://upload.wikimedia.org/wikipedia/commons/b/b8/Sparsityl1.png?1531465205841" class="img-fluid"></p>
<ul>
<li>vectors often have many dimensions, so model sizes get BIG. regulariation helps to select features and reduce dimensions, as well as reduce overfitting</li>
<li>L1 regularization (or Lasso) adds a penalty equal to the sum of absoulte coefficients</li>
<li>L2 regularization (or Ridge) adds a penalty equal to the sum of squared coefficients</li>
</ul>
<blockquote class="blockquote">
<p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.</p>
</blockquote>
<ul>
<li>as a rule of thumb, L2 almost always works better than L1 .</li>
</ul>
<p>Dropout regularization</p>
<ul>
<li>for each iteration, use a probablity to determine whether to “drop” a neuron. So each iteration through the NN drops a different, random set of neurons.</li>
<li>each layer in the NN can have a different dropout probability, the downside is that we have more hyperparameters to tweak</li>
<li>in computer vision, we almost always use dropout becuase we are almost always overfitting in vision problems</li>
<li>a downside of dropout is that the cost function is no longer well defined, so turn off dropout to see that the loss is dropping, which implies that our model is working, then turn on dropout for better training</li>
<li>remove dropout at test time</li>
<li>dropout intuitions:
<ul>
<li>can’t rely on on any given neuron, so have to spread out weights</li>
<li>can work similar to L2 regularization</li>
<li>helps prevent overfitting</li>
</ul></li>
</ul>
<p>Data augmentation</p>
<ul>
<li>inexpensive way to get more data - e.g with images flip, distory, crop, rotate etc to get more images</li>
<li>this helps as a regularization technique</li>
<li></li>
</ul>
<p>Early stopping</p>
<ul>
<li>stop training the NN when the dev set and training set error start diverging - get the huperparameters from the lowest dev and training set cost.</li>
<li>this prevents the NN from overfitting on the training set but stops gradient descent early</li>
<li>Andrew NG generally prefers using L2 regularization instead of early stopping as that</li>
</ul>
<p>Deep NN suffer from vanishing and exploding gradients</p>
<ul>
<li>this happens when gradients become very small or big</li>
<li>in a deep NN if activations are linear, than the activations and derivates will increase exponentially with layers</li>
</ul>
<p>Weight Initialization for Deep Networks</p>
<ul>
<li>this is a paritial solution to vanishing/exploding gradients</li>
<li>initialize the weights with a variance equal to 1/n - where n is the number of input features - sure weights are not too small or not to large</li>
</ul>
<p>Numerical approximation of gradients</p>
<p>Gradient Checking</p>
<ul>
<li>check our gradient computation functions - this helps debug backprop</li>
<li></li>
</ul>
</section>
<section id="week-2optimizatoni-algorithims" class="level2">
<h2 class="anchored" data-anchor-id="week-2optimizatoni-algorithims">Week 2:Optimizatoni algorithims</h2>
<p>Mini-batch gradient descent</p>
<ul>
<li>training big data is slow - breaking it up into smaller batches speeds things up</li>
<li>vectorization allows us to put our entire data set of m examples into a huge matrix and process it all in one go</li>
<li>but this way we have to process the entire set before our gradient descent can make a small step in the right direction</li>
<li>training on mini batches allows gradient descent to work much faster, as in one iteration over the dataset we would have taken m / batch_size gradient descent steps
<ul>
<li>makes the leaning more ‘noisy’ since each mini-batch is new data (compared to going over the entire dataset)</li>
</ul></li>
<li>a typical mini-batch size is 64, 128, 256, 512
<ul>
<li>should be a power of 2 as thats how computer memory is setup</li>
<li>make sure the batch fits inside cpu/gpu memory</li>
</ul></li>
<li></li>
</ul>
<p>Exponentially weighted averages</p>
<ul>
<li>also called exponentially weighted moving averages in statistics</li>
<li>this decreases the weight of older data exponentially, enhancing the effect of more recent numbers which makes it easy to spot new trends. Of course the parameters can be tweaked, like changing beta depending on how many data points to average<code>(1 / (1 - beta))</code>.</li>
<li>key component of several optimization algos</li>
</ul>
<p><code>V(t) = beta * v(t-1) + (1-beta) * theta(t)</code></p>
<p>bias correction in exponentially weighted averages:</p>
<ul>
<li>the moving avg in the beginning is low since there isn’t past data to refer from and it starts from zero.</li>
<li>so add a bias term, dividing the above equation by <code>(1 - beta^t)</code>:</li>
</ul>
<p><code>v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)</code></p>
<p>Gradient descent with momentum</p>
<ul>
<li>modify gradient descent so on each iteration compute the exponential weighted averages of the gradients and update weights</li>
<li>this takes us faster to the min point, and dampens out oscillations</li>
<li>most common value of beta is <code>0.9</code>. generally we don’t bother with bias correction since we do so many iterations</li>
<li><a href="https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d">this explains why momentum works</a>:</li>
</ul>
<blockquote class="blockquote">
<p>With Stochastic Gradient Descent we don’t compute the exact derivate of our loss function. Instead, we’re estimating it on a small batch. Which means we’re not always going in the optimal direction, because our derivatives are ‘noisy’. Just like in my graphs above. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations.</p>
</blockquote>
<ul>
<li>also see <a href="http://cs231n.github.io/neural-networks-3/#sgd">Nesterov Momentum</a></li>
</ul>
<p>RMSprop or Root mean square prop</p>
<ul>
<li>we want learning to go fast horizontally and slower vertically - so we divide updates in the vertical direction by a large number and updates in the horizontal direction by a much smaller number</li>
<li>this dampens oscillations, so we can use a faster learning rate</li>
<li>first proposed in Hinton’s coursera course</li>
</ul>
<p>Adam optimization algorithim</p>
<ul>
<li>mashes together momemtum and RMSprop, works very well</li>
<li>parameters:
<ul>
<li>learning rate alpha</li>
<li>beta1: moving avg or momemtum parameter, same as 0.9 above</li>
<li>beta2: RMSprop, <code>0.999</code> works well</li>
<li>epsilon <code>10^8</code> - less important, can leave it at default</li>
<li>generally leave values at default, just try out different learning rates</li>
</ul></li>
</ul>
<p>Learning rate decay</p>
<ul>
<li>slowly decrease learning rate over epochs</li>
<li>this makes intuitive sense as in the beginning bigger steps are ok and as the NN starts converging, we need smaller steps</li>
<li>other methods: exponential decay, discrete steps, etc</li>
<li>manual decay - watch the model as it trains, pause and manually change the learning rate - works if running a small number of models which take a long time to train</li>
<li>this is lower down on the list of things to try when tuning</li>
</ul>
<p>The problem of local optima</p>
<ul>
<li>people used to worry a lot about NN getting in local optima, but in multi-dimensional space most points of zero gradient are saddle points so its very unlikely to get stuck in a local optima</li>
<li>a lot of our intuitions about low dimensional spaces don’t transfer over the high dimensional space practically all NN’s use - i.e if we have 20K parameters, the NN is operating in a 20K dimensional space</li>
<li>but plateaus can slow down learning, so techniques like momentum, Adam help here</li>
</ul>
</section>
<section id="week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks" class="level2">
<h2 class="anchored" data-anchor-id="week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks">Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks</h2>
<section id="hyperparameter-tuning" class="level3">
<h3 class="anchored" data-anchor-id="hyperparameter-tuning">Hyperparameter tuning</h3>
<p>The Tuning Process</p>
<ul>
<li>there are tons of hyperparameters to tune, but some are more important than others</li>
<li><strong>alpha</strong> or the learning rate is the most important parameter most the time</li>
<li>then the second most important:
<ul>
<li><strong>momentum</strong> (0.9 being a good default,</li>
<li><strong>mini-batch size</strong></li>
<li><strong>hidden units</strong></li>
</ul></li>
<li>third in important:
<ul>
<li>num of layers -learning rate decay</li>
</ul></li>
<li>when using Adam, you pretty much never have to tune beta1, beta2 and epsilon</li>
<li>of course, this depends on the NN, dataset etc</li>
<li>don’t use a grid of one val vs the other - this works when you have a small number of hyperparameters, but in Deep Learning choose hyperparameter combinations at random</li>
<li>use coarse to fine sampling - find the range where a parameter is working, then do finer grained sampling to get the best val</li>
</ul>
<p>Using an appropriate scale to pick hyperparameters</p>
<ul>
<li>pick the appropriate scale for hyperparameters, generally better to use log scale rather than linear</li>
</ul>
<p>Hyperparameters tuning in practice: Pandas vs.&nbsp;Caviar</p>
<ul>
<li>intuitions about hyperparameters often don’t transfer to other domains e.g logistcs, nlpo, vision, speech will all have different best parameters</li>
<li>two major ways to find the best parameters:
<ul>
<li><strong>babysit one model</strong> - as its training, tweak the parameters and see how its doing on metrics. This is helpful when we don’t have enough computation capcity (panda approach)</li>
<li><strong>train many models in parallel</strong> - run many models in parallel, with different parameters (caviar approach)</li>
</ul></li>
</ul>
</section>
<section id="batch-norm" class="level3">
<h3 class="anchored" data-anchor-id="batch-norm">Batch Norm</h3>
<p>Normalizing activations in a network</p>
<ul>
<li>instead of just normalizing the inputs to a NN, we also normalize the outputs of each layer of a NN - this is called batch norm.</li>
<li>using the standard <code>(intermediate val - mean) / (std dev + epsilon)</code>. Epsilon is needed for numerical stability if variance is zero</li>
<li>batch norm sets the hidden layer to have mean 0 and variance 1, but sometimes we might want it to have a different distribution, so we can add a parameter beta to the hidden units to change the shape of the distribution. (like we might want a larger variance to take advantage of the nonlinearity of the sigmoid function).</li>
<li>batch norm makes the NN more robust and speeds up learning</li>
<li>while batch norm can be applied before or after the activation function, in practice it is generally applied before the activation funciton.</li>
</ul>
<p>Fitting Batch Normalization into a neural network</p>
<ul>
<li>Deep learning frameworks have batch norm built in, like <a href="https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization">tensorflow</a>, <a href="https://keras.io/layers/normalization/">keras</a> etc.</li>
<li>implementing this ourself is straightforward, as we add in a norm step just before applying the activation function at each layer in the NN.</li>
<li>In each mini-batch, for each hidden layer compute the mean and the variance in that mini-batch, normalize, then apply the activation function as before.</li>
</ul>
<p>Why does Batch normalization work?</p>
<p>Three main reaons:</p>
<ul>
<li>1: normalizing input features speeds up learning, so one intuition is that this is doing a similar thing for each hidden layer</li>
<li>2: makes weights in deeper layers more robust to changes in weights in earlier layers
<ul>
<li>for example, we train a network on black cats, and we try to classify a coloured cat. Every input is shifted, but the decision boundaries haven’t changed. This has a fancy name, coviariate shift.</li>
<li>allows each layer to learn more independently</li>
</ul></li>
<li>3: regularization of hidden units
<ul>
<li>adds some noise to each mini-batch, as each one is regularized on its own mean/variance, which has a slight regularization effect (bigger batches reduce noise/regularization)</li>
<li>but don’t rely on batch norm for regularization, as its just a unintended side effect, use other techniques like L2 or dropout</li>
</ul></li>
</ul>
<p>Batch normalization at test time</p>
<ul>
<li>we often predict one example at a time at test time, so the idea of a mean/variance doesn’t apply - we no longer have a mini-batch to process</li>
<li>we calculate the exponentially weighted average across all the mini-batches and use that to get a mean/variance to apply at test time on the one test example</li>
</ul>
</section>
<section id="multi-class-classification" class="level3">
<h3 class="anchored" data-anchor-id="multi-class-classification">Multi-class Classification</h3>
<p>Softmax regresssion</p>
<ul>
<li>is generalization of logistic regression which can predict multiple classes rather than just a binary.</li>
</ul>
<p>Training a Softmax classifier</p>
<ul>
<li>a hard max would look at a vector and put 1 for the max and zero for everything else</li>
<li>soft max takes in a vector and puts in a probability for each value such that they all sum up to 1</li>
<li>loss function is trying to make the probablity of the “right” class as high as possible</li>
</ul>
</section>
<section id="intro-to-programming-frameworks" class="level3">
<h3 class="anchored" data-anchor-id="intro-to-programming-frameworks">Intro to programming frameworks</h3>
<p>Deep learning frameworks</p>
<ul>
<li>implementing a basic NN libary is a great learning framework</li>
<li>as we implemnt complex/large models its not practical implement everything from scratch - there are many good frameworks to choose from</li>
<li>the DL frameworks not only speed up coding but implement many optimizations</li>
<li>choose one by looking at programning ease, running speed and how open it is</li>
<li>my own research has led my to tensorflow/keras or pytorch</li>
</ul>
<p><a href="https://www.tensorflow.org/">Tensorflow</a></p>
<ul>
<li>covers the very basics of tensorflow, though the <a href="https://www.tensorflow.org/tutorials/">tensorflow guide is better</a>.</li>
<li>we implement forward prop, tf automatically does the backprop</li>
<li>a tf program looks like:
<ul>
<li>make tensors</li>
<li>write operations to those tensors</li>
<li>initialize tensors</li>
<li>create and run a Session</li>
</ul></li>
<li>tf.placeholder is a variable to which we assign a value later</li>
</ul>
</section>
<section id="yuanqing-lin-interview" class="level3">
<h3 class="anchored" data-anchor-id="yuanqing-lin-interview"><a href="https://twitter.com/yuanqinglin">Yuanqing Lin</a> interview</h3>
<ul>
<li>heads China’s National Deep Learning Research Lab</li>
<li>building a really large deep learning platform</li>
</ul>


<!-- -->

</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "deeplearning.ai: Improving Deep Neural Networks"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2017-01-01</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> my notes on part 2 of the deeplearning.ai course</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> /imgages/deeplearningai-2-of-5-cert.png</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">- courses</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu"># deeplearning.ai: Improving Deep Neural Networks</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</span><span class="co">](https://www.coursera.org/learn/deep-neural-network/home/welcome)</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>**Course Resources**</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Discussion forum</span><span class="co">](https://www.coursera.org/learn/deep-neural-network/discussions)</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">YouTube playlist for Course 1</span><span class="co">](https://www.youtube.com/watch?v=1waHlpKiNyY&amp;list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc)</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>course reviews: <span class="co">[</span><span class="ot">1</span><span class="co">](https://towardsdatascience.com/review-of-deeplearning-ai-courses-aed1328e4ffe)</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 1: Practical aspects of Deep Learning</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>Train / Dev / Test sets</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>training is a highly iterative process as you test ideas and parameters</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>even highly skilled practicioners find it impossible to predict the best parameters for a problem, so going through</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>split data into train/dev/test sets</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>training set - the actual dataset used to train the model</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>cross validation or dev set - used to evaluate the model</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>test set - only used once the model is finally trained to get a unbiased estimate of the models performance</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>depending on the dataset size, use different splits, eg:</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>100 to 1M ==&gt; 60/20/20</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>1M to INF ==&gt; 98/1/1 or 99.5/0.25/0.25</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>you can mismatch train/test distribution, so make sure they come from the same distribution</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>Bias / Variance</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="al">![](/img/bias-variance.png)</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>high bias - model is under fitting, or not even fitting the training set</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>high variance - model is over fitting the training and dev set</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>you can have both high bias AND high variance</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>balance the bias and variance, and also consider the underlying problems tractibility, as are we aiming for 99% accuracy or 70%? Compare with other models and human performance to get a sense of a baseline error</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>Basic recipe for machine learning</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>for high bias errors: bigger NN,more layers, different model, try different activations</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>high variance: more data, regularize appropirately, try a different model</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>training a bigger NN almost never hurts</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>Regularization</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="al">![](https://upload.wikimedia.org/wikipedia/commons/b/b8/Sparsityl1.png?1531465205841)</span></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>vectors often have many dimensions, so model sizes get BIG. regulariation helps to select features and reduce dimensions, as well as reduce overfitting</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>L1 regularization (or Lasso) adds a penalty equal to the sum of absoulte coefficients</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>L2 regularization (or Ridge) adds a penalty equal to the sum of squared coefficients</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>as a rule of thumb, L2 almost always works better than L1 .</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>Dropout regularization</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>for each iteration, use a probablity to determine whether to "drop" a neuron. So each iteration through the NN drops a different, random set of neurons.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>each layer in the NN can have a different dropout probability, the downside is that we have more hyperparameters to tweak</span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>in computer vision, we almost always use dropout becuase we are almost always overfitting in vision problems</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a downside of dropout is that the cost function is no longer well defined, so turn off dropout to see that the loss is dropping, which implies that our model is working, then turn on dropout for better training</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>remove dropout at test time</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>dropout intuitions:</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>can't rely on on any given neuron, so have to spread out weights</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>can work similar to L2 regularization</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>helps prevent overfitting</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>Data augmentation</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>inexpensive way to get more data - e.g with images flip, distory, crop, rotate etc to get more images</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this helps as a regularization technique</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>-</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Early stopping</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>stop training the NN when the dev set and training set error start diverging - get the huperparameters from the lowest dev and training set cost.</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this prevents the NN from overfitting on the training set but stops gradient descent early</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Andrew NG generally prefers using L2 regularization instead of early stopping as that</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>Deep NN suffer from vanishing and exploding gradients</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this happens when gradients become very small or big</span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>in a deep NN if activations are linear, than the activations and derivates will increase exponentially with layers</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a>Weight Initialization for Deep Networks</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this is a paritial solution to vanishing/exploding gradients</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>initialize the weights with a variance equal to 1/n - where n is the number of input features - sure weights are not too small or not to large</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>Numerical approximation of gradients</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a>Gradient Checking</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>check our gradient computation functions - this helps debug backprop</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a>-</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 2:Optimizatoni algorithims</span></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a>Mini-batch gradient descent</span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>training big data is slow - breaking it up into smaller batches speeds things up</span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>vectorization allows us to put our entire data set of m examples into a huge matrix and process it all in one go</span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>but this way we have to process the entire set before our gradient descent can make a small step in the right direction</span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>training on mini batches allows gradient descent to work much faster, as in one iteration over the dataset we would have taken m / batch_size gradient descent steps</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>makes the leaning more 'noisy' since each mini-batch is new data (compared to going over the entire dataset)</span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a typical mini-batch size is 64, 128, 256, 512</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>should be a power of 2 as thats how computer memory is setup</span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>make sure the batch fits inside cpu/gpu memory</span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a>-</span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a>Exponentially weighted averages</span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>also called exponentially weighted moving averages in statistics</span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this decreases the weight of older data exponentially, enhancing the effect of more recent numbers which makes it easy to spot new trends. Of course the parameters can be tweaked, like changing beta depending on how many data points to average<span class="in">`(1 / (1 - beta))`</span>.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>key component of several optimization algos</span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a>```V(t) = beta * v(t-1) + (1-beta) * theta(t)```</span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a>bias correction in exponentially weighted averages:</span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the moving avg in the beginning is low since there isn't past data to refer from and it starts from zero.</span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>so add a bias term, dividing the above equation by <span class="in">`(1 - beta^t)`</span>:</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a>```v(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)```</span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>Gradient descent with momentum</span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>modify gradient descent so on each iteration compute the exponential weighted averages of the gradients and update weights</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this takes us faster to the min point, and dampens out oscillations</span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>most common value of beta is <span class="in">`0.9`</span>. generally we don't bother with bias correction since we do so many iterations</span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">this explains why momentum works</span><span class="co">](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d)</span>:</span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; With Stochastic Gradient Descent we don’t compute the exact derivate of our loss function. Instead, we’re estimating it on a small batch. Which means we’re not always going in the optimal direction, because our derivatives are ‘noisy’. Just like in my graphs above. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations.</span></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>also see <span class="co">[</span><span class="ot">Nesterov Momentum</span><span class="co">](http://cs231n.github.io/neural-networks-3/#sgd)</span></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a>RMSprop or Root mean square prop</span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we want learning to go fast horizontally and slower vertically - so we divide updates in the vertical direction by a large number and updates in the horizontal direction by a much smaller number</span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this dampens oscillations, so we can use a faster learning rate</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>first proposed in Hinton's coursera course</span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a>Adam optimization algorithim</span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>mashes together momemtum and RMSprop, works very well</span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>parameters:</span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>learning rate alpha</span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>beta1: moving avg or momemtum parameter, same as 0.9 above</span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>beta2: RMSprop, <span class="in">`0.999`</span> works well</span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>epsilon <span class="in">`10^8`</span> - less important, can leave it at default</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>generally leave values at default, just try out different learning rates</span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a>Learning rate decay</span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>slowly decrease learning rate over epochs</span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this makes intuitive sense as in the beginning bigger steps are ok and as the NN starts converging, we need smaller steps</span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>other methods: exponential decay, discrete steps, etc</span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>manual decay - watch the model as it trains, pause and manually change the learning rate - works if running a small number of models which take a long time to train</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this is lower down on the list of things to try when tuning</span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a>The problem of local optima</span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>people used to worry a lot about NN getting in local optima, but in multi-dimensional space most points of zero gradient are saddle points so its very unlikely to get stuck in a local optima</span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a lot of our intuitions about low dimensional spaces don't transfer over the high dimensional space practically all NN's use - i.e if we have 20K parameters, the NN is operating in a 20K dimensional space</span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>but plateaus can slow down learning, so techniques like momentum, Adam help here</span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 3: Hyperparameter tuning, Batch Normalization and Programming Frameworks</span></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hyperparameter tuning</span></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a>The Tuning Process</span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>there are tons of hyperparameters to tune, but some are more important than others</span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**alpha** or the learning rate is the most important parameter most the time</span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>then the second  most important:</span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**momentum** (0.9 being a good default,</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**mini-batch size**</span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**hidden units**</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>third in important:</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>num of layers</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a>  -learning rate decay</span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>when using Adam, you pretty much never have to tune beta1, beta2 and epsilon</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>of course, this depends on the NN, dataset etc</span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>don't use a grid of one val vs the other - this works when you have a small number of hyperparameters, but in Deep Learning choose hyperparameter combinations at random</span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use coarse to fine sampling - find the range where a parameter is working, then do finer grained sampling to get the best val</span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a>Using an appropriate scale to pick hyperparameters</span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>pick the appropriate scale for hyperparameters, generally better to use log scale rather than linear</span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a>Hyperparameters tuning in practice: Pandas vs. Caviar</span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>intuitions about hyperparameters often don't transfer to other domains e.g logistcs, nlpo, vision, speech will all have different best parameters</span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>two major ways to find the best parameters:</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**babysit one model** - as its training, tweak the parameters and see how its doing on metrics. This is helpful when we don't have enough computation capcity (panda approach)</span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**train many models in parallel** - run many models in parallel, with different parameters (caviar approach)</span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a><span class="fu">### Batch Norm</span></span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a>Normalizing activations in a network</span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>instead of just normalizing the inputs to a NN, we also normalize the outputs of each layer of a NN - this is called batch norm.</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>using the standard <span class="in">`(intermediate val - mean) / (std dev + epsilon)`</span>. Epsilon is needed for numerical stability if variance is zero</span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>batch norm sets the hidden layer to have mean 0 and variance 1, but sometimes we might want it to have a different distribution, so we can add a parameter beta to the hidden units to change the shape of the distribution. (like we might want a larger variance to take advantage of the nonlinearity of the sigmoid function).</span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>batch norm makes the NN more robust and speeds up learning</span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>while batch norm can be applied before or after the activation function, in practice it is generally applied before the activation funciton.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a>Fitting Batch Normalization into a neural network</span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Deep learning frameworks have batch norm built in, like <span class="co">[</span><span class="ot">tensorflow</span><span class="co">](https://www.tensorflow.org/api_docs/python/tf/nn/batch_normalization)</span>, <span class="co">[</span><span class="ot">keras</span><span class="co">](https://keras.io/layers/normalization/)</span> etc.</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>implementing this ourself is straightforward, as we add in a norm step just before applying the activation function at each layer in the NN.</span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>In each mini-batch, for each hidden layer compute the mean and the variance in that mini-batch, normalize, then apply the activation function as before.</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a>Why does Batch normalization work?</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a>Three main reaons:</span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1: normalizing input features speeds up learning, so one intuition is that this is doing a similar thing for each hidden layer</span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>2: makes weights in deeper layers more robust to changes in weights in earlier layers</span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>for example, we train a network on black cats, and we try to classify a coloured cat. Every input is shifted, but the decision boundaries haven't changed. This has a fancy name, coviariate shift.</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>allows each layer to learn more independently</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>3: regularization of hidden units</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>adds some noise to each mini-batch, as each one is regularized on its own mean/variance, which has a slight regularization effect (bigger batches reduce noise/regularization)</span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>but don't rely on batch norm for regularization, as its just a unintended side effect, use other techniques like L2 or dropout</span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a>  </span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>Batch normalization at test time</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we often predict one example at a time at test time, so the idea of a mean/variance doesn't apply - we no longer have a mini-batch to process</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we calculate the exponentially weighted average across all the mini-batches and use that to get a mean/variance to apply at test time on the one test example</span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a><span class="fu">### Multi-class Classification</span></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a>Softmax regresssion</span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>is generalization of logistic regression which can predict multiple classes rather than just a binary.</span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a>Training a Softmax classifier</span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a hard max would look at a vector and put 1 for the max and zero for everything else</span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>soft max takes in a vector and puts in a probability for each value such that they all sum up to 1</span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>loss function is trying to make the probablity of the "right" class as high as possible</span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a><span class="fu">### Intro to programming frameworks</span></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a>Deep learning frameworks</span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>implementing a basic NN libary is a great learning framework</span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>as we implemnt complex/large models its not practical implement everything from scratch - there are many good frameworks to choose from</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the DL frameworks not only speed up coding but implement many optimizations</span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>choose one by looking at programning ease, running speed and how open it is</span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>my own research has led my to tensorflow/keras or pytorch</span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Tensorflow</span><span class="co">](https://www.tensorflow.org/)</span></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>covers the very basics of tensorflow, though the <span class="co">[</span><span class="ot">tensorflow guide is better</span><span class="co">](https://www.tensorflow.org/tutorials/)</span>.</span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we implement forward prop, tf automatically does the backprop</span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a tf program looks like:</span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>make tensors</span>
<span id="cb1-266"><a href="#cb1-266" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>write operations to those tensors</span>
<span id="cb1-267"><a href="#cb1-267" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>initialize tensors</span>
<span id="cb1-268"><a href="#cb1-268" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>create and run a Session</span>
<span id="cb1-269"><a href="#cb1-269" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tf.placeholder is a variable to which we assign a value later</span>
<span id="cb1-270"><a href="#cb1-270" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-271"><a href="#cb1-271" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Yuanqing Lin](https://twitter.com/yuanqinglin) interview</span></span>
<span id="cb1-272"><a href="#cb1-272" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-273"><a href="#cb1-273" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>heads China's National Deep Learning Research Lab</span>
<span id="cb1-274"><a href="#cb1-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>building a really large deep learning platform</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>