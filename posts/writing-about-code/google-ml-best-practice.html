<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.433">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KO">
<meta name="dcterms.date" content="2019-03-02">
<meta name="description" content="my read through notes on googles ml guide">

<title>khalido.org - Googles Best Practices for ML Engineering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">khalido.org</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/khalido" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/KO" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Googles Best Practices for ML Engineering</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          my read through notes on googles ml guide
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">ml</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>KO </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 2, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#googles-best-practices-for-ml-engineering" id="toc-googles-best-practices-for-ml-engineering" class="nav-link active" data-scroll-target="#googles-best-practices-for-ml-engineering">Googles Best Practices for ML Engineering</a></li>
  <li><a href="#do-we-need-ml" id="toc-do-we-need-ml" class="nav-link" data-scroll-target="#do-we-need-ml">do we need ML?</a>
  <ul class="collapse">
  <li><a href="#rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed." id="toc-rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed." class="nav-link" data-scroll-target="#rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed.">Rule 1: don’t do ML unless there is enough data and ML is actually needed.</a></li>
  <li><a href="#rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want." id="toc-rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want." class="nav-link" data-scroll-target="#rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want.">Rule 2: first write down what you want the ML to do and what data you might want.</a></li>
  <li><a href="#rule-3-use-ml-over-complex-heristics." id="toc-rule-3-use-ml-over-complex-heristics." class="nav-link" data-scroll-target="#rule-3-use-ml-over-complex-heristics.">Rule 3: Use ML over complex heristics.</a></li>
  </ul></li>
  <li><a href="#ml-phase-1-pipelines-and-infrastructure" id="toc-ml-phase-1-pipelines-and-infrastructure" class="nav-link" data-scroll-target="#ml-phase-1-pipelines-and-infrastructure">ML Phase 1: Pipelines and infrastructure</a>
  <ul class="collapse">
  <li><a href="#rule-4-start-with-simple-ml-and-concentrate-on-infrastructure" id="toc-rule-4-start-with-simple-ml-and-concentrate-on-infrastructure" class="nav-link" data-scroll-target="#rule-4-start-with-simple-ml-and-concentrate-on-infrastructure">Rule 4: Start with simple ML and concentrate on infrastructure:</a></li>
  <li><a href="#rule-5-test-data-flows-and-infrastructure-seperately-from-ml" id="toc-rule-5-test-data-flows-and-infrastructure-seperately-from-ml" class="nav-link" data-scroll-target="#rule-5-test-data-flows-and-infrastructure-seperately-from-ml">Rule 5: Test data flows and infrastructure seperately from ML</a></li>
  <li><a href="#rule-6-dont-drop-data-if-copying-pipelines" id="toc-rule-6-dont-drop-data-if-copying-pipelines" class="nav-link" data-scroll-target="#rule-6-dont-drop-data-if-copying-pipelines">Rule 6: don’t drop data if copying pipelines</a></li>
  <li><a href="#rule-7-use-existing-knowledge" id="toc-rule-7-use-existing-knowledge" class="nav-link" data-scroll-target="#rule-7-use-existing-knowledge">Rule 7: use existing knowledge</a></li>
  </ul></li>
  <li><a href="#monitering" id="toc-monitering" class="nav-link" data-scroll-target="#monitering">Monitering</a>
  <ul class="collapse">
  <li><a href="#rule-8-freshness" id="toc-rule-8-freshness" class="nav-link" data-scroll-target="#rule-8-freshness">Rule 8: freshness</a></li>
  <li><a href="#rule-9-sanity-check-models-before-using" id="toc-rule-9-sanity-check-models-before-using" class="nav-link" data-scroll-target="#rule-9-sanity-check-models-before-using">Rule 9: sanity check models before using</a></li>
  <li><a href="#rule-10-watch-for-silent-failures" id="toc-rule-10-watch-for-silent-failures" class="nav-link" data-scroll-target="#rule-10-watch-for-silent-failures">Rule 10: watch for silent failures</a></li>
  <li><a href="#rule-11-document-features-and-assign-owners" id="toc-rule-11-document-features-and-assign-owners" class="nav-link" data-scroll-target="#rule-11-document-features-and-assign-owners">Rule 11: document features and assign owners</a></li>
  </ul></li>
  <li><a href="#first-objectives" id="toc-first-objectives" class="nav-link" data-scroll-target="#first-objectives">First Objectives</a>
  <ul class="collapse">
  <li><a href="#rule-12-dont-overthink-which-objective-to-optimize" id="toc-rule-12-dont-overthink-which-objective-to-optimize" class="nav-link" data-scroll-target="#rule-12-dont-overthink-which-objective-to-optimize">Rule 12: dont overthink which objective to optimize</a></li>
  <li><a href="#rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective" id="toc-rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective" class="nav-link" data-scroll-target="#rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective">Rule 13: Use a simple, observable and attributtable metric as a first objective</a></li>
  <li><a href="#rule-14-start-with-an-interpretable-model-to-make-debugging-easier" id="toc-rule-14-start-with-an-interpretable-model-to-make-debugging-easier" class="nav-link" data-scroll-target="#rule-14-start-with-an-interpretable-model-to-make-debugging-easier">Rule 14: Start with an interpretable model to make debugging easier</a></li>
  <li><a href="#rule-15-seperate-spam-filtering-and-quality-ranking" id="toc-rule-15-seperate-spam-filtering-and-quality-ranking" class="nav-link" data-scroll-target="#rule-15-seperate-spam-filtering-and-quality-ranking">Rule 15: Seperate spam filtering and quality ranking</a></li>
  </ul></li>
  <li><a href="#ml-phase-ii-feature-engineering" id="toc-ml-phase-ii-feature-engineering" class="nav-link" data-scroll-target="#ml-phase-ii-feature-engineering">ML Phase II: Feature Engineering</a>
  <ul class="collapse">
  <li><a href="#rule-16-launch-and-iterate" id="toc-rule-16-launch-and-iterate" class="nav-link" data-scroll-target="#rule-16-launch-and-iterate">Rule 16: Launch and iterate</a></li>
  <li><a href="#rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features" id="toc-rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features" class="nav-link" data-scroll-target="#rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features">Rule 17: Start with directly observed and reported features as opposed to learned features</a></li>
  <li><a href="#rule-18-explore-with-features-of-content-that-generalizes-across-contexts" id="toc-rule-18-explore-with-features-of-content-that-generalizes-across-contexts" class="nav-link" data-scroll-target="#rule-18-explore-with-features-of-content-that-generalizes-across-contexts">Rule 18: Explore with features of content that generalizes across contexts</a></li>
  <li><a href="#rule-19-user-very-specific-features-when-you-can" id="toc-rule-19-user-very-specific-features-when-you-can" class="nav-link" data-scroll-target="#rule-19-user-very-specific-features-when-you-can">Rule 19: User very specific features when you can</a></li>
  <li><a href="#rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways" id="toc-rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways" class="nav-link" data-scroll-target="#rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways">Rule 20: combine and modify existing features to create new features in human understandable ways</a></li>
  <li><a href="#rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data" id="toc-rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data" class="nav-link" data-scroll-target="#rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data">Rule 21: number of feature weights is roughly proportional to the amount of data</a></li>
  <li><a href="#rule-22-clean-up-features-no-longer-in-use" id="toc-rule-22-clean-up-features-no-longer-in-use" class="nav-link" data-scroll-target="#rule-22-clean-up-features-no-longer-in-use">Rule 22: clean up features no longer in use</a></li>
  </ul></li>
  <li><a href="#human-analysis-of-the-system" id="toc-human-analysis-of-the-system" class="nav-link" data-scroll-target="#human-analysis-of-the-system">Human analysis of the System</a>
  <ul class="collapse">
  <li><a href="#rule-23-youre-not-a-typical-end-user" id="toc-rule-23-youre-not-a-typical-end-user" class="nav-link" data-scroll-target="#rule-23-youre-not-a-typical-end-user">Rule 23: you’re not a typical end user</a></li>
  <li><a href="#rule-24-measure-the-dela-within-models" id="toc-rule-24-measure-the-dela-within-models" class="nav-link" data-scroll-target="#rule-24-measure-the-dela-within-models">Rule 24: measure the dela within models</a></li>
  <li><a href="#rule-25-utilitarian-performance-trumps-predictive-power" id="toc-rule-25-utilitarian-performance-trumps-predictive-power" class="nav-link" data-scroll-target="#rule-25-utilitarian-performance-trumps-predictive-power">Rule 25: utilitarian performance trumps predictive power</a></li>
  <li><a href="#rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features" id="toc-rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features" class="nav-link" data-scroll-target="#rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features">Rule 26: look for patterns in the measured errors and create new features</a></li>
  <li><a href="#rule-27-try-to-quantify-observed-behaviour" id="toc-rule-27-try-to-quantify-observed-behaviour" class="nav-link" data-scroll-target="#rule-27-try-to-quantify-observed-behaviour">Rule 27: try to quantify observed behaviour</a></li>
  <li><a href="#rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour" id="toc-rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour" class="nav-link" data-scroll-target="#rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour">Rule 28: be aware that identical short-term behaviour does not imply identical long-term behaviour</a></li>
  </ul></li>
  <li><a href="#training-serving-skew" id="toc-training-serving-skew" class="nav-link" data-scroll-target="#training-serving-skew">Training-Serving Skew</a>
  <ul class="collapse">
  <li><a href="#rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time" id="toc-rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time" class="nav-link" data-scroll-target="#rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time">Rule 29: best way to make sure you train like you serve is to save the set of features used at serving time and pipe those features to a log to use them at training time</a></li>
  <li><a href="#rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it" id="toc-rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it" class="nav-link" data-scroll-target="#rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it">Rule 30: Importance-weight sampled data, don’t arbitarily drop it</a></li>
  <li><a href="#rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change" id="toc-rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change" class="nav-link" data-scroll-target="#rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change">Rule 31: beware if you join data from a table at training and serving time, the data in the table may change</a></li>
  <li><a href="#rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible" id="toc-rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible" class="nav-link" data-scroll-target="#rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible">Rule 32: reuse code b/w training and serviing pipelines whenever possible</a></li>
  <li><a href="#rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th" id="toc-rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th" class="nav-link" data-scroll-target="#rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th">Rule 33: if you make a model on data till Jan 5th, test it on data after Jan 6th</a></li>
  <li><a href="#rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data" id="toc-rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data" class="nav-link" data-scroll-target="#rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data">Rule 34: in binary classification for filtering make small short-term sacrifices in performance for very clean data</a></li>
  <li><a href="#rule-35-beware-of-inherent-skew-in-ranking-problems" id="toc-rule-35-beware-of-inherent-skew-in-ranking-problems" class="nav-link" data-scroll-target="#rule-35-beware-of-inherent-skew-in-ranking-problems">Rule 35: Beware of inherent skew in ranking problems</a></li>
  <li><a href="#rule-36-aviod-feedback-loops-with-positional-features" id="toc-rule-36-aviod-feedback-loops-with-positional-features" class="nav-link" data-scroll-target="#rule-36-aviod-feedback-loops-with-positional-features">Rule 36: Aviod feedback loops with positional features</a></li>
  <li><a href="#rule-37-measure-trainingserving-skew" id="toc-rule-37-measure-trainingserving-skew" class="nav-link" data-scroll-target="#rule-37-measure-trainingserving-skew">Rule 37: Measure Training/Serving skew</a></li>
  </ul></li>
  <li><a href="#ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models" id="toc-ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models" class="nav-link" data-scroll-target="#ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models">ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models</a>
  <ul class="collapse">
  <li><a href="#rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue" id="toc-rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue" class="nav-link" data-scroll-target="#rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue">Rule 38: don’t waste time on new features if unaligined objectives have become the issue</a></li>
  <li><a href="#rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals" id="toc-rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals" class="nav-link" data-scroll-target="#rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals">Rule 39: launch decisions are a proxy for long-term product goals</a></li>
  <li><a href="#rule-40-keep-ensembles-simple" id="toc-rule-40-keep-ensembles-simple" class="nav-link" data-scroll-target="#rule-40-keep-ensembles-simple">Rule 40: Keep ensembles simple</a></li>
  <li><a href="#rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals." id="toc-rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals." class="nav-link" data-scroll-target="#rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals.">Rule 41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.</a></li>
  <li><a href="#rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are." id="toc-rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are." class="nav-link" data-scroll-target="#rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are.">Rule 42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.</a></li>
  <li><a href="#rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be." id="toc-rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be." class="nav-link" data-scroll-target="#rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be.">Rule 43: Your friends tend to be the same across different products. Your interests tend not to be.</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="googles-best-practices-for-ml-engineering" class="level1">
<h1>Googles Best Practices for ML Engineering</h1>
<p>Notes on <a href="https://developers.google.com/machine-learning/guides/rules-of-ml/">Google’s best practices guide for ML engineering</a>:</p>
<blockquote class="blockquote">
<p>This document is intended to help those with a basic knowledge of machine learning get the benefit of Google’s best practices in machine learning.</p>
</blockquote>
</section>
<section id="do-we-need-ml" class="level1">
<h1>do we need ML?</h1>
<section id="rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed." class="level2">
<h2 class="anchored" data-anchor-id="rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed.">Rule 1: don’t do ML unless there is enough data and ML is actually needed.</h2>
<ul>
<li>will human hueristics do well enough?</li>
<li>is there enough data?</li>
</ul>
</section>
<section id="rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want." class="level2">
<h2 class="anchored" data-anchor-id="rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want.">Rule 2: first write down what you want the ML to do and what data you might want.</h2>
<ul>
<li>design and implement metrics</li>
<li>have a way to run experiments</li>
</ul>
</section>
<section id="rule-3-use-ml-over-complex-heristics." class="level2">
<h2 class="anchored" data-anchor-id="rule-3-use-ml-over-complex-heristics.">Rule 3: Use ML over complex heristics.</h2>
<ul>
<li>Once enough data and goals, move on to ML. ML models are easier to update and maintain then complex heuristics.</li>
</ul>
</section>
</section>
<section id="ml-phase-1-pipelines-and-infrastructure" class="level1">
<h1>ML Phase 1: Pipelines and infrastructure</h1>
<p>Focus on system infrastucture at first.</p>
<section id="rule-4-start-with-simple-ml-and-concentrate-on-infrastructure" class="level2">
<h2 class="anchored" data-anchor-id="rule-4-start-with-simple-ml-and-concentrate-on-infrastructure">Rule 4: Start with simple ML and concentrate on infrastructure:</h2>
<ul>
<li>how is data getting to the model</li>
<li>what does it mean to be good or bad for us</li>
<li>how are results delivered to our app/server/user whatever</li>
<li>use simple features and models to start off with.</li>
</ul>
</section>
<section id="rule-5-test-data-flows-and-infrastructure-seperately-from-ml" class="level2">
<h2 class="anchored" data-anchor-id="rule-5-test-data-flows-and-infrastructure-seperately-from-ml">Rule 5: Test data flows and infrastructure seperately from ML</h2>
<ul>
<li>have a way to check how data is getting to the algo</li>
<li>manually check inputs</li>
<li>have statistics for the inputs</li>
<li>get models, see if they behave the same in testing and production</li>
<li>ML algorithims can be unpredictable, so have tests for code and use a fixed model in production</li>
<li><a href="http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html">Understand your data</a></li>
</ul>
</section>
<section id="rule-6-dont-drop-data-if-copying-pipelines" class="level2">
<h2 class="anchored" data-anchor-id="rule-6-dont-drop-data-if-copying-pipelines">Rule 6: don’t drop data if copying pipelines</h2>
<p>When setting up data pipelines, be carefuly about copying existing ones - every data pipeline is serving a specifc need.</p>
</section>
<section id="rule-7-use-existing-knowledge" class="level2">
<h2 class="anchored" data-anchor-id="rule-7-use-existing-knowledge">Rule 7: use existing knowledge</h2>
<p>Most ML is applied to problems with existing solutions. Use the older hueristics and rules and incorporate them into the ML algo by:</p>
<ul>
<li>preprocess using the hueristics</li>
<li>create features from the heuristic - i.e if we have a function computing a score, make that score a feature</li>
<li>look at the inputs of a heuristic and feed them as features into the ML</li>
<li>modify the label, e.g multiply downloads by number of stars to make quality apps stand out more</li>
</ul>
<p>See if there is a simpler way to incorporate old heuristics.</p>
</section>
</section>
<section id="monitering" class="level1">
<h1>Monitering</h1>
<p>Alerts! Dashboards!</p>
<section id="rule-8-freshness" class="level2">
<h2 class="anchored" data-anchor-id="rule-8-freshness">Rule 8: freshness</h2>
<ul>
<li>how frequently does the model need to be updated? monitor and alert accordingly.</li>
</ul>
</section>
<section id="rule-9-sanity-check-models-before-using" class="level2">
<h2 class="anchored" data-anchor-id="rule-9-sanity-check-models-before-using">Rule 9: sanity check models before using</h2>
<ul>
<li>make sure the model’s perf is reasonable on validation data - e.g check area under <a href="https://www.wikiwand.com/en/Receiver_operating_characteristic">roc curve</a>.</li>
</ul>
</section>
<section id="rule-10-watch-for-silent-failures" class="level2">
<h2 class="anchored" data-anchor-id="rule-10-watch-for-silent-failures">Rule 10: watch for silent failures</h2>
<p>Supid things happen, like part of the incoming data is being dropped somewhere. the ML system will keep chugging along as they adjust to feature changes and will decay gradually.</p>
<ul>
<li>tracking statistics for data helps here, for example if a certain feature column has less populated data then before</li>
</ul>
</section>
<section id="rule-11-document-features-and-assign-owners" class="level2">
<h2 class="anchored" data-anchor-id="rule-11-document-features-and-assign-owners">Rule 11: document features and assign owners</h2>
<ul>
<li>document all the features, duh</li>
<li>for larger systems, assign ownership and responsbilities</li>
</ul>
</section>
</section>
<section id="first-objectives" class="level1">
<h1>First Objectives</h1>
<section id="rule-12-dont-overthink-which-objective-to-optimize" class="level2">
<h2 class="anchored" data-anchor-id="rule-12-dont-overthink-which-objective-to-optimize">Rule 12: dont overthink which objective to optimize</h2>
<p>Don’t optimize a single metric early, initially all metrics we care about should typically be going up. Consider the overall ‘health’ of the ml process first.</p>
</section>
<section id="rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective" class="level2">
<h2 class="anchored" data-anchor-id="rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective">Rule 13: Use a simple, observable and attributtable metric as a first objective</h2>
<p>This should be straightward and serve as a proxy for the true objective, in fact, often there is no ‘true’ objective. Model direct things, leave indirect things for later or to a different layer altogether.</p>
</section>
<section id="rule-14-start-with-an-interpretable-model-to-make-debugging-easier" class="level2">
<h2 class="anchored" data-anchor-id="rule-14-start-with-an-interpretable-model-to-make-debugging-easier">Rule 14: Start with an interpretable model to make debugging easier</h2>
<p>Linear regression, logistic regression etc are straightforward to understand and debug vs the more complicated models.</p>
</section>
<section id="rule-15-seperate-spam-filtering-and-quality-ranking" class="level2">
<h2 class="anchored" data-anchor-id="rule-15-seperate-spam-filtering-and-quality-ranking">Rule 15: Seperate spam filtering and quality ranking</h2>
<p>Rank content seperately from how you determine what kind of content it is - for example have a different model for post quality score, and another one for categorzing spam.</p>
<p>i.e don’t do something overly simple like assuming posts with a low score are spam.</p>
</section>
</section>
<section id="ml-phase-ii-feature-engineering" class="level1">
<h1>ML Phase II: Feature Engineering</h1>
<p>Phase 1 should give us a simple working end to end system with unit and system tests.</p>
<p>In Phase 2 we pick all the low hanging fruit, like pulling in as many features are possible and combining them in intuitive ways.</p>
<section id="rule-16-launch-and-iterate" class="level2">
<h2 class="anchored" data-anchor-id="rule-16-launch-and-iterate">Rule 16: Launch and iterate</h2>
<p>As you build the model, keep adding, removing and recombining features and launching new models. Don’t wait to do all the features in one go, iterate.</p>
</section>
<section id="rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features" class="level2">
<h2 class="anchored" data-anchor-id="rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features">Rule 17: Start with directly observed and reported features as opposed to learned features</h2>
<p>Learned features can be coming from elsewhere can have a lot of issues and should not be in the first model. These outside features come with their own objectives which could be only weakly correlated to our objective, and there is no gurantee they will lead to optimal solutions.</p>
<p>We can get excellent baseline performance without deep deatures.</p>
</section>
<section id="rule-18-explore-with-features-of-content-that-generalizes-across-contexts" class="level2">
<h2 class="anchored" data-anchor-id="rule-18-explore-with-features-of-content-that-generalizes-across-contexts">Rule 18: Explore with features of content that generalizes across contexts</h2>
</section>
<section id="rule-19-user-very-specific-features-when-you-can" class="level2">
<h2 class="anchored" data-anchor-id="rule-19-user-very-specific-features-when-you-can">Rule 19: User very specific features when you can</h2>
<p>with big data, its easier to learn millions of simple features than a few complex features. Consider making groups of features if each feature only applies to a small fraction of data, but the group coverage is &gt; 90%.</p>
</section>
<section id="rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways" class="level2">
<h2 class="anchored" data-anchor-id="rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways">Rule 20: combine and modify existing features to create new features in human understandable ways</h2>
<p>There are many ways to do this, for example turning an age feature into infant/child/teenager/adult/senior using age ranges. Don’t overthink - basic quantiles gives the most impact.</p>
<p>Also consider cross combining features like {male,female} with say a country feature like {US,Canada} to get a feature like (male, US). This takes a lot of data, but can be useful in some contexts, while overfitting in others.</p>
</section>
<section id="rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data" class="level2">
<h2 class="anchored" data-anchor-id="rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data">Rule 21: number of feature weights is roughly proportional to the amount of data</h2>
<p>There is a lot of stats theory about the appropriate level of complexity for a model, but this rule is the most important to know. Scale the learning to the size of your data.</p>
<p>With small numbers of training data, you probably have to human engineer features. With larger data sets, regularization and feature selection helps to cut down features.</p>
</section>
<section id="rule-22-clean-up-features-no-longer-in-use" class="level2">
<h2 class="anchored" data-anchor-id="rule-22-clean-up-features-no-longer-in-use">Rule 22: clean up features no longer in use</h2>
<p>drop useless features. keep infrastructure clean so that promising features can be tried fast. Add features back if necesary.</p>
<p>Consider the coverage of features, like how many examples are covered? If a feature only covers 8% of the data, it might be useless, but on the other hand a feature covering only 1% data could have a 90% prediction rate.</p>
</section>
</section>
<section id="human-analysis-of-the-system" class="level1">
<h1>Human analysis of the System</h1>
<p>important to look at existing models and think about how to improve them.</p>
<section id="rule-23-youre-not-a-typical-end-user" class="level2">
<h2 class="anchored" data-anchor-id="rule-23-youre-not-a-typical-end-user">Rule 23: you’re not a typical end user</h2>
<p>You’re too close to the code and results, so its important to bring in outside users - from within the company, lay people or a crowdsourcing platform.</p>
<p>Develop user experience methodologies, and create user personas and do usablity testing.</p>
</section>
<section id="rule-24-measure-the-dela-within-models" class="level2">
<h2 class="anchored" data-anchor-id="rule-24-measure-the-dela-within-models">Rule 24: measure the dela within models</h2>
<p>measure both results b/w different models, and also results with the same model to make sure its stable.</p>
</section>
<section id="rule-25-utilitarian-performance-trumps-predictive-power" class="level2">
<h2 class="anchored" data-anchor-id="rule-25-utilitarian-performance-trumps-predictive-power">Rule 25: utilitarian performance trumps predictive power</h2>
<p>the key question is what we are doing with the prediction - for example when ranking something the quality of the final ranking matters more than the quality of the prediction itself. But in a spam filter which has a cutoff on what is blocked, the precision matters more.</p>
</section>
<section id="rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features" class="level2">
<h2 class="anchored" data-anchor-id="rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features">Rule 26: look for patterns in the measured errors and create new features</h2>
<p>Say for some reason the system is demoting longer posts, then a post lengh feature could be useful. Don’t overthink it - add relevant features and let the model figure it out.</p>
</section>
<section id="rule-27-try-to-quantify-observed-behaviour" class="level2">
<h2 class="anchored" data-anchor-id="rule-27-try-to-quantify-observed-behaviour">Rule 27: try to quantify observed behaviour</h2>
<p>if the model has ‘bad’ properties, like recommending gag apps too often, try to measure or get humans to add category labels and add as a feature.</p>
<blockquote class="blockquote">
<p>If your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is “measure first, optimize second”.</p>
</blockquote>
</section>
<section id="rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour" class="level2">
<h2 class="anchored" data-anchor-id="rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour">Rule 28: be aware that identical short-term behaviour does not imply identical long-term behaviour</h2>
<p>A model for say predicting apps will work great in practice but in production turn out to not recommend any new apps, since in practice there wasn’t a way to learn that new apps should be shown too.</p>
</section>
</section>
<section id="training-serving-skew" class="level1">
<h1>Training-Serving Skew</h1>
<p>this is the difference b/w performance during training and serving, typically caused by:</p>
<ul>
<li>difference in how data is handles in training and serving pipelines</li>
<li>change in data b/w training and serving</li>
<li>feedback loop b/w model and algorithim</li>
</ul>
<p>Best solution is to explicitly monitor things so system and data changes don’t introduce skew unnoticed.</p>
<section id="rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time" class="level2">
<h2 class="anchored" data-anchor-id="rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time">Rule 29: best way to make sure you train like you serve is to save the set of features used at serving time and pipe those features to a log to use them at training time</h2>
<p>If not for all examples, log a fraction of the data.</p>
</section>
<section id="rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it" class="level2">
<h2 class="anchored" data-anchor-id="rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it">Rule 30: Importance-weight sampled data, don’t arbitarily drop it</h2>
<p>don’t arbitarily decide to take the first ten results, rather weight by importance - say if we want to sample X with a 30# probablity, give it a weight of 10/3.</p>
</section>
<section id="rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change" class="level2">
<h2 class="anchored" data-anchor-id="rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change">Rule 31: beware if you join data from a table at training and serving time, the data in the table may change</h2>
<p>b/w training and serving time, features in a table may be changed. Avoid this by logging features at training time.</p>
</section>
<section id="rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible" class="level2">
<h2 class="anchored" data-anchor-id="rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible">Rule 32: reuse code b/w training and serviing pipelines whenever possible</h2>
<p>While how data is arriving is different, requiring different code, try to transform initial data into a human readable object which can be tested and transformed by a common method for use by the machine learning system. Try to use the same programming language for training and serving.</p>
</section>
<section id="rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th" class="level2">
<h2 class="anchored" data-anchor-id="rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th">Rule 33: if you make a model on data till Jan 5th, test it on data after Jan 6th</h2>
<p>this better reflects how the model will do in production.</p>
</section>
<section id="rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data" class="level2">
<h2 class="anchored" data-anchor-id="rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data">Rule 34: in binary classification for filtering make small short-term sacrifices in performance for very clean data</h2>
<p>Say our spam filter blocks 75%, and we might be tempted to learn from the 25% which gets through and the user labels spam. This introduces sampling bias, we will get cleaner data by labelling 1% of traffic as “held-out” and send all those to the users, and we can than use those labelled samples as training data.</p>
</section>
<section id="rule-35-beware-of-inherent-skew-in-ranking-problems" class="level2">
<h2 class="anchored" data-anchor-id="rule-35-beware-of-inherent-skew-in-ranking-problems">Rule 35: Beware of inherent skew in ranking problems</h2>
<p>When we change a ranking algorithim so that different results show up, this effectively changes the data the algorithim is going to see in the future. Design algos around this:</p>
<ul>
<li>have higher regularization for features that cover more queries, over features that are only for one query. This allows the model to favour features that are specific to one or a few queries, and helps from preventing very popular results into leaking into irrelevant quries.</li>
<li>only allow features to have positive weights</li>
<li>don’t have document only features</li>
</ul>
</section>
<section id="rule-36-aviod-feedback-loops-with-positional-features" class="level2">
<h2 class="anchored" data-anchor-id="rule-36-aviod-feedback-loops-with-positional-features">Rule 36: Aviod feedback loops with positional features</h2>
</section>
<section id="rule-37-measure-trainingserving-skew" class="level2">
<h2 class="anchored" data-anchor-id="rule-37-measure-trainingserving-skew">Rule 37: Measure Training/Serving skew</h2>
<ul>
<li>difference in performance on training and holdout data - will always exist, so doesn’t necessarily mean something bad.</li>
<li>difference in performace on holdout data and the nextday data - large drops in perf here could indicate some features are time-sensitive and possibly degrading model perf.</li>
<li>difference in performance bw nextday day and live data. differences here could indicate model error.</li>
</ul>
</section>
</section>
<section id="ml-phase-iii-slowed-growth-optimization-refinement-and-complex-models" class="level1">
<h1>ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models</h1>
<p>The second phase is ending when gains are diminishing and we have tradeoffs b/w metrics.</p>
<p>Since gains are harder to achieve, the machine learning has to get more sophisticated.</p>
<section id="rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue" class="level2">
<h2 class="anchored" data-anchor-id="rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue">Rule 38: don’t waste time on new features if unaligined objectives have become the issue</h2>
<p>as measurements plateau, teams often look at issues outside the scope of the current machine learning system. If those goals aren’t covered by the exisiting ml system, change the system or the goals.</p>
</section>
<section id="rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals" class="level2">
<h2 class="anchored" data-anchor-id="rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals">Rule 39: launch decisions are a proxy for long-term product goals</h2>
<p>For example, adding a feature might increase installs, but drop daily active use. Launch decisions depend on multiple factors, only some of which are optimizable by ML.</p>
<p>Metrics are a proxy for more longterm goals.</p>
</section>
<section id="rule-40-keep-ensembles-simple" class="level2">
<h2 class="anchored" data-anchor-id="rule-40-keep-ensembles-simple">Rule 40: Keep ensembles simple</h2>
<p>each model should either be ensemble taking input of other models, or a base model taking in features, not both. Don’t pile on models on top of models - this can result in bad behaviour.</p>
<p>Make sure that in crease in the predicted probality of a underlying classifier doesn’t decrease the predicted probability of the classifier.</p>
</section>
<section id="rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals." class="level2">
<h2 class="anchored" data-anchor-id="rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals.">Rule 41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.</h2>
<p>add more relevant signals, build out infrastructure for radically different features, use more deep learning, weigh benefits of new features against increased complexity.</p>
</section>
<section id="rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are." class="level2">
<h2 class="anchored" data-anchor-id="rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are.">Rule 42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.</h2>
<p>popularity is easy to measure, diversity, personalization and relavance is harder - adding features for these can turn out to not work very well, i.e they get less weight in the model.</p>
<p>Postprocessing can help, or directly modifing the objective to increase diversity or relevance.</p>
</section>
<section id="rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be." class="level2">
<h2 class="anchored" data-anchor-id="rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be.">Rule 43: Your friends tend to be the same across different products. Your interests tend not to be.</h2>
<p>While one thing might be close to another, doesn’t mean it applies across products.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Googles Best Practices for ML Engineering"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2019-03-02</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> my read through notes on googles ml guide </span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">- ml</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu"># Googles Best Practices for ML Engineering</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>Notes on <span class="co">[</span><span class="ot">Google's best practices guide for ML engineering</span><span class="co">](https://developers.google.com/machine-learning/guides/rules-of-ml/)</span>:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; This document is intended to help those with a basic knowledge of machine learning get the benefit of Google's best practices in machine learning.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu"># do we need ML?</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 1: don't do ML unless there is enough data and ML is actually needed.</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>will human hueristics do well enough?</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>is there enough data?</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 2: first write down what you want the ML to do and what data you might want.</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>design and implement metrics</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>have a way to run experiments</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 3: Use ML over complex heristics.</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Once enough data and goals, move on to ML. ML models are easier to update and maintain then complex heuristics.</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="fu"># ML Phase 1: Pipelines and infrastructure</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>Focus on system infrastucture at first.</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 4: Start with simple ML and concentrate on infrastructure:</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how is data getting to the model</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what does it mean to be good or bad for us</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how are results delivered to our app/server/user whatever</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use simple features and models to start off with.</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 5: Test data flows and infrastructure seperately from ML</span></span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>have a way to check how data is getting to the algo</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>manually check inputs</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>have statistics for the inputs</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>get models, see if they behave the same in testing and production</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ML algorithims can be unpredictable, so have tests for code and use a fixed model in production</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Understand your data</span><span class="co">](http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html)</span></span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 6: don't drop data if copying pipelines</span></span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>When setting up data pipelines, be carefuly about copying existing ones - every data pipeline is serving a specifc need.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 7: use existing knowledge</span></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>Most ML is applied to problems with existing solutions. Use the older hueristics and rules and incorporate them into the ML algo by:</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>preprocess using the hueristics</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>create features from the heuristic - i.e if we have a function computing a score, make that score a feature</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>look at the inputs of a heuristic and feed them as features into the ML</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>modify the label, e.g multiply downloads by number of stars to make quality apps stand out more</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>See if there is a simpler way to incorporate old heuristics.</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="fu"># Monitering</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>Alerts! Dashboards!</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 8: freshness</span></span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how frequently does the model need to be updated? monitor and alert accordingly.</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 9: sanity check models before using</span></span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>make sure the model's perf is reasonable on validation data - e.g check area under <span class="co">[</span><span class="ot">roc curve</span><span class="co">](https://www.wikiwand.com/en/Receiver_operating_characteristic)</span>.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 10: watch for silent failures</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>Supid things happen, like part of the incoming data is being dropped somewhere. the ML system will keep chugging along as they adjust to feature changes and will decay gradually.</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>tracking statistics for data helps here, for example if a certain feature column has less populated data then before</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 11: document features and assign owners</span></span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>document all the features, duh</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>for larger systems, assign ownership and responsbilities</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a><span class="fu"># First Objectives</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 12: dont overthink which objective to optimize</span></span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a>Don't optimize a single metric early, initially all metrics we care about should typically be going up. Consider the overall 'health' of the ml process first.</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 13: Use a simple, observable and attributtable metric as a first objective</span></span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a>This should be straightward and serve as a proxy for the true objective, in fact, often there is no 'true' objective. Model direct things, leave indirect things for later or to a different layer altogether.</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 14: Start with an interpretable model to make debugging easier</span></span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a>Linear regression, logistic regression etc are straightforward to understand and debug vs the more complicated models. </span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 15: Seperate spam filtering and quality ranking</span></span>
<span id="cb1-104"><a href="#cb1-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-105"><a href="#cb1-105" aria-hidden="true" tabindex="-1"></a>Rank content seperately from how you determine what kind of content it is - for example have a different model for post quality score, and another one for categorzing spam.</span>
<span id="cb1-106"><a href="#cb1-106" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-107"><a href="#cb1-107" aria-hidden="true" tabindex="-1"></a>i.e don't do something overly simple like assuming posts with a low score are spam.</span>
<span id="cb1-108"><a href="#cb1-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-109"><a href="#cb1-109" aria-hidden="true" tabindex="-1"></a><span class="fu"># ML Phase II: Feature Engineering</span></span>
<span id="cb1-110"><a href="#cb1-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-111"><a href="#cb1-111" aria-hidden="true" tabindex="-1"></a>Phase 1 should give us a simple working end to end system with unit and system tests.</span>
<span id="cb1-112"><a href="#cb1-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-113"><a href="#cb1-113" aria-hidden="true" tabindex="-1"></a>In Phase 2 we pick all the low hanging fruit, like pulling in as many features are possible and combining them in intuitive ways.</span>
<span id="cb1-114"><a href="#cb1-114" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-115"><a href="#cb1-115" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 16: Launch and iterate</span></span>
<span id="cb1-116"><a href="#cb1-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-117"><a href="#cb1-117" aria-hidden="true" tabindex="-1"></a>As you build the model, keep adding, removing and recombining features and launching new models. Don't wait to do all the features in one go, iterate.</span>
<span id="cb1-118"><a href="#cb1-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-119"><a href="#cb1-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 17: Start with directly observed and reported features as opposed to learned features</span></span>
<span id="cb1-120"><a href="#cb1-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-121"><a href="#cb1-121" aria-hidden="true" tabindex="-1"></a>Learned features can be coming from elsewhere can have a lot of issues and should not be in the first model. These outside features come with their own objectives which could be only weakly correlated to our objective, and there is no gurantee they will lead to optimal solutions.</span>
<span id="cb1-122"><a href="#cb1-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-123"><a href="#cb1-123" aria-hidden="true" tabindex="-1"></a>We can get excellent baseline performance without deep deatures.</span>
<span id="cb1-124"><a href="#cb1-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-125"><a href="#cb1-125" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 18: Explore with features of content that generalizes across contexts</span></span>
<span id="cb1-126"><a href="#cb1-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-127"><a href="#cb1-127" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 19: User very specific features when you can</span></span>
<span id="cb1-128"><a href="#cb1-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-129"><a href="#cb1-129" aria-hidden="true" tabindex="-1"></a>with big data, its easier to learn millions of simple features than a few complex features. Consider making groups of features if each feature only applies to a small fraction of data, but the group coverage is &gt; 90%.</span>
<span id="cb1-130"><a href="#cb1-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-131"><a href="#cb1-131" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 20: combine and modify existing features to create new features in human understandable ways</span></span>
<span id="cb1-132"><a href="#cb1-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-133"><a href="#cb1-133" aria-hidden="true" tabindex="-1"></a>There are many ways to do this, for example turning an age feature into infant/child/teenager/adult/senior using age ranges. Don't overthink - basic quantiles gives the most impact. </span>
<span id="cb1-134"><a href="#cb1-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-135"><a href="#cb1-135" aria-hidden="true" tabindex="-1"></a>Also consider cross combining features like {male,female} with say a country feature like {US,Canada} to get a feature like (male, US). This takes a lot of data, but can be useful in some contexts, while overfitting in others.</span>
<span id="cb1-136"><a href="#cb1-136" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-137"><a href="#cb1-137" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 21: number of feature weights is roughly proportional to the amount of data</span></span>
<span id="cb1-138"><a href="#cb1-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-139"><a href="#cb1-139" aria-hidden="true" tabindex="-1"></a>There is a lot of stats theory about the appropriate level of complexity for a model, but this rule is the most important to know. Scale the learning to the size of your data.</span>
<span id="cb1-140"><a href="#cb1-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-141"><a href="#cb1-141" aria-hidden="true" tabindex="-1"></a>With small numbers of training data, you probably have to human engineer features. With larger data sets, regularization and feature selection helps to cut down features.</span>
<span id="cb1-142"><a href="#cb1-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-143"><a href="#cb1-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 22: clean up features no longer in use</span></span>
<span id="cb1-144"><a href="#cb1-144" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-145"><a href="#cb1-145" aria-hidden="true" tabindex="-1"></a>drop useless features. keep infrastructure clean so that promising features can be tried fast. Add features back if necesary. </span>
<span id="cb1-146"><a href="#cb1-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-147"><a href="#cb1-147" aria-hidden="true" tabindex="-1"></a>Consider the coverage of features, like how many examples are covered? If a feature only covers 8% of the data, it might be useless, but on the other hand a feature covering only 1% data could have a 90% prediction rate.</span>
<span id="cb1-148"><a href="#cb1-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-149"><a href="#cb1-149" aria-hidden="true" tabindex="-1"></a><span class="fu"># Human analysis of the System</span></span>
<span id="cb1-150"><a href="#cb1-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-151"><a href="#cb1-151" aria-hidden="true" tabindex="-1"></a>important to look at existing models and think about how to improve them.</span>
<span id="cb1-152"><a href="#cb1-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-153"><a href="#cb1-153" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 23: you're not a typical end user</span></span>
<span id="cb1-154"><a href="#cb1-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-155"><a href="#cb1-155" aria-hidden="true" tabindex="-1"></a>You're too close to the code and results, so its important to bring in outside users - from within the company, lay people or a crowdsourcing platform. </span>
<span id="cb1-156"><a href="#cb1-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-157"><a href="#cb1-157" aria-hidden="true" tabindex="-1"></a>Develop user experience methodologies, and create user personas and do usablity testing.</span>
<span id="cb1-158"><a href="#cb1-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-159"><a href="#cb1-159" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 24: measure the dela within models</span></span>
<span id="cb1-160"><a href="#cb1-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-161"><a href="#cb1-161" aria-hidden="true" tabindex="-1"></a>measure both results b/w different models, and also results with the same model to make sure its stable.</span>
<span id="cb1-162"><a href="#cb1-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-163"><a href="#cb1-163" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 25: utilitarian performance trumps predictive power</span></span>
<span id="cb1-164"><a href="#cb1-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-165"><a href="#cb1-165" aria-hidden="true" tabindex="-1"></a>the key question is what we are doing with the prediction - for example when ranking something the quality of the final ranking matters more than the quality of the prediction itself. But in a spam filter which has a cutoff on what is blocked, the precision matters more.</span>
<span id="cb1-166"><a href="#cb1-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-167"><a href="#cb1-167" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 26: look for patterns in the measured errors and create new features</span></span>
<span id="cb1-168"><a href="#cb1-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-169"><a href="#cb1-169" aria-hidden="true" tabindex="-1"></a>Say for some reason the system is demoting longer posts, then a post lengh feature could be useful. Don't overthink it - add relevant features and let the model figure it out.</span>
<span id="cb1-170"><a href="#cb1-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-171"><a href="#cb1-171" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 27: try to quantify observed behaviour</span></span>
<span id="cb1-172"><a href="#cb1-172" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-173"><a href="#cb1-173" aria-hidden="true" tabindex="-1"></a>if the model has 'bad' properties, like recommending gag apps too often, try to measure or get humans to add category labels and add as a feature.</span>
<span id="cb1-174"><a href="#cb1-174" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-175"><a href="#cb1-175" aria-hidden="true" tabindex="-1"></a><span class="at">&gt;  If your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is "measure first, optimize second".</span></span>
<span id="cb1-176"><a href="#cb1-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-177"><a href="#cb1-177" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 28: be aware that identical short-term behaviour does not imply identical long-term behaviour</span></span>
<span id="cb1-178"><a href="#cb1-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-179"><a href="#cb1-179" aria-hidden="true" tabindex="-1"></a>A model for say predicting apps will work great in practice but in production turn out to not recommend any new apps, since in practice there wasn't a way to learn that new apps should be shown too.</span>
<span id="cb1-180"><a href="#cb1-180" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-181"><a href="#cb1-181" aria-hidden="true" tabindex="-1"></a><span class="fu"># Training-Serving Skew</span></span>
<span id="cb1-182"><a href="#cb1-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-183"><a href="#cb1-183" aria-hidden="true" tabindex="-1"></a>this is the difference b/w performance during training and serving, typically caused by:</span>
<span id="cb1-184"><a href="#cb1-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-185"><a href="#cb1-185" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>difference in how data is handles in training and serving pipelines</span>
<span id="cb1-186"><a href="#cb1-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>change in data b/w training and serving</span>
<span id="cb1-187"><a href="#cb1-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>feedback loop b/w model and algorithim</span>
<span id="cb1-188"><a href="#cb1-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-189"><a href="#cb1-189" aria-hidden="true" tabindex="-1"></a>Best solution is to explicitly monitor things so system and data changes don't introduce skew unnoticed.</span>
<span id="cb1-190"><a href="#cb1-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-191"><a href="#cb1-191" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 29: best way to make sure you train like you serve is to save the set of features used at serving time and pipe those features to a log to use them at training time</span></span>
<span id="cb1-192"><a href="#cb1-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-193"><a href="#cb1-193" aria-hidden="true" tabindex="-1"></a>If not for all examples, log a fraction of the data.</span>
<span id="cb1-194"><a href="#cb1-194" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-195"><a href="#cb1-195" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 30: Importance-weight sampled data, don't arbitarily drop it</span></span>
<span id="cb1-196"><a href="#cb1-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-197"><a href="#cb1-197" aria-hidden="true" tabindex="-1"></a>don't arbitarily decide to take the first ten results, rather weight by importance - say if we want to sample X with a 30# probablity, give it a weight of 10/3.</span>
<span id="cb1-198"><a href="#cb1-198" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-199"><a href="#cb1-199" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 31: beware if you join data from a table at training and serving time, the data in the table may change</span></span>
<span id="cb1-200"><a href="#cb1-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-201"><a href="#cb1-201" aria-hidden="true" tabindex="-1"></a>b/w training and serving time, features in a table may be changed. Avoid this by logging features at training time.</span>
<span id="cb1-202"><a href="#cb1-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-203"><a href="#cb1-203" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 32: reuse code b/w training and serviing pipelines whenever possible</span></span>
<span id="cb1-204"><a href="#cb1-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-205"><a href="#cb1-205" aria-hidden="true" tabindex="-1"></a>While how data is arriving is different, requiring different code, try to transform initial data into a human readable object which can be tested and transformed by a common method for use by the machine learning system. Try to use the same programming language for training and serving.</span>
<span id="cb1-206"><a href="#cb1-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-207"><a href="#cb1-207" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 33: if you make a model on data till Jan 5th, test it on data after Jan 6th</span></span>
<span id="cb1-208"><a href="#cb1-208" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-209"><a href="#cb1-209" aria-hidden="true" tabindex="-1"></a>this better reflects how the model will do in production.</span>
<span id="cb1-210"><a href="#cb1-210" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-211"><a href="#cb1-211" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 34: in binary classification for filtering make small short-term sacrifices in performance for very clean data</span></span>
<span id="cb1-212"><a href="#cb1-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-213"><a href="#cb1-213" aria-hidden="true" tabindex="-1"></a>Say our spam filter blocks 75%, and we might be tempted to learn from the 25% which gets through and the user labels spam. This introduces sampling bias, we will get cleaner data by labelling 1% of traffic as "held-out" and send all those to the users, and we can than use those labelled samples as training data.</span>
<span id="cb1-214"><a href="#cb1-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-215"><a href="#cb1-215" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 35: Beware of inherent skew in ranking problems</span></span>
<span id="cb1-216"><a href="#cb1-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-217"><a href="#cb1-217" aria-hidden="true" tabindex="-1"></a>When we change a ranking algorithim so that different results show up, this effectively changes the data the algorithim is going to see in the future. Design algos around this:</span>
<span id="cb1-218"><a href="#cb1-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-219"><a href="#cb1-219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>have higher regularization for features that cover more queries, over features that are only for one query. This allows the model to favour features that are specific to one or a few queries, and helps from preventing very popular results into leaking into irrelevant quries.</span>
<span id="cb1-220"><a href="#cb1-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>only allow features to have positive weights</span>
<span id="cb1-221"><a href="#cb1-221" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>don't have document only features</span>
<span id="cb1-222"><a href="#cb1-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-223"><a href="#cb1-223" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 36: Aviod feedback loops with positional features</span></span>
<span id="cb1-224"><a href="#cb1-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-225"><a href="#cb1-225" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 37: Measure Training/Serving skew</span></span>
<span id="cb1-226"><a href="#cb1-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-227"><a href="#cb1-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>difference in performance on training and holdout data - will always exist, so doesn't necessarily mean something bad.</span>
<span id="cb1-228"><a href="#cb1-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>difference in performace on holdout data and the nextday data - large drops in perf here could indicate some features are time-sensitive and possibly degrading model perf.</span>
<span id="cb1-229"><a href="#cb1-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>difference in performance bw nextday day and live data. differences here could indicate model error.</span>
<span id="cb1-230"><a href="#cb1-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-231"><a href="#cb1-231" aria-hidden="true" tabindex="-1"></a><span class="fu"># ML Phase III: Slowed Growth, Optimization Refinement, and Complex Models</span></span>
<span id="cb1-232"><a href="#cb1-232" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-233"><a href="#cb1-233" aria-hidden="true" tabindex="-1"></a>The second phase is ending when gains are diminishing and we have tradeoffs b/w metrics.</span>
<span id="cb1-234"><a href="#cb1-234" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-235"><a href="#cb1-235" aria-hidden="true" tabindex="-1"></a>Since gains are harder to achieve, the machine learning has to get more sophisticated.</span>
<span id="cb1-236"><a href="#cb1-236" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-237"><a href="#cb1-237" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 38: don't waste time on new features if unaligined objectives have become the issue</span></span>
<span id="cb1-238"><a href="#cb1-238" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-239"><a href="#cb1-239" aria-hidden="true" tabindex="-1"></a>as measurements plateau, teams often look at issues outside the scope of the current machine learning system. If those goals aren't covered by the exisiting ml system, change the system or the goals.</span>
<span id="cb1-240"><a href="#cb1-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-241"><a href="#cb1-241" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 39: launch decisions are a proxy for long-term product goals</span></span>
<span id="cb1-242"><a href="#cb1-242" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-243"><a href="#cb1-243" aria-hidden="true" tabindex="-1"></a>For example, adding a feature might increase installs, but drop daily active use. Launch decisions depend on multiple factors, only some of which are optimizable by ML.</span>
<span id="cb1-244"><a href="#cb1-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-245"><a href="#cb1-245" aria-hidden="true" tabindex="-1"></a>Metrics are a proxy for more longterm goals.</span>
<span id="cb1-246"><a href="#cb1-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-247"><a href="#cb1-247" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 40: Keep ensembles simple</span></span>
<span id="cb1-248"><a href="#cb1-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-249"><a href="#cb1-249" aria-hidden="true" tabindex="-1"></a>each model should either be ensemble taking input of other models, or a base model taking in features, not both. Don't pile on models on top of models - this can result in bad behaviour.</span>
<span id="cb1-250"><a href="#cb1-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-251"><a href="#cb1-251" aria-hidden="true" tabindex="-1"></a>Make sure that in crease in the predicted probality of a underlying classifier doesn't decrease the predicted probability of the classifier.</span>
<span id="cb1-252"><a href="#cb1-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-253"><a href="#cb1-253" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.</span></span>
<span id="cb1-254"><a href="#cb1-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-255"><a href="#cb1-255" aria-hidden="true" tabindex="-1"></a>add more relevant signals, build out infrastructure for radically different features, use more deep learning, weigh benefits of new features against increased complexity.</span>
<span id="cb1-256"><a href="#cb1-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-257"><a href="#cb1-257" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.</span></span>
<span id="cb1-258"><a href="#cb1-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-259"><a href="#cb1-259" aria-hidden="true" tabindex="-1"></a>popularity is easy to measure, diversity, personalization and relavance is harder - adding features for these can turn out to not work very well, i.e they get less weight in the model.</span>
<span id="cb1-260"><a href="#cb1-260" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-261"><a href="#cb1-261" aria-hidden="true" tabindex="-1"></a>Postprocessing can help, or directly modifing the objective to increase diversity or relevance.</span>
<span id="cb1-262"><a href="#cb1-262" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-263"><a href="#cb1-263" aria-hidden="true" tabindex="-1"></a><span class="fu">## Rule 43: Your friends tend to be the same across different products. Your interests tend not to be.</span></span>
<span id="cb1-264"><a href="#cb1-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-265"><a href="#cb1-265" aria-hidden="true" tabindex="-1"></a>While one thing might be close to another, doesn't mean it applies across products. </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>