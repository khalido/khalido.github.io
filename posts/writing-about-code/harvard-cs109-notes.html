<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KO">
<meta name="dcterms.date" content="2017-04-04">
<meta name="description" content="Notes as I worked through CS109.">

<title>khalido.org - Notes for Harvard’s Cs109 data science class</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">khalido.org</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/khalido"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/KO"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Notes for Harvard’s Cs109 data science class</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          Notes as I worked through CS109.
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">python</div>
                <div class="quarto-category">data science</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>KO </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 4, 2017</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#cs109-notes-by-the-class-schedule" id="toc-cs109-notes-by-the-class-schedule" class="nav-link active" data-scroll-target="#cs109-notes-by-the-class-schedule">CS109 notes, by the class schedule:</a>
  <ul class="collapse">
  <li><a href="#week-1-what-is-data-science" id="toc-week-1-what-is-data-science" class="nav-link" data-scroll-target="#week-1-what-is-data-science">Week 1: What is Data Science</a></li>
  <li><a href="#week-2-intro-data-analysis-and-viz" id="toc-week-2-intro-data-analysis-and-viz" class="nav-link" data-scroll-target="#week-2-intro-data-analysis-and-viz">Week 2: Intro Data Analysis and Viz</a></li>
  <li><a href="#week-3-databases-sql-and-more-pandas" id="toc-week-3-databases-sql-and-more-pandas" class="nav-link" data-scroll-target="#week-3-databases-sql-and-more-pandas">Week 3 : Databases, SQL and more Pandas</a></li>
  <li><a href="#week-4-probablity-regression-and-some-stats" id="toc-week-4-probablity-regression-and-some-stats" class="nav-link" data-scroll-target="#week-4-probablity-regression-and-some-stats">Week 4: Probablity, regression and some stats</a>
  <ul class="collapse">
  <li><a href="#lecture-6-story-telling-and-effective-communication-slides-video" id="toc-lecture-6-story-telling-and-effective-communication-slides-video" class="nav-link" data-scroll-target="#lecture-6-story-telling-and-effective-communication-slides-video">Lecture 6: Story Telling and Effective Communication (slides, video)</a></li>
  <li><a href="#lecture-7-bias-and-regression-slides-video" id="toc-lecture-7-bias-and-regression-slides-video" class="nav-link" data-scroll-target="#lecture-7-bias-and-regression-slides-video">Lecture 7: Bias and Regression (slides, video)</a></li>
  </ul></li>
  <li><a href="#week-5-scikit-learn-regression" id="toc-week-5-scikit-learn-regression" class="nav-link" data-scroll-target="#week-5-scikit-learn-regression">Week 5: Scikit learn &amp; regression</a>
  <ul class="collapse">
  <li><a href="#lab-4---regression-in-python-video-notebook" id="toc-lab-4---regression-in-python-video-notebook" class="nav-link" data-scroll-target="#lab-4---regression-in-python-video-notebook">Lab 4 - Regression in Python (video, notebook)</a></li>
  <li><a href="#lecture-8-more-regression-video-slides" id="toc-lecture-8-more-regression-video-slides" class="nav-link" data-scroll-target="#lecture-8-more-regression-video-slides">Lecture 8: More Regression (video, slides)</a></li>
  <li><a href="#lecture-9-classification-video-slides" id="toc-lecture-9-classification-video-slides" class="nav-link" data-scroll-target="#lecture-9-classification-video-slides">Lecture 9: Classification (video, slides)</a></li>
  <li><a href="#hw2-q1-notebook" id="toc-hw2-q1-notebook" class="nav-link" data-scroll-target="#hw2-q1-notebook">HW2 Q1 (notebook)</a></li>
  </ul></li>
  <li><a href="#week-6-svm-trees-and-forests" id="toc-week-6-svm-trees-and-forests" class="nav-link" data-scroll-target="#week-6-svm-trees-and-forests">Week 6: SVM, trees and forests</a>
  <ul class="collapse">
  <li><a href="#hw-2-questions-23-4-notebook" id="toc-hw-2-questions-23-4-notebook" class="nav-link" data-scroll-target="#hw-2-questions-23-4-notebook">HW 2 Questions 2,3 &amp; 4 (notebook)</a></li>
  <li><a href="#lab-5-machine-learning" id="toc-lab-5-machine-learning" class="nav-link" data-scroll-target="#lab-5-machine-learning">Lab 5: Machine Learning</a></li>
  <li><a href="#lecture-10-svm-evaluation-video-slides" id="toc-lecture-10-svm-evaluation-video-slides" class="nav-link" data-scroll-target="#lecture-10-svm-evaluation-video-slides">Lecture 10: SVM, Evaluation (video, slides)</a></li>
  <li><a href="#lecture-11-decision-trees-and-random-forests-video-slides" id="toc-lecture-11-decision-trees-and-random-forests-video-slides" class="nav-link" data-scroll-target="#lecture-11-decision-trees-and-random-forests-video-slides">Lecture 11: Decision Trees and Random Forests (video, slides)</a></li>
  </ul></li>
  <li><a href="#week-7-machine-learning-best-practices" id="toc-week-7-machine-learning-best-practices" class="nav-link" data-scroll-target="#week-7-machine-learning-best-practices">Week 7: Machine Learning best practices</a>
  <ul class="collapse">
  <li><a href="#lab-6-machine-learning-2" id="toc-lab-6-machine-learning-2" class="nav-link" data-scroll-target="#lab-6-machine-learning-2">Lab 6: Machine Learning 2</a></li>
  <li><a href="#lecture-12-ensemble-methods-video-slides" id="toc-lecture-12-ensemble-methods-video-slides" class="nav-link" data-scroll-target="#lecture-12-ensemble-methods-video-slides">Lecture 12: Ensemble Methods ((video, slides))</a></li>
  <li><a href="#lecture-13-best-practices-video-slides" id="toc-lecture-13-best-practices-video-slides" class="nav-link" data-scroll-target="#lecture-13-best-practices-video-slides">Lecture 13: Best Practices (video, slides)</a></li>
  </ul></li>
  <li><a href="#week-8-ec2-and-spark" id="toc-week-8-ec2-and-spark" class="nav-link" data-scroll-target="#week-8-ec2-and-spark">Week 8: EC2 and Spark</a>
  <ul class="collapse">
  <li><a href="#lab-7-decision-trees-random-forests-ensemble-methods-video-notebook" id="toc-lab-7-decision-trees-random-forests-ensemble-methods-video-notebook" class="nav-link" data-scroll-target="#lab-7-decision-trees-random-forests-ensemble-methods-video-notebook">Lab 7: Decision Trees, Random Forests, Ensemble Methods (video, notebook)</a></li>
  <li><a href="#lecture-14-best-practices-recommendations-and-mapreduce-video-slides" id="toc-lecture-14-best-practices-recommendations-and-mapreduce-video-slides" class="nav-link" data-scroll-target="#lecture-14-best-practices-recommendations-and-mapreduce-video-slides">Lecture 14: Best Practices, Recommendations and MapReduce (video, slides)</a></li>
  <li><a href="#lecture-15-mapreduce-combiners-and-spark-video-slides" id="toc-lecture-15-mapreduce-combiners-and-spark-video-slides" class="nav-link" data-scroll-target="#lecture-15-mapreduce-combiners-and-spark-video-slides">Lecture 15: MapReduce Combiners and Spark (video, slides)</a></li>
  </ul></li>
  <li><a href="#week-9-bayes" id="toc-week-9-bayes" class="nav-link" data-scroll-target="#week-9-bayes">Week 9: Bayes!</a>
  <ul class="collapse">
  <li><a href="#lab-8-vagrant-and-virtualbox-aws-and-spark-video-notebook" id="toc-lab-8-vagrant-and-virtualbox-aws-and-spark-video-notebook" class="nav-link" data-scroll-target="#lab-8-vagrant-and-virtualbox-aws-and-spark-video-notebook">Lab 8: Vagrant and VirtualBox, AWS, and Spark (video, notebook)</a></li>
  <li><a href="#lecture-16-bayes-theorem-and-bayesian-methods-video-slides" id="toc-lecture-16-bayes-theorem-and-bayesian-methods-video-slides" class="nav-link" data-scroll-target="#lecture-16-bayes-theorem-and-bayesian-methods-video-slides">Lecture 16: Bayes Theorem and Bayesian Methods (video, slides)</a></li>
  <li><a href="#lecture-17-bayesian-methods-continued-video-slides" id="toc-lecture-17-bayesian-methods-continued-video-slides" class="nav-link" data-scroll-target="#lecture-17-bayesian-methods-continued-video-slides">Lecture 17: Bayesian Methods Continued (video, slides)</a></li>
  </ul></li>
  <li><a href="#week-10-text" id="toc-week-10-text" class="nav-link" data-scroll-target="#week-10-text">Week 10: Text</a></li>
  <li><a href="#week-11-clustering" id="toc-week-11-clustering" class="nav-link" data-scroll-target="#week-11-clustering">Week 11: Clustering!</a></li>
  <li><a href="#week-12-deep-learning" id="toc-week-12-deep-learning" class="nav-link" data-scroll-target="#week-12-deep-learning">Week 12: Deep Learning</a></li>
  <li><a href="#week-13-final-project-wrapup" id="toc-week-13-final-project-wrapup" class="nav-link" data-scroll-target="#week-13-final-project-wrapup">Week 13: Final Project &amp; Wrapup</a></li>
  </ul></li>
  <li><a href="#additional-resources" id="toc-additional-resources" class="nav-link" data-scroll-target="#additional-resources">Additional Resources</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Notes for Harvard’s Cs109 data science class</p>
<p>These are my notes for <a href="http://cs109.github.io/2015/">Harvard’s 2015 CS109 class</a>, which i went through with Sydney Machine Learning’s <a href="https://sydneymachinelearningblog.wordpress.com/cs-109-study-group/">study group</a> from Ausgust to October 2017 at Amazon Web Service’s sydney office.</p>
<p><strong>Why CS109?</strong> This class was recommended at Quora and a few other places as being a good resource for practical data science, so here goes. These notes are updated as I work through the <a href="http://cs109.github.io/2015/pages/schedule.html">course syllabus</a> and the <a href="https://porter.io/github.com/cs109/content">labs and homeworks</a>.</p>
<p>The stuff to watch and work through: - <a href="http://cs109.github.io/2015/pages/videos.html">Lecture videos &amp; notes</a></p>
<p>Note: download the videos using <a href="https://github.com/christopher-beckham/cs109-dl-videos">this script</a>, and merge <a href="https://github.com/christopher-beckham/cs109-dl-videos/pull/11">pull request 11</a> to get the 2015 videos.</p>
<p><strong>Study Suggestions before starting:</strong></p>
<ul>
<li>Use a internet blocker app like <a href="https://selfcontrolapp.com/">SelfControl</a> to stop procrastinating and a <a href="http://tomighty.org/">pomodoro app</a> to break up study into chunks.</li>
<li>Use a paper notebook for notes as you watch the videos and do the labs.</li>
<li>Get a second monitor so you can watch videos/have lab notebooks open and work through at the same time. (I got a 1440p 27inch monitor from ebay and it made things so much easier from just using my laptop).</li>
<li>Don’t look at the lab and hw answers - try to do them first on your own. Discuss with others before looking at solutions.</li>
</ul>
<section id="cs109-notes-by-the-class-schedule" class="level1">
<h1>CS109 notes, by the class schedule:</h1>
<section id="week-1-what-is-data-science" class="level2">
<h2 class="anchored" data-anchor-id="week-1-what-is-data-science">Week 1: What is Data Science</h2>
<p><a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/01-Introduction.pdf">Lecture 1</a> introduces data science. The basic stuff covered in every blog post.</p>
</section>
<section id="week-2-intro-data-analysis-and-viz" class="level2">
<h2 class="anchored" data-anchor-id="week-2-intro-data-analysis-and-viz">Week 2: Intro Data Analysis and Viz</h2>
<p>The <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScraping.ipynb">lecture 2 notebook</a> goes through getting data and putting it into a pandas dataframe.</p>
<p>Lab 1 has three very introductory notebooks: <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-pythonpandas.ipynb">pythonpandas</a>, followed by <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-babypython.ipynb">babypython</a>, and finally <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-git.ipynb">git</a>. However, since the course dates back to 2015, some of the python is a bit dated and uses 2.x code.</p>
<p>After doing the three intro notebooks, <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/hw0.ipynb">hw0</a> runs you through installing anaconda, git, and setting up a github and aws account.</p>
<p>Hw0 has one interesting section, where you solve the <a href="https://en.wikipedia.org/wiki/Monty_Hall_problem">montyhall problem</a> one step at a time. I didn’t really like their steps, so made a <a href="https://github.com/khalido/algorithims/blob/master/monty_hall.ipynb">simpler monty hall implementation</a>.</p>
<p>Moving on to the <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScraping.ipynb">Lecture 2</a> &amp; its <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScrapingQuizzes.ipynb">quiz notebook</a>, this goes through some more pandas and data scraping web pages and parsing them.</p>
<p>I made a couple of notebooks expand on some of the stuff covered:</p>
<ul>
<li><a href="https://github.com/khalido/cs109-2015/blob/master/movielens.ipynb">movielens notebook for basic pandas</a> workflow of downloading a zip file, extracting it and putting into pandas dataframes and doing some q&amp;a</li>
<li><a href="https://github.com/khalido/cs109-2015/blob/master/twitter.ipynb">twitter notebook</a> - basic usage of twitter api and doing something with tweets</li>
</ul>
<p><strong>Lecture 3</strong> (<a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/03-EDA.pdf">slides</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=a4e81697-fd86-415c-9b29-c14ea7ec15f2">video</a>):</p>
<ul>
<li>ask a q, get data to answer it, explore &amp; check data, then model it and finally communicate and visualize the results.</li>
<li>keep viz simple and think of the <a href="http://extremepresentation.typepad.com/blog/files/choosing_a_good_chart.pdf">kind of chart</a> needed.</li>
</ul>
</section>
<section id="week-3-databases-sql-and-more-pandas" class="level2">
<h2 class="anchored" data-anchor-id="week-3-databases-sql-and-more-pandas">Week 3 : Databases, SQL and more Pandas</h2>
<p><a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab2/Lab2.ipynb">Lab 2</a> introduces web scraping with <a href="http://docs.python-requests.org/en/master/">requests</a> and then parsing html with <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">beautiful soup 4</a>.</p>
<p>Lecture 4 (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f8a832cb-56e7-401b-b485-aec3c9928069">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/04-PandasSQL.pdf">slides</a>) (covers some more <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/Lecture4/PandasAndSQL.ipynb">Pandas and SQL</a>.</p>
<p>Lecture 5 (<a href="https://github.com/cs109/2015/raw/master/Lectures/05-StatisticalModels.pdf">slides</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=873964c6-d345-4f46-a8bc-727b96432d63">video</a>) on stats is a bit sparse. Some supplementary material:</p>
<ul>
<li><a href="https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/info">Stanford Statistics Course</a> - check on this one vs the MIT one.</li>
<li><a href="http://greenteapress.com/thinkstats2/index.html">Think Stats</a> is a good basic book covering stats using Python.</li>
<li><a href="http://greenteapress.com/wp/think-bayes/">Think Bayes</a> follows on from Think Stats and covers Bayesian stats in Python.</li>
</ul>
</section>
<section id="week-4-probablity-regression-and-some-stats" class="level2">
<h2 class="anchored" data-anchor-id="week-4-probablity-regression-and-some-stats">Week 4: Probablity, regression and some stats</h2>
<p><a href="https://github.com/khalido/cs109-2015/tree/master/Labs/2015lab3">Lab 3</a> has three notebooks: - <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab3/Lab3-probability.ipynb">Lab3-Probability</a> covers basic probability. Uses a lot of numpy methods, so its a good idea to brush up on numpy. - <a href="https://docs.scipy.org/doc/scipy/reference/stats.html">scipy.stats</a> - very handy, has most stats stuff needed for DS built in. - <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab3/Lab3-Freq.ipynb">Lab3-Frequentism, Samples and the Bootstrap</a> - use seaborn for plotting, very handy. a <a href="http://blog.insightdatalabs.com/advanced-functionality-in-seaborn/">good guide to sns factorplot and facetgrids</a> - <a href="https://en.wikipedia.org/wiki/Probability_density_function">PDF</a> tells us the probability of where a continuus random variable will be in set of possible values that random variable can be (the sample space). - <a href="https://en.wikipedia.org/wiki/Probability_mass_function">PMF</a> tells us the probability that a discrete random variable will be ecactly equal to some value - <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function">CDF</a> function tells us the probability that a random discrete or continous variable X will take a value less than or equal to X. <a href="https://youtu.be/bGS19PxlGC4?list=PLF8E9E4FDAAA8018A">Video</a></p>
<section id="lecture-6-story-telling-and-effective-communication-slides-video" class="level3">
<h3 class="anchored" data-anchor-id="lecture-6-story-telling-and-effective-communication-slides-video">Lecture 6: Story Telling and Effective Communication (<a href="https://github.com/cs109/2015/raw/master/Lectures/06-StoryTelling.pdf">slides</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7">video</a>)</h3>
<p>Good insights on how to tell a story with data. Infer, model, use an algorithim and draw conclusions (and check!).</p>
<ul>
<li>Start with two fundamental questions:
<ul>
<li>Whats the goal? think first of that rather than going first to all the many ways you can slice and dice data.</li>
<li>Who cares? Know your audience and tell them a story. Have a clear sense of direction and logic.</li>
</ul></li>
<li>Read some howto’s on scientific writing</li>
<li>have some memorable examples or small stories</li>
</ul>
<p>Tell a story:</p>
<ul>
<li>know your audience and why/what they care about this data - what do they want?</li>
<li>Don’t make them think - clearly tell them what you want them to know in a way they can follow. highlight key facts and insights.</li>
<li>unexpectedness - show something the audience didn’t expect. I liked the story which highlighted that bigfoot sightings are dropping sharply</li>
<li>What tools can we give the audience? For example, a web app for them to further explore the data, or a takeaway presentation with key points.</li>
<li>be careful of your point of view and don’t distort the data, but depending on the audience you can frame your story - for example presenting war deaths in red rather than a regular plot color.</li>
<li>important to put the message up front - what does my graph show? Show it in stages if a lot of data, highlighting what to look at. Design matters.</li>
</ul>
<p>More resources:</p>
<ul>
<li><a href="http://www.thefunctionalart.com/">The Functional Art</a></li>
</ul>
</section>
<section id="lecture-7-bias-and-regression-slides-video" class="level3">
<h3 class="anchored" data-anchor-id="lecture-7-bias-and-regression-slides-video">Lecture 7: Bias and Regression (<a href="https://github.com/cs109/2015/raw/master/Lectures/07-BiasAndRegression.pdf">slides</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7">video</a>)</h3>
<ul>
<li>think about bias, missing data, etc</li>
<li>combine independent, unbiased estimators for a parameter into one:
<ul>
<li>fisher weighting</li>
<li>nate silver weighting method</li>
</ul></li>
<li>Bonferroni</li>
<li>good segment on weighting variables</li>
<li>regression towards the mean</li>
<li>think of regression in terms of population or a projection of the column space of x - i.e what combination of the variables of x gets us closest to the value of y?</li>
<li>linear regression means we’re taking linear combination of predictors, the actual regression equation can be nonlinear</li>
<li>what function of x gives the best predictor of y?</li>
<li>Gauss-Markov Theorem</li>
<li>the residuals are the diff b/w the actual value of y and the predicted value - plot residuals vs fitted values and vs each predictor variable - good way to eyeball quality of linear regression model</li>
<li>variance R^2 measures goodness of fit, but doesn’t mean model is good.</li>
<li>Best way to check a model is prediction.</li>
</ul>
</section>
</section>
<section id="week-5-scikit-learn-regression" class="level2">
<h2 class="anchored" data-anchor-id="week-5-scikit-learn-regression">Week 5: Scikit learn &amp; regression</h2>
<section id="lab-4---regression-in-python-video-notebook" class="level3">
<h3 class="anchored" data-anchor-id="lab-4---regression-in-python-video-notebook">Lab 4 - Regression in Python (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=483c8b93-3700-4ee8-80ed-aad7f3da7ac2">video</a>, <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab4/Lab4-stats.ipynb">notebook</a>)</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Linear_regression">Wikipeda article</a></li>
<li>We have data X, which is related to Y in some way.</li>
<li>Linear regression uses X to predict Y, and also tells us the predictive power of each variable of X</li>
<li>Linear regression assumes the distribution of each Y is normal (which isn’t always so)</li>
<li>there are many ways to fit a linear regression model, most common is the <a href="http://en.wikipedia.org/wiki/Least_squares">least squares</a> method</li>
<li>be careful that the features (variables in X) aren’t too similar</li>
<li>explore your data, plot variables, scatterplots etc. Use seaborn to plot regression.</li>
<li>use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html">sklearn to split dataset into a train/test</a></li>
<li>use <a href="http://scikit-learn.org/stable/modules/cross_validation.html">cross-validation</a></li>
<li>overfitting happens when the model ‘learns’ the train data so performs better on that than the test dataset</li>
<li>there are <a href="http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use">many types of regressions</a> so think about which one to use</li>
<li>for high d data, closed form vs gradient decent:
<ul>
<li>closed form - use math to solve. this becomes computationally intensive very quickly, is ordered n cubed</li>
<li>gradient descent is O(n), so for large high d data it’s gradient descent all the way</li>
</ul></li>
<li>Logistic regression - used where outcome is binary, for example a chance of success/failure. read:<a href="http://adit.io/posts/2016-03-13-Logistic-Regression.html">Adit’s explanation.</a></li>
</ul>
</section>
<section id="lecture-8-more-regression-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-8-more-regression-video-slides">Lecture 8: More Regression (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=664f668e-e008-4f44-8600-e09ee6d629b0">video</a>, <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/08-RegressionContinued.pdf">slides</a>)</h3>
<ul>
<li>collinerarity - when some variables are highly correlated with each other - this is bad</li>
<li>Logistic Regression</li>
<li>Odds Ratio: ratio of two different people’s odds of an outcome.</li>
<li>Crude Odds Ratio Estimate - quick estimate but flawed as doesn’t control for anything.</li>
<li>Confounding Factors - i.e is one group pre-disposed to our outcome for some reason?</li>
<li>Curse of dimensionality - in high d settings, vast majority of data is near boundaries, not center. But, high d can also be a <a href="http://andrewgelman.com/2004/10/27/the_blessing_of/">blessing</a>.</li>
<li>dealing with high dimensionality: ridge regression, shrinkage estimation</li>
<li>Stein’s Paradox <a href="https://en.wikipedia.org/wiki/Stein%27s_example">wikipedia</a>, <a href="http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf">good article</a></li>
<li>LASSO and Ridge help with high D data by reducing features
<ul>
<li>Lasso does L1 regularization, reduces number of features</li>
<li>Ridge does L2 regularization, doesn’t necessarily reduce features but reduces the impace of features on the model by reducing coefficient value</li>
</ul></li>
<li>Elasticnet does both L1 and L2 regularization</li>
</ul>
</section>
<section id="lecture-9-classification-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-9-classification-video-slides">Lecture 9: Classification (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=c322c0d5-9cf9-4deb-b59f-d6741064ba8a">video</a>, <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/09-ClassificationPCA.pdf">slides</a>)</h3>
<ul>
<li>we take data and assign labels</li>
<li>1 nearest neighbour - simple classification method for low d data
<ul>
<li>slow, has to check all points to find nearest neighbour</li>
</ul></li>
<li>k nearest neighbours - use k nearest points to find decision boundary
<ul>
<li>find ideal k</li>
<li>what distance function to use?</li>
<li>my own very simple <a href="https://github.com/khalido/algorithims/blob/master/k%20nearest%20neighbours.ipynb">kNN algo implementation</a></li>
</ul></li>
<li>cross validation - for 5 fold cross validation, the data is split into 6 folds - 4 for training, one for validation and the sixth for testing, which is only used at the end.</li>
<li>CIFAR-10 for 60K images - is split into 50K training and 10K test
<ul>
<li>pics are 32x32x3</li>
</ul></li>
<li>L1 distance is the absolute diff b/w two vectors</li>
<li>L2 is the Euclidean distance i.e “ordinary” straight-line distance</li>
<li>for images, l1 and l2 are pretty bad, so there are a lot more methods</li>
<li>more features are good for classification, but too many features means the data gets sparse - the curse of dimensionality strikes</li>
<li>so often we want to reduce dimensionality</li>
<li>Principal Component Analysis - project a dataset from many variables into fewer less correlated ones, called the principal components.</li>
<li>Singular Value Decomposition (SVD) - computational method to calculate pricipal components of a dataset. It transforms a large matrix of data into three smallter matrixes: <code>A (m*n) = U(m*r) x E(r*r) x V(r*n)</code>. The values in the middle matrix <code>r*r</code> are the <em>singular</em> values and we can discard bits of them to reduce the amount of data to a more manageable number.</li>
<li><a href="https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294">good pca and svd explanation</a></li>
<li>Watch <a href="https://www.youtube.com/watch?v=Iq9DzN6mvYA">Statistics for Hackers</a></li>
</ul>
</section>
<section id="hw2-q1-notebook" class="level3">
<h3 class="anchored" data-anchor-id="hw2-q1-notebook">HW2 Q1 (<a href="https://github.com/khalido/cs109-2015/blob/master/homework/HW2.ipynb">notebook</a>)</h3>
<ul>
<li>Uses svd and pca to analyze gene data</li>
<li>a pandas excercise in downloading csv files into a data frame, usijng pd.datetime and visualising samples vs time</li>
</ul>
</section>
</section>
<section id="week-6-svm-trees-and-forests" class="level2">
<h2 class="anchored" data-anchor-id="week-6-svm-trees-and-forests">Week 6: SVM, trees and forests</h2>
<p>Now the course finally gets interesting. Before starting this weeks work, think about project ideas and see <a href="https://www.gapminder.org/videos/">Hans Rosling</a> videos to see how to present data. Pitch this project idea (to study group or the internet at large).</p>
<p>There are quite a few companies automating the entire datascience chain, so the key is being able to present your findings well.</p>
<section id="hw-2-questions-23-4-notebook" class="level3">
<h3 class="anchored" data-anchor-id="hw-2-questions-23-4-notebook">HW 2 Questions 2,3 &amp; 4 (<a href="https://github.com/khalido/cs109-2015/blob/master/homework/HW2.ipynb">notebook</a>)</h3>
<p>H2 depends wholly on week 5, so good idea to get it done first. Used seaborn for all the viz questions makes some of them trivial.</p>
<ul>
<li>q2 looks at polling data and bias</li>
<li>q3 is more of the same but with seaborn</li>
<li>q4 nothing much to see here besides using list comprehensions to make a list of all the .csv files <em>(I’m trying to use python to do all the work instead of my stone age past life of copying and pasting links)</em></li>
</ul>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>url_str <span class="op">=</span> <span class="st">"http://elections.huffingtonpost.com/pollster/api/charts/?topic=2014-senate"</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>election_urls <span class="op">=</span> [election[<span class="st">'url'</span>] <span class="op">+</span> <span class="st">'.csv'</span> <span class="cf">for</span> election <span class="kw">in</span> requests.get(url_str).json()]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="lab-5-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="lab-5-machine-learning">Lab 5: Machine Learning</h3>
<p><strong>Learning Models</strong> (<a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab5/LearningModels.ipynb">notebook</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=e509f996-9633-4b75-a48a-e29246a316db">video</a>)</p>
<ul>
<li>we often have a small sample of a much dataset, and we want to predict the larger data from our sample.</li>
<li>this isn’t just statistical analysis, as we make models which involve domain knowledge and choices.</li>
<li>need to think about whether our sample is in some way representative of the population</li>
<li>Stochastic noise, i.e randomness</li>
<li>systematic error, i.e where the sampling isn’t representative, like polling ppl using landlines</li>
<li>overfitting: models can ’memrize the the data points in the training set, becoming useless or inaccurate at predicting real world data. With many data sets a more and more complex dataset will keep getting better while getting worse on test/validation data. The best model minimizes test set error, and not training set error.</li>
<li>great illustration of variance at 24:30 and 35min in the video</li>
<li>use <code>from sklearn.cross_validation import train_test_split</code> for splitting datasets into a train test split. See <a href="http://scikit-learn.org/stable/">sklearn</a></li>
<li>sklearn has 3 main features:
<ul>
<li>build and fit models</li>
<li>predict</li>
<li>transform data.</li>
</ul></li>
<li>sklearn expects days in a 2d array or a matrix of size <code>[n_samples, n_features]</code>, so reshape 1d data using <code>np.reshape(-1,1)</code></li>
<li>Validation - keep a chunk of data seperate to check the model after training on the test/train data.</li>
<li>Cross Validation: randomly split data into K diff train/test splits - so you traion on K-1 partitions and test on 1, so there are a total of K combinations, leading to K risks. This leads to better results then just doing one test/train split.</li>
<li>regularization helps with overfitting</li>
</ul>
<p><strong>Classification</strong> (<a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab5/Classification.ipynb">notebook</a>, <a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=90e73c64-855c-4b06-afb2-94da608ecfbf">video</a>)</p>
<ul>
<li>sort data into classes, i.e what kind of fruit</li>
<li>most datasets can be projected onto a lower dimensial space, for e.g using PCA</li>
<li>read sklearn’s PCA docs</li>
<li>kNN:</li>
<li>Logistic Regression - use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn</a>, main paramater is C which defines how much to regularize the data. Read <a href="http://adit.io/posts/2016-03-13-Logistic-Regression.html">this explanation</a></li>
<li>Use sklearns <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a> to find hyperparameters</li>
<li>one way to classify: use <a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">PCA</a> to reduce the feature space, then use logistic regression to classify</li>
<li>many datatypes, like images, have tons of features, so important to reduce dimensionality.</li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html">sklearn PCA</a> returns the principal components in order of how much they explain the data:</li>
</ul>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">60</span>) <span class="co"># PCA with no. of components to return</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pca.fit_transform(data)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca.explained_variance_ratio_) <span class="co"># how much of variance is explained</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<ul>
<li>sklearn uses the same interface for all its classifiers, so makes it easy to put a wrapper around gridsearchCV and pass in diff classifiers to compare.</li>
<li>discriminative classifier - finds the decision boundary b/w classes</li>
<li>maximum-margin classifier - for many classifincation problems, multiplie diff lines can seperate classes. Choose that line where the margin b/w classes is the largest, which makes this model senstive to boundaries b/w classes, rather than to point samples deep inside a class.</li>
<li>SVM is a discrimant classier which finds the widest possible margin b/w classes, including some points touching the boundary which are called the support vectors. (since they support or establish the margins.)</li>
</ul>
</section>
<section id="lecture-10-svm-evaluation-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-10-svm-evaluation-video-slides">Lecture 10: SVM, Evaluation (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f21fcc8f-93a8-49f6-9ff8-0f339b0728bd">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/10-SVMAndEvaluation.pdf">slides</a>)</h3>
<ul>
<li>KNN - training is fast, prediction slow since we need to check all the data points to find the nearest neighbours</li>
<li>but if we know the decision boundary (the seperating hyperplane) we don’t need all the data points
<ul>
<li>w: weight vector defines the orientation of the hyperplane, and bias b.</li>
<li>so a new point x is classified by <code>w(transpose)*x + b</code></li>
<li>this is the mathematical model of a neuron, invented 1957 by Rosenblatt</li>
</ul></li>
<li>step function vs sigmoid activation</li>
<li>Support Vector Machines (SVM) are widely used, some consider it best of the shelf classifier. They add a new dimension to help seperate classes and also use maximum margin classification. SVM is called svm becuase of the support vectors defining the max margin lines for the classification boundary.</li>
<li>large data is good for training svm as the points on the boundary are rare and svm cares about establishing the boundary</li>
<li>since outliers can change the svm boundaries, there is a concept of slack variables - it allows the SVM to missclassify outliers to make a neat decision boundary. sklearn uses the parameter C to define the slack. the lower the number the more the slack.</li>
<li>kernel tricks for svm - go to aribitarily mary dimensions with little computational cost. need to think about what kernel to use. Read <a href="https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them">What are kernels in machine learning and SVM and why do we need them?</a>.</li>
<li>read <a href="http://cs229.stanford.edu/notes/cs229-notes3.pdf">Andrew Ng’s cs229 svm notes</a></li>
<li>todo: tale sklearns ‘faces’ dataset and use svm to predict</li>
<li>svm tips:
<ul>
<li>normalize data to 0,1 or -1,1 interval. (check whether the library already normalizes)</li>
<li>RBF kernel is a good default</li>
<li>Read Chich-Wei Hsu practical guide to SVM</li>
<li>tune paramaters - which kernel, what parameters for it and what C?</li>
</ul></li>
<li>ROC curve: plot true positive rate vs false positive rate
<ul>
<li>true +ve is tp/(tp+fn)</li>
<li>false +ve is fp/(fp+tn)</li>
<li>one useful summary stat is area under the curve</li>
</ul></li>
<li>Precision Recall Curve <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html">sklearn</a>, <a href="https://www.quora.com/What-is-Precision-Recall-PR-curve">quora</a>
<ul>
<li>Precision: <code>tp/(tp+fp)</code> - how much are we getting right, or the probability a a random +ve sample is actually +ve (since some +ve samples are false positives).</li>
<li>Recall <code>tp/(tp+fn)</code> - same as in the ROC, how much of the data are we finding. for a random +ve sameple, the probability that it’s making a correct prediction. (consider the false negatives.)</li>
<li>ideally we want both to be one.</li>
</ul></li>
<li>good way to compare classifiers</li>
<li>How do you classify multple classes with a SVM, e.g 10 types of fruit
<ul>
<li>One vs all - pick one class, and train it against all the other classes one by one. So you train n classifers for n classes.</li>
<li>One vs One - Train n(n-1)/2 classifiers, take majority vote</li>
<li>use a confusion matrix of predicted label vs true labels to see classification results</li>
</ul></li>
</ul>
</section>
<section id="lecture-11-decision-trees-and-random-forests-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-11-decision-trees-and-random-forests-video-slides">Lecture 11: Decision Trees and Random Forests (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=8892a8b7-25eb-4bc5-80b6-47b9cf681a05">video</a>, <a href="https://github.com/khalido/cs109-2015/blob/master/Lectures/11-DecisionTreesAndRandomForest.pdf">slides</a>)</h3>
<ul>
<li>Books: Read <a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statiscal Learning</a> and <a href="https://www.microsoft.com/en-us/research/people/cmbishop/">Pattern Recognition and Machine Learning</a></li>
<li>Decision Tree - fast training and prediction, easy to understand and interpret. DT basically paritions a feature space into cells using a series of decision rules on one feature at a time</li>
<li>which feature to query and what thresholds to use?</li>
<li>node purity: do splits so cells are pure, i.e have only one class in them
<ul>
<li>Gini Impurity gives us the expected error of predicted a class from a random sample</li>
<li>Gini Index</li>
<li>Node purity gain: compare gini impurity of parent vs child nodes. Lets us see whether a split has improved classification better than a simple missclassification number.</li>
</ul></li>
<li>Optimal trees - diff ways to get there</li>
<li>Tree pruning - easy to overfit, so first we make the tree, then go backwards and remove ‘bad’ decisions, or merge cells etc.</li>
<li>DT disadvatages: sensitive to small changes in data, overfit, only axis aligned splits</li>
<li>DT vs SVM:</li>
<li>Netflix prize winners used an ensemble of over 800 models. somewhat disapointing as they didn’t come up with a new method</li>
<li>DT doesn’t perform well, but what if we use many of them?</li>
<li>Bootstrap is one way to do this. It’s a resampling method from statistics, useful to get error bags on estimates.
<ul>
<li>Bootstrap lets us generate more sample sets from one dataset, each one slightly different.</li>
<li>Take N data points and draw N times with replacement, then get an estimate from each bootstrapped samples.</li>
<li>bagging: bootstrap aggregrating, where you learn a classifer from each bootstrap sample and average the . (normally uses just one type of classifier)
<ul>
<li>See <a href="https://github.com/khalido/cs109-2015/blob/master/Bootstrapping.ipynb">bootstrap example notebook</a></li>
<li>bagged DT’s perform better than one a single tree</li>
<li>not useful for linear models</li>
</ul></li>
<li>Bias-Variance trade off: train models with a high variance, then the average might get close</li>
</ul></li>
<li><strong>Random Forest</strong> builds on bagging, builds a tree from each bootstrap sample with node splits selected from random feature subsets. See <a href="http://blog.yhat.com/posts/random-forests-in-python.html">1</a>, or rather <a href="https://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html">2</a></li>
</ul>
</section>
</section>
<section id="week-7-machine-learning-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="week-7-machine-learning-best-practices">Week 7: Machine Learning best practices</h2>
<p><a href="https://github.com/khalido/cs109-2015/blob/master/homework/HW3.ipynb">HW 3 Q1 due</a>: a lot of pandas manipulation on baseball data</p>
<p>Start the <a href="http://cs109.github.io/2015/pages/projects.html">project</a></p>
<blockquote class="blockquote">
<p>Towards the end of the course you will work on a month-long data science project. The goal of the project is to go through the complete data science process to answer questions you have about some topic of your own choosing. You will acquire the data, design your visualizations, run statistical analysis, and communicate the results. You will work closely with other classmates in a 3-4 person project team.</p>
</blockquote>
<section id="lab-6-machine-learning-2" class="level3">
<h3 class="anchored" data-anchor-id="lab-6-machine-learning-2">Lab 6: Machine Learning 2</h3>
<p><strong>Classification, Probabilities, ROC curves, and Cost</strong> (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=83dfe4c9-fa1b-429c-a84c-839195fbede8">video</a>,<a href="https://github.com/cs109/2015lab6/blob/master/lab6-classification-redux.ipynb">notebook</a>)</p>
<ul>
<li>Not a very useful lab, essentially better of from sciki-learn understand how to use:
<ul>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression</a></li>
<li><a href="http://scikit-learn.org/stable/modules/svm.html">SVM</a>, or <a href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html">swm with kernels</a> - <a href="https://www.oreilly.com/learning/intro-to-svm">good tutorial</a></li>
<li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html">Confusion Matrix</a></li>
</ul></li>
</ul>
<p><strong>Comparing Models</strong> (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=4676adde-557f-469b-bca8-5ffe868094eb">video</a>, <a href="https://github.com/cs109/2015lab6/blob/master/lab6-churn.ipynb">notebook</a>)</p>
<ul>
<li>learn Bayes classifiers in sklean</li>
<li>use <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic">ROC</a> curves - often accuracy is the the relevant, the true +ve and false -ve rate is more important. Since False negatives can be costly, you often want to change the threshold probability from the default 0.5. So write your own prediction function as the sklearn one uses 0.5, or with bayes classifiers adjust the prior probability.</li>
<li>sklearn has a <a href="http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py">roc function</a>, <a href="http://benalexkeen.com/scoring-classifier-models-using-scikit-learn/">tutorial</a></li>
</ul>
</section>
<section id="lecture-12-ensemble-methods-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-12-ensemble-methods-video-slides">Lecture 12: Ensemble Methods ((<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=4831ebf0-7832-42c5-9339-5b5e08dd3e92">video</a>, <a href="https://github.com/cs109/2015/blob/master/Lectures/12-Ensemble%20Learning%20and%20Random%20Forests.pdf">slides</a>))</h3>
<ul>
<li>philisophical point: who do ensenble methods work so well? in real life wisdom of the crowds is overrated, but it does a lot better in computerland. Some averaging methods pick up useful stuff in the data, while others cancel out each others errors.</li>
<li>Decision trees are easy but have poor predictive accuracy, tend to ovefit</li>
<li>Ensemble learning combines many learners using methods like weighted avg, boosting, etc. See <a href="http://scikit-learn.org/stable/modules/ensemble.html">sklearn’s ensemble page</a></li>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">random forests</a> is an extension of bagging of decision trees
<ul>
<li>random forests using sqrt(features) of random predicters to make trees give the best results</li>
</ul></li>
<li>Boosting is another ensemble method like bagging, but better for most applications.
<ul>
<li>tune by num of trees, splits in each tree, weights</li>
<li>often using very simple trees, which you adjust the weights as u make more trees to classify better the things u got wrong. At the end u make a weighted avg.</li>
<li>most popular &amp; succesful boosting algorithim is <a href="http://scikit-learn.org/stable/modules/ensemble.html#adaboost">AdaBoost</a></li>
</ul></li>
</ul>
<p>Note: read <a href="https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12">this series on machine learning</a></p>
</section>
<section id="lecture-13-best-practices-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-13-best-practices-video-slides">Lecture 13: Best Practices (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=b33eec92-d049-4353-a904-5054eb718aff">video</a>, <a href="https://github.com/cs109/2015/blob/master/Lectures/13-BestPractices_Recommendations.pdf">slides</a>)</h3>
<ul>
<li>story telling is important for data scientists - explain whats going on, even in your own notebooks. good presentation is very important.</li>
<li>Diff b/w bagging and random forest: rf has bagging idea + random feature subsets</li>
<li>didn’t really find this video that useful, for example:
<ul>
<li>knn and trees can be used for regression too, but why would we?</li>
<li>SVM’s can be used for regression</li>
</ul></li>
<li>take care to normalize your data in a sane manner</li>
</ul>
</section>
</section>
<section id="week-8-ec2-and-spark" class="level2">
<h2 class="anchored" data-anchor-id="week-8-ec2-and-spark">Week 8: EC2 and Spark</h2>
<p>HW3 <a href="https://nbviewer.jupyter.org/github/khalido/cs109-2015/blob/master/homework/HW3.ipynb#Problem-2">q2</a> uses the iris data set &amp; <a href="https://nbviewer.jupyter.org/github/khalido/cs109-2015/blob/master/homework/HW3.ipynb#Problem-3:">q3</a> uses sklearn’s digits dataset and gridsearchCV to find best parameters for a KNN classifier for two simple datasets.</p>
<section id="lab-7-decision-trees-random-forests-ensemble-methods-video-notebook" class="level3">
<h3 class="anchored" data-anchor-id="lab-7-decision-trees-random-forests-ensemble-methods-video-notebook">Lab 7: Decision Trees, Random Forests, Ensemble Methods (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=0c645a5c-d262-4bb5-a137-3a78db60f3e7">video</a>, <a href="https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab7/Lab7-Botany%20and%20Ensemble%20Methods.ipynb">notebook</a>)</h3>
<ul>
<li><a href="http://scikit-learn.org/stable/modules/tree.html#tree">decision trees</a>, but they use their own function on top of sklearn so its a bit annoying</li>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html#forest">random forests</a>:
<ul>
<li>take a random subsample of the data</li>
<li>select a subset of all possible variables (at each node) and build the tree on the best split</li>
<li>repeat, then finally take a majority vote</li>
</ul></li>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html">Ensemble</a> learning - put together a lot of classifiers to build a better one</li>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html#adaboost">AdaBoost Classifier</a> uses weights on the training data.</li>
<li><a href="http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting">Gradient Boost Classifier</a> is similar to AdaBoost but uses tree as its base classifier, can do regression or classification.</li>
</ul>
</section>
<section id="lecture-14-best-practices-recommendations-and-mapreduce-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-14-best-practices-recommendations-and-mapreduce-video-slides">Lecture 14: Best Practices, Recommendations and MapReduce (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afee45b9-dcf5-4f29-bc60-871aa78f1cf8">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/14-Recommendations_MapReduce.pdf">slides</a>)</h3>
<ul>
<li>should have started final project by now, if not, start now!</li>
<li><strong>Nesting:</strong> use 5 fold cross validation, take each fold and further apply 5 fold cv to find the right hyperparameters, then use those params in the original fold to train the classifier.</li>
<li>think about how and why to normalize data, e.g data already in a range from 0-1 might not need to be normalized, think about normazlization as hyperparameters to your model.
<ul>
<li>get the mean estimates from your training data and use that to normalize training, validation and testing data. these values need to be stored to normalize future data.</li>
</ul></li>
<li>know your application domain</li>
<li>many problems are to do with imbalanced data. Fixes to get balanced data for training:
<ul>
<li>oversample: take bigger samples of the sparse clasess</li>
<li>subsampling: take smaller samples of the more frequently occuring classes</li>
<li>or weight classes so classifier pays more attention to the sparese classes, see <a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold">sklearn</a></li>
</ul></li>
<li>Missing data, where some of the features are missing for certain samples:
<ul>
<li>delete, but can reduce data too much</li>
<li>use mean</li>
<li>use regression to estimate missing values</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Collaborative_filtering">Collobrative filtering</a> is a common technique for recommendations or filling in missing values. Can be user based or item based.
<ul>
<li>KNN is widely used for collobrative filtering.</li>
<li>SVD is widely used, famously in the netflix contest. watch this <a href="https://www.youtube.com/watch?v=YKmkAoIUxkU">svd video</a> for a refresher if needed.</li>
</ul></li>
</ul>
<p>Moving on from Machine Learning…</p>
<p>Map reduce is a way to deal with very large data sets by distributing work over many machines. Developed by Google, and Apache Hadoop is a open source implementation.</p>
<ul>
<li>data is in key value pairs and is distributed to however many servers</li>
<li>we write a map function which does something to a key value pair.</li>
<li>the mapreduce implementation reaggregrates the key value pairs incoming from all the servers (sorted by keys)</li>
<li>we write a reduce which does something else</li>
<li>simple mapreduce example:
<ul>
<li>map takes in sentences, sends them off to diff machines to process,</li>
<li>those machines send back key:value pairs, say we are just counting words so we get back from machine 1: <code>"the":1</code>, <code>"the":1</code>, <code>"a":4</code> and machine 2 sends back “the”:4, “a”10,</li>
<li>there can be a combine step here, which takes the output from one mapper and combines it, so the two <code>the</code> words from machine 1 become one <code>"the":2</code>. this reduces network traffic and makes the work upstream easier. the combine output has to be of the same type as the mapper, since combine is just an optional optimizatizion step not a DO WORK thing.</li>
<li>the mapreduce thingamajig aggregrates all the key:value pairs and sends them to the</li>
<li>reduce function, which counts the words and finally we end up with “the”:6 and “a”:14 <img src="../../img/mapreduce.png" class="img-fluid"></li>
<li>Udacity has a <a href="https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617">course on mapreduce with hadoop</a></li>
</ul></li>
</ul>
</section>
<section id="lecture-15-mapreduce-combiners-and-spark-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-15-mapreduce-combiners-and-spark-video-slides">Lecture 15: MapReduce Combiners and Spark (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=10964b32-dcdc-49d2-b766-039b707a5c37">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/15a-MapReduce_Combiner.pdf">slides</a>)</h3>
<ul>
<li>refresher on mapreduce: it takes away the headache of dealing with machines, clusters, etc
<ul>
<li>be careful on how you use reducers, they shouldn’t effect the algorithims effectiveness</li>
<li>the reducer should work whether or not the combiner runs</li>
<li>you could have a in-mapper combining, but here we are gettting into complex territory</li>
</ul></li>
<li>Why did we switch to map reduce from good old write a function and run it computing to the horror that is map reduce?
<ul>
<li>large large data which looms like a t-rex, scaring our current teeny tiny computers cowering away in fear. luckily the computers gathered in data centers where mapreduce and other such algos can harness them in parallel.</li>
<li>many problems are embarrisingly parallel</li>
</ul></li>
<li><a href="http://hadoop.apache.org/">Apache Hadoop</a> is an open source implementation of map reduce, built way back in 2006. It provides us a way to store data on clusters of commodity machines and process it using mapreduce, without having to worry about the rocketscience of managing the clusters and splitting data and processes across all the machines.</li>
<li>Though these days you probably want to use Hadoop with <a href="http://spark.apache.org/">Apache Spark</a> running on <a href="https://aws.amazon.com/emr/">Amazon’s EMR</a> or <a href="https://cloud.google.com/dataproc/">Google Cloud</a>.</li>
<li>python has tools like <a href="https://ipyparallel.readthedocs.io/en/latest/">dask</a> and <a href="https://ipyparallel.readthedocs.io/en/latest/">ipyparallel</a> for parallel computing. for many projects python is enough</li>
<li>Functional programming: where your functions take an input, process it and return an output, without messing around with other functions or data stored elsewhere. this makes so much sense that it shouldn’t be a big deal, it sounds like a toyota corolla of the computing world, but its a big deal because ppl manage to write functions which fuck around with your programming world enough that after a few rounds of their unfunctional running around no one has a good idea of whats the actual state of things.
<ul>
<li>Functional programming is clean! Watch Joel Grus <a href="http://pyvideo.org/pydata-seattle-2015/learning-data-science-using-functional-python.html">Learning Data Science Using Functional Python</a> or <a href="https://www.youtube.com/watch?v=LkpHQL863mw">this one</a></li>
</ul></li>
<li>Getting back to mapreduce, when you are programming functionally, then it makes it possible to distribute the data and computing across different machines as the functions aren’t trying to mess with each other. When a machine dies, its no big deal as we know what data and function we sent it to process, we just resend it to some other machine.
<ul>
<li>aside: this is why pandas always outputs a new dataframe whenever we do something, trying to emulate this ideal of not fucking up existing things. Spark also does similar stuff.</li>
</ul></li>
<li><a href="http://spark.apache.org/">Apache Spark</a> makes all this easy for us. Runs on top of Hadoop and provides nice interface and methods for all kind of stuff. So nicer, faster, shineir than Hadoop.
<ul>
<li>Spark stores data on a Resilient distributed dataset (RDD) - a fault tolerant collection of stuff which can be operated on in parallel.</li>
<li>the basics of using sparl: write a function, some kind of mapreduce job, spark runs it on the RDD and makes a new processed RDD.</li>
<li>a lot of the pandas style commands work on spark</li>
<li>spark is lazy - it makes a graph of the stuff we want to do and then runs it from start to end every time we execute. so caching is important so we don’t keep computing same stuff over and over again</li>
<li>explore spark…. <em>todo</em></li>
</ul></li>
</ul>
</section>
</section>
<section id="week-9-bayes" class="level2">
<h2 class="anchored" data-anchor-id="week-9-bayes">Week 9: Bayes!</h2>
<section id="lab-8-vagrant-and-virtualbox-aws-and-spark-video-notebook" class="level3">
<h3 class="anchored" data-anchor-id="lab-8-vagrant-and-virtualbox-aws-and-spark-video-notebook">Lab 8: Vagrant and VirtualBox, AWS, and Spark (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=6f1deb9e-051b-4019-99ec-19aee14be9d9">video</a>, <a href="">notebook</a>)</h3>
<ul>
<li>Moving on from sklearn…</li>
<li></li>
</ul>
</section>
<section id="lecture-16-bayes-theorem-and-bayesian-methods-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-16-bayes-theorem-and-bayesian-methods-video-slides">Lecture 16: Bayes Theorem and Bayesian Methods (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=233f6c34-306f-481b-8ea5-be33076eb6a8">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/16-BayesianMethods.pdf">slides</a>)</h3>
<ul>
<li><p>book recommendations from the lecture: <em>(the one I liked best in bold)</em></p>
<ul>
<li>easy reading: <a href="https://www.amazon.com/Theory-That-Would-Not-Die-ebook/dp/B0050QB3EQ/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=">How Bayes’ Rule Cracked the Enigma Code</a> also see her <a href="https://www.youtube.com/watch?v=8oD6eBkjF9o">talk at google</a>.</li>
<li>simple stuff, in python: <a href="http://greenteapress.com/wp/think-bayes/">Think Bayes</a>, though maybe too basic</li>
<li><strong><a href="http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/">Probabilistic Programming &amp; Bayesian Methods for Hackers</a> - text and python code is in <a href="https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers">jupyter notebooks</a> which u can clone to your own pc and go through.</strong></li>
<li>Proper textbooks: <a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis</a> and <a href="http://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> - this comes with <a href="https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3">python examples</a>.</li>
</ul></li>
<li><p>Bayes rule: P(A|B) = P(B|A)P(A) / P(B)</p></li>
<li><p>bayes rules tells us how to update our beliefs (the prior) as we get new data, which gives us a new posterior (which is just a fancy word for our new, updated belief). A more <a href="http://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html">wordy description</a>:</p>
<blockquote class="blockquote">
<p>The theorem itself can be stated simply. Beginning with a provisional hypothesis about the world (there are, of course, no other kinds), we assign to it an initial probability called the prior probability or simply the prior. After actively collecting or happening upon some potentially relevant evidence, we use Bayes’s theorem to recalculate the probability of the hypothesis in light of the new evidence. This revised probability is called the posterior probability or simply the posterior.</p>
</blockquote>
<ul>
<li>bayes is controversial becuase traditional stats doesn’t like giving numbers to unknown thinks, for example bayes essentially makes up the prior. the prior is often our subject belief about something.
<ul>
<li>however, even when starting out with different priors, given that they aren’t ridiculously dogmatic, with a large sample size the different priors will converge</li>
</ul></li>
<li>discriminative model: focus on predicting y given x, generative model: we simulate the entire model, i.e we can generate x and y</li>
<li>naive bayes: assumes probablities are conditionally independent of each other, greatly simplifies the calculations. sometimes unrealistic but works well for many scenarios. <a href="http://scikit-learn.org/stable/modules/naive_bayes.html">sklearn has bayes, of course</a>.</li>
<li>Conjugate prior says that if u start with a family of distributions, like beta, you stay in the same distribution. simplifies computations</li>
<li>one way to think about bayesian</li>
</ul></li>
</ul>
<p>More Reading:</p>
<ul>
<li><a href="https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics">Count Baye’s intro to bayesian statistics</a></li>
</ul>
</section>
<section id="lecture-17-bayesian-methods-continued-video-slides" class="level3">
<h3 class="anchored" data-anchor-id="lecture-17-bayesian-methods-continued-video-slides">Lecture 17: Bayesian Methods Continued (<a href="https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=e63c650e-4cf6-4bee-b57e-f16e927ba25a">video</a>, <a href="https://github.com/cs109/2015/raw/master/Lectures/17BayesianMethodsContinued.pdf">slides</a>)</h3>
<ul>
<li>you can estimate your prior from the data, though some bayesians would say you’re tainting your priors and the data by doing that, but this is an accepted way to get an acceptable prior.</li>
<li>the lecture works through examples from a blog, which has collected its bayes posts into this book: <a href="http://varianceexplained.org/r/empirical-bayes-book/">Introduction to Empirical Bayes: Examples from Baseball Statistics</a>. the explanations in the book look great.</li>
</ul>
<p>Note: Bayes is simple to do yet hard to understand. So read a number of guides/blogs/posts/<a href="http://pyvideo.org/search.html?q=bayes">youtubes</a> till it makes sense. Some talks to see:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=p1IB4zWq9C8">Eric J Ma Bayesian Statistical Analysis with Python PyCon 2017</a> - 30 min talk, uses PyMC3</li>
<li><a href="https://www.youtube.com/watch?v=5TyvJ6jXHYE">Christopher Fonnesbeck Probabilistic Programming with PyMC3 PyCon 2017</a> - 30min, more PyMC3</li>
</ul>
</section>
</section>
<section id="week-10-text" class="level2">
<h2 class="anchored" data-anchor-id="week-10-text">Week 10: Text</h2>
<p>hw5 - did this with my group, so need to redo the hw and commit to my own github.</p>
</section>
<section id="week-11-clustering" class="level2">
<h2 class="anchored" data-anchor-id="week-11-clustering">Week 11: Clustering!</h2>
</section>
<section id="week-12-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="week-12-deep-learning">Week 12: Deep Learning</h2>
<p>Used tensorflow and keras. update repo.</p>
</section>
<section id="week-13-final-project-wrapup" class="level2">
<h2 class="anchored" data-anchor-id="week-13-final-project-wrapup">Week 13: Final Project &amp; Wrapup</h2>
<p>My final project was a proof of concept bot which ingested your bank transactions and answered questions about your money. It used wit.ai to parse user queries, machine learning to categorize transactions and straight forward scipy to crunch numbers and make graphs using matplotlib and seaborn. It was a fun learning excercise, to make something which lived briefly live on facebook messenger and could answer questions. using wit.ai made the NLP easy, though with more time writing my own NLP parser or using one of the many offline libraries would be a good learning excercise.</p>
</section>
</section>
<section id="additional-resources" class="level1">
<h1>Additional Resources</h1>
<p>Stuff I found useful to understand the class material better.</p>
<ul>
<li><a href="https://ds8.gitbooks.io/textbook/content/">Computational and Inferential Thinking</a> - the textbook for UC Berkely’s [Foundations of Data Science class](http://data8.org/.</li>
<li><a href="https://jakevdp.github.io/PythonDataScienceHandbook/">Pythonb Data Science Handbook</a></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb3" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Notes for Harvard's Cs109 data science class"</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2017-04-04</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> Notes as I worked through CS109.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">- python</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">- data science</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>Notes for Harvard's Cs109 data science class</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>These are my notes for <span class="co">[</span><span class="ot">Harvard's 2015 CS109 class</span><span class="co">](http://cs109.github.io/2015/)</span>, which i went through with Sydney Machine Learning's <span class="co">[</span><span class="ot">study group</span><span class="co">](https://sydneymachinelearningblog.wordpress.com/cs-109-study-group/)</span> from Ausgust to October 2017 at Amazon Web Service's sydney office.</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>**Why CS109?** This class was recommended at Quora and a few other places as being a good resource for practical data science, so here goes. These notes are updated as I work through the <span class="co">[</span><span class="ot">course syllabus</span><span class="co">](http://cs109.github.io/2015/pages/schedule.html)</span> and the <span class="co">[</span><span class="ot">labs and homeworks</span><span class="co">](https://porter.io/github.com/cs109/content)</span>.</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>The stuff to watch and work through:</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Lecture videos &amp; notes</span><span class="co">](http://cs109.github.io/2015/pages/videos.html)</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>Note: download the videos using <span class="co">[</span><span class="ot">this script</span><span class="co">](https://github.com/christopher-beckham/cs109-dl-videos)</span>, and merge <span class="co">[</span><span class="ot">pull request 11</span><span class="co">](https://github.com/christopher-beckham/cs109-dl-videos/pull/11)</span> to get the 2015 videos.</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>**Study Suggestions before starting:**</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use a internet blocker app like <span class="co">[</span><span class="ot">SelfControl</span><span class="co">](https://selfcontrolapp.com/)</span> to stop procrastinating and a <span class="co">[</span><span class="ot">pomodoro app</span><span class="co">](http://tomighty.org/)</span> to break up study into chunks.</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use a paper notebook for notes as you watch the videos and do the labs.</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Get a second monitor so you can watch videos/have lab notebooks open and work through at the same time. (I got a 1440p 27inch monitor from ebay and it made things so much easier from just using my laptop).</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Don't look at the lab and hw answers - try to do them first on your own. Discuss with others before looking at solutions.</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a><span class="fu"># CS109 notes, by the class schedule:</span></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 1: What is Data Science</span></span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Lecture 1</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/01-Introduction.pdf)</span> introduces data science. The basic stuff covered in every blog post.</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 2: Intro Data Analysis and Viz</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>The <span class="co">[</span><span class="ot">lecture 2 notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScraping.ipynb)</span> goes through getting data and putting it into a pandas dataframe.</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>Lab 1 has three very introductory notebooks: <span class="co">[</span><span class="ot">pythonpandas</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-pythonpandas.ipynb)</span>, followed by <span class="co">[</span><span class="ot">babypython</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-babypython.ipynb)</span>, and finally <span class="co">[</span><span class="ot">git</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/Lab1-git.ipynb)</span>. However, since the course dates back to 2015, some of the python is a bit dated and uses 2.x code.</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>After doing the three intro notebooks, <span class="co">[</span><span class="ot">hw0</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab1/hw0.ipynb)</span> runs you through installing anaconda, git, and setting up a github and aws account.</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>Hw0 has one interesting section, where you solve the <span class="co">[</span><span class="ot">montyhall problem</span><span class="co">](https://en.wikipedia.org/wiki/Monty_Hall_problem)</span> one step at a time. I didn't really like their steps, so made a <span class="co">[</span><span class="ot">simpler monty hall implementation</span><span class="co">](https://github.com/khalido/algorithims/blob/master/monty_hall.ipynb)</span>.</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>Moving on to the <span class="co">[</span><span class="ot">Lecture 2</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScraping.ipynb)</span> &amp; its <span class="co">[</span><span class="ot">quiz notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/02-DataScrapingQuizzes.ipynb)</span>, this goes through some more pandas and data scraping web pages and parsing them.</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>I made a couple of notebooks expand on some of the stuff covered:</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">movielens notebook for basic pandas</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/movielens.ipynb)</span> workflow of downloading a zip file, extracting it and putting into pandas dataframes and doing some q&amp;a</span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">twitter notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/twitter.ipynb)</span> - basic usage of twitter api and doing something with tweets</span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a>**Lecture 3** (<span class="co">[</span><span class="ot">slides</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/03-EDA.pdf)</span>, <span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=a4e81697-fd86-415c-9b29-c14ea7ec15f2)</span>):</span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>ask a q, get data to answer it, explore &amp; check data, then model it and finally communicate and visualize the results.</span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>keep viz simple and think of the <span class="co">[</span><span class="ot">kind of chart</span><span class="co">](http://extremepresentation.typepad.com/blog/files/choosing_a_good_chart.pdf)</span> needed.</span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 3 : Databases, SQL and more Pandas</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Lab 2</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab2/Lab2.ipynb)</span> introduces web scraping with <span class="co">[</span><span class="ot">requests</span><span class="co">](http://docs.python-requests.org/en/master/)</span> and then parsing html with <span class="co">[</span><span class="ot">beautiful soup 4</span><span class="co">](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)</span>.</span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a>Lecture 4 (<span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f8a832cb-56e7-401b-b485-aec3c9928069)</span>, <span class="co">[</span><span class="ot">slides</span><span class="co">](https://github.com/cs109/2015/raw/master/Lectures/04-PandasSQL.pdf)</span>) (covers some more <span class="co">[</span><span class="ot">Pandas and SQL</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Lectures/Lecture4/PandasAndSQL.ipynb)</span>.</span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a>Lecture 5 (<span class="co">[</span><span class="ot">slides</span><span class="co">](https://github.com/cs109/2015/raw/master/Lectures/05-StatisticalModels.pdf)</span>, <span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=873964c6-d345-4f46-a8bc-727b96432d63)</span>) on stats is a bit sparse. Some supplementary material:</span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Stanford Statistics Course</span><span class="co">](https://lagunita.stanford.edu/courses/course-v1:OLI+ProbStat+Open_Jan2017/info)</span> - check on this one vs the MIT one.</span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span><span class="co">[</span><span class="ot">Think Stats</span><span class="co">](http://greenteapress.com/thinkstats2/index.html)</span> is a good basic book covering stats using Python.</span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span><span class="co">[</span><span class="ot">Think Bayes</span><span class="co">](http://greenteapress.com/wp/think-bayes/)</span> follows on from Think Stats and covers Bayesian stats in Python.</span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 4: Probablity, regression and some stats</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Lab 3</span><span class="co">](https://github.com/khalido/cs109-2015/tree/master/Labs/2015lab3)</span> has three notebooks:</span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Lab3-Probability</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab3/Lab3-probability.ipynb)</span> covers basic probability. Uses a lot of numpy methods, so its a good idea to brush up on numpy.</span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">scipy.stats</span><span class="co">](https://docs.scipy.org/doc/scipy/reference/stats.html)</span> - very handy, has most stats stuff needed for DS built in.</span>
<span id="cb3-74"><a href="#cb3-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Lab3-Frequentism, Samples and the Bootstrap</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab3/Lab3-Freq.ipynb)</span></span>
<span id="cb3-75"><a href="#cb3-75" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>use seaborn for plotting, very handy. a <span class="co">[</span><span class="ot">good guide to sns factorplot and facetgrids</span><span class="co">](http://blog.insightdatalabs.com/advanced-functionality-in-seaborn/)</span></span>
<span id="cb3-76"><a href="#cb3-76" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">PDF</span><span class="co">](https://en.wikipedia.org/wiki/Probability_density_function)</span> tells us the probability of where a continuus random variable will be in set of possible values that random variable can be (the sample space).</span>
<span id="cb3-77"><a href="#cb3-77" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">PMF</span><span class="co">](https://en.wikipedia.org/wiki/Probability_mass_function)</span> tells us the probability that a discrete random variable will be ecactly equal to some value</span>
<span id="cb3-78"><a href="#cb3-78" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">CDF</span><span class="co">](https://en.wikipedia.org/wiki/Cumulative_distribution_function)</span> function tells us the probability that a random discrete or continous variable X will take a value less than or equal to X. <span class="co">[</span><span class="ot">Video</span><span class="co">](https://youtu.be/bGS19PxlGC4?list=PLF8E9E4FDAAA8018A)</span></span>
<span id="cb3-79"><a href="#cb3-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-80"><a href="#cb3-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 6: Story Telling and Effective Communication ([slides](https://github.com/cs109/2015/raw/master/Lectures/06-StoryTelling.pdf), [video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7))</span></span>
<span id="cb3-81"><a href="#cb3-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-82"><a href="#cb3-82" aria-hidden="true" tabindex="-1"></a>Good insights on how to tell a story with data. Infer, model, use an algorithim and draw conclusions (and check!).</span>
<span id="cb3-83"><a href="#cb3-83" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-84"><a href="#cb3-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Start with two fundamental questions:</span>
<span id="cb3-85"><a href="#cb3-85" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Whats the goal? think first of that rather than going first to all the many ways you can slice and dice data.</span>
<span id="cb3-86"><a href="#cb3-86" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Who cares? Know your audience and tell them a story. Have a clear sense of direction and logic.</span>
<span id="cb3-87"><a href="#cb3-87" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Read some howto's on scientific writing</span>
<span id="cb3-88"><a href="#cb3-88" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>have some memorable examples or small stories</span>
<span id="cb3-89"><a href="#cb3-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-90"><a href="#cb3-90" aria-hidden="true" tabindex="-1"></a>Tell a story:</span>
<span id="cb3-91"><a href="#cb3-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-92"><a href="#cb3-92" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>know your audience and why/what they care about this data - what do they want?</span>
<span id="cb3-93"><a href="#cb3-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Don't make them think - clearly tell them what you want them to know in a way they can follow. highlight key facts and insights.</span>
<span id="cb3-94"><a href="#cb3-94" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>unexpectedness - show something the audience didn't expect. I liked the story which highlighted that bigfoot sightings are dropping sharply</span>
<span id="cb3-95"><a href="#cb3-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>What tools can we give the audience? For example, a web app for them to further explore the data, or a takeaway presentation with key points.</span>
<span id="cb3-96"><a href="#cb3-96" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>be careful of your point of view and don't distort the data, but depending on the audience you can frame your story - for example presenting war deaths in red rather than a regular plot color.</span>
<span id="cb3-97"><a href="#cb3-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>important to put the message up front - what does my graph show? Show it in stages if a lot of data, highlighting what to look at. Design matters.</span>
<span id="cb3-98"><a href="#cb3-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-99"><a href="#cb3-99" aria-hidden="true" tabindex="-1"></a>More resources:</span>
<span id="cb3-100"><a href="#cb3-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-101"><a href="#cb3-101" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">The Functional Art</span><span class="co">](http://www.thefunctionalart.com/)</span></span>
<span id="cb3-102"><a href="#cb3-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-103"><a href="#cb3-103" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 7: Bias and Regression ([slides](https://github.com/cs109/2015/raw/master/Lectures/07-BiasAndRegression.pdf), [video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afe70053-b8b7-43d3-9c2f-f482f479baf7))</span></span>
<span id="cb3-104"><a href="#cb3-104" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-105"><a href="#cb3-105" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>think about bias, missing data, etc</span>
<span id="cb3-106"><a href="#cb3-106" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>combine independent, unbiased estimators for a parameter into one:</span>
<span id="cb3-107"><a href="#cb3-107" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>fisher weighting</span>
<span id="cb3-108"><a href="#cb3-108" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>nate silver weighting method</span>
<span id="cb3-109"><a href="#cb3-109" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bonferroni</span>
<span id="cb3-110"><a href="#cb3-110" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>good segment on weighting variables</span>
<span id="cb3-111"><a href="#cb3-111" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>regression towards the mean</span>
<span id="cb3-112"><a href="#cb3-112" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>think of regression in terms of population or a projection of the column space of x - i.e what combination of the variables of x gets us closest to the value of y?</span>
<span id="cb3-113"><a href="#cb3-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>linear regression means we're taking linear combination of predictors, the actual regression equation can be nonlinear</span>
<span id="cb3-114"><a href="#cb3-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what function of x gives the best predictor of y?</span>
<span id="cb3-115"><a href="#cb3-115" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Gauss-Markov Theorem</span>
<span id="cb3-116"><a href="#cb3-116" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the residuals are the diff  b/w the actual value of y and the predicted value - plot residuals vs fitted values and vs each predictor variable - good way to eyeball quality of linear regression model</span>
<span id="cb3-117"><a href="#cb3-117" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>variance R^2 measures goodness of fit, but doesn't mean model is good.</span>
<span id="cb3-118"><a href="#cb3-118" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Best way to check a model is prediction.</span>
<span id="cb3-119"><a href="#cb3-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-120"><a href="#cb3-120" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 5: Scikit learn &amp; regression</span></span>
<span id="cb3-121"><a href="#cb3-121" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-122"><a href="#cb3-122" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lab 4 - Regression in Python ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=483c8b93-3700-4ee8-80ed-aad7f3da7ac2), [notebook](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab4/Lab4-stats.ipynb))</span></span>
<span id="cb3-123"><a href="#cb3-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-124"><a href="#cb3-124" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Wikipeda article</span><span class="co">](https://en.wikipedia.org/wiki/Linear_regression)</span></span>
<span id="cb3-125"><a href="#cb3-125" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>We have data X, which is related to Y in some way.</span>
<span id="cb3-126"><a href="#cb3-126" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear regression uses X to predict Y, and also tells us the predictive power of each variable of X</span>
<span id="cb3-127"><a href="#cb3-127" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Linear regression assumes the distribution of each Y is normal (which isn't always so)</span>
<span id="cb3-128"><a href="#cb3-128" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>there are many ways to fit a linear regression model, most common is the <span class="co">[</span><span class="ot">least squares</span><span class="co">](http://en.wikipedia.org/wiki/Least_squares)</span> method</span>
<span id="cb3-129"><a href="#cb3-129" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>be careful that the features (variables in X) aren't too similar</span>
<span id="cb3-130"><a href="#cb3-130" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>explore your data, plot variables, scatterplots etc. Use seaborn to plot regression.</span>
<span id="cb3-131"><a href="#cb3-131" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use <span class="co">[</span><span class="ot">sklearn to split dataset into a train/test</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)</span></span>
<span id="cb3-132"><a href="#cb3-132" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use <span class="co">[</span><span class="ot">cross-validation</span><span class="co">](http://scikit-learn.org/stable/modules/cross_validation.html)</span></span>
<span id="cb3-133"><a href="#cb3-133" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>overfitting happens when the model 'learns' the train data so performs better on that than the test dataset</span>
<span id="cb3-134"><a href="#cb3-134" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>there are <span class="co">[</span><span class="ot">many types of regressions</span><span class="co">](http://www.datasciencecentral.com/profiles/blogs/10-types-of-regressions-which-one-to-use)</span> so think about which one to use</span>
<span id="cb3-135"><a href="#cb3-135" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>for high d data, closed form vs gradient decent:</span>
<span id="cb3-136"><a href="#cb3-136" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>closed form - use math to solve. this becomes computationally intensive very quickly, is ordered n cubed</span>
<span id="cb3-137"><a href="#cb3-137" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>gradient descent is O(n), so for large high d data it's gradient descent all the way</span>
<span id="cb3-138"><a href="#cb3-138" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Logistic regression - used where outcome is binary, for example a chance of success/failure. read:<span class="co">[</span><span class="ot">Adit's explanation.</span><span class="co">](http://adit.io/posts/2016-03-13-Logistic-Regression.html)</span></span>
<span id="cb3-139"><a href="#cb3-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-140"><a href="#cb3-140" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 8: More Regression ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=664f668e-e008-4f44-8600-e09ee6d629b0), [slides](https://github.com/khalido/cs109-2015/blob/master/Lectures/08-RegressionContinued.pdf))</span></span>
<span id="cb3-141"><a href="#cb3-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-142"><a href="#cb3-142" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>collinerarity - when some variables are highly correlated with each other - this is bad</span>
<span id="cb3-143"><a href="#cb3-143" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Logistic Regression</span>
<span id="cb3-144"><a href="#cb3-144" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Odds Ratio: ratio of two different people's odds of an outcome.</span>
<span id="cb3-145"><a href="#cb3-145" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Crude Odds Ratio Estimate - quick estimate but flawed as doesn't control for anything.</span>
<span id="cb3-146"><a href="#cb3-146" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Confounding Factors - i.e is one group pre-disposed to our outcome for some reason?</span>
<span id="cb3-147"><a href="#cb3-147" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Curse of dimensionality - in high d settings, vast majority of data is near boundaries, not center. But, high d can also be a <span class="co">[</span><span class="ot">blessing</span><span class="co">](http://andrewgelman.com/2004/10/27/the_blessing_of/)</span>.</span>
<span id="cb3-148"><a href="#cb3-148" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>dealing with high dimensionality: ridge regression, shrinkage estimation</span>
<span id="cb3-149"><a href="#cb3-149" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stein's Paradox <span class="co">[</span><span class="ot">wikipedia</span><span class="co">](https://en.wikipedia.org/wiki/Stein%27s_example)</span>, <span class="co">[</span><span class="ot">good article</span><span class="co">](http://statweb.stanford.edu/~ckirby/brad/other/Article1977.pdf)</span></span>
<span id="cb3-150"><a href="#cb3-150" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>LASSO and Ridge help with high D data by reducing features</span>
<span id="cb3-151"><a href="#cb3-151" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Lasso does L1 regularization, reduces number of features</span>
<span id="cb3-152"><a href="#cb3-152" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Ridge does L2 regularization, doesn't necessarily reduce features but reduces the impace of features on the model by reducing coefficient value</span>
<span id="cb3-153"><a href="#cb3-153" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Elasticnet does both L1 and L2 regularization</span>
<span id="cb3-154"><a href="#cb3-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-155"><a href="#cb3-155" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 9: Classification ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=c322c0d5-9cf9-4deb-b59f-d6741064ba8a), [slides](https://github.com/khalido/cs109-2015/blob/master/Lectures/09-ClassificationPCA.pdf))</span></span>
<span id="cb3-156"><a href="#cb3-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-157"><a href="#cb3-157" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we take data and assign labels</span>
<span id="cb3-158"><a href="#cb3-158" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>1 nearest neighbour - simple classification method for low d data</span>
<span id="cb3-159"><a href="#cb3-159" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>slow, has to check all points to find nearest neighbour</span>
<span id="cb3-160"><a href="#cb3-160" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>k nearest neighbours - use k nearest points to find decision boundary</span>
<span id="cb3-161"><a href="#cb3-161" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>find ideal k</span>
<span id="cb3-162"><a href="#cb3-162" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>what distance function to use?</span>
<span id="cb3-163"><a href="#cb3-163" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>my own very simple <span class="co">[</span><span class="ot">kNN algo implementation</span><span class="co">](https://github.com/khalido/algorithims/blob/master/k%20nearest%20neighbours.ipynb)</span></span>
<span id="cb3-164"><a href="#cb3-164" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>cross validation - for 5 fold cross validation, the data is split into 6 folds - 4 for training, one for validation and the sixth for testing, which is only used at the end.</span>
<span id="cb3-165"><a href="#cb3-165" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>CIFAR-10 for 60K images - is split into 50K training and 10K test</span>
<span id="cb3-166"><a href="#cb3-166" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>pics are 32x32x3</span>
<span id="cb3-167"><a href="#cb3-167" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>L1 distance is the absolute diff b/w two vectors</span>
<span id="cb3-168"><a href="#cb3-168" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>L2 is the Euclidean distance i.e "ordinary" straight-line distance</span>
<span id="cb3-169"><a href="#cb3-169" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>for images, l1 and l2 are pretty bad, so there are a lot more methods</span>
<span id="cb3-170"><a href="#cb3-170" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>more features are good for classification, but too many features means the data gets sparse - the curse of dimensionality strikes</span>
<span id="cb3-171"><a href="#cb3-171" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>so often we want to reduce dimensionality</span>
<span id="cb3-172"><a href="#cb3-172" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Principal Component Analysis - project a dataset from many variables into fewer less correlated ones, called the principal components.</span>
<span id="cb3-173"><a href="#cb3-173" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Singular Value Decomposition (SVD) - computational method to calculate pricipal components of a dataset. It transforms a large matrix of data into three smallter matrixes: <span class="in">`A (m*n) = U(m*r) x E(r*r) x V(r*n)`</span>. The values in the middle matrix <span class="in">`r*r`</span> are the *singular* values and we can discard bits of them to reduce the amount of data to a more manageable number.</span>
<span id="cb3-174"><a href="#cb3-174" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">good pca and svd explanation</span><span class="co">](https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294)</span></span>
<span id="cb3-175"><a href="#cb3-175" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Watch <span class="co">[</span><span class="ot">Statistics for Hackers</span><span class="co">](https://www.youtube.com/watch?v=Iq9DzN6mvYA)</span></span>
<span id="cb3-176"><a href="#cb3-176" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-177"><a href="#cb3-177" aria-hidden="true" tabindex="-1"></a><span class="fu">### HW2 Q1 ([notebook](https://github.com/khalido/cs109-2015/blob/master/homework/HW2.ipynb))</span></span>
<span id="cb3-178"><a href="#cb3-178" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-179"><a href="#cb3-179" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Uses svd and pca to analyze gene data</span>
<span id="cb3-180"><a href="#cb3-180" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a pandas excercise in downloading csv files into a data frame, usijng pd.datetime and visualising samples vs time</span>
<span id="cb3-181"><a href="#cb3-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-182"><a href="#cb3-182" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 6: SVM, trees and forests</span></span>
<span id="cb3-183"><a href="#cb3-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-184"><a href="#cb3-184" aria-hidden="true" tabindex="-1"></a>Now the course finally gets interesting. Before starting this weeks work, think about project ideas and see <span class="co">[</span><span class="ot">Hans Rosling</span><span class="co">](https://www.gapminder.org/videos/)</span> videos to see how to present data. Pitch this project idea (to study group or the internet at large).</span>
<span id="cb3-185"><a href="#cb3-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-186"><a href="#cb3-186" aria-hidden="true" tabindex="-1"></a>There are quite a few companies automating the entire datascience chain, so the key is being able to present your findings well.</span>
<span id="cb3-187"><a href="#cb3-187" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-188"><a href="#cb3-188" aria-hidden="true" tabindex="-1"></a><span class="fu">### HW 2 Questions 2,3 &amp; 4 ([notebook](https://github.com/khalido/cs109-2015/blob/master/homework/HW2.ipynb))</span></span>
<span id="cb3-189"><a href="#cb3-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-190"><a href="#cb3-190" aria-hidden="true" tabindex="-1"></a>H2 depends wholly on week 5, so good idea to get it done first. Used seaborn for all the viz questions makes some of them trivial.</span>
<span id="cb3-191"><a href="#cb3-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-192"><a href="#cb3-192" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>q2 looks at polling data and bias</span>
<span id="cb3-193"><a href="#cb3-193" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>q3 is more of the same but with seaborn</span>
<span id="cb3-194"><a href="#cb3-194" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>q4 nothing much to see here besides using list comprehensions to make a list of all the .csv files _(I'm trying to use python to do all the work instead of my stone age past life of copying and pasting links)_</span>
<span id="cb3-195"><a href="#cb3-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-196"><a href="#cb3-196" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb3-197"><a href="#cb3-197" aria-hidden="true" tabindex="-1"></a>url_str <span class="op">=</span> <span class="st">"http://elections.huffingtonpost.com/pollster/api/charts/?topic=2014-senate"</span></span>
<span id="cb3-198"><a href="#cb3-198" aria-hidden="true" tabindex="-1"></a>election_urls <span class="op">=</span> [election[<span class="st">'url'</span>] <span class="op">+</span> <span class="st">'.csv'</span> <span class="cf">for</span> election <span class="kw">in</span> requests.get(url_str).json()]</span>
<span id="cb3-199"><a href="#cb3-199" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-200"><a href="#cb3-200" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-201"><a href="#cb3-201" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lab 5: Machine Learning</span></span>
<span id="cb3-202"><a href="#cb3-202" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-203"><a href="#cb3-203" aria-hidden="true" tabindex="-1"></a>**Learning Models** (<span class="co">[</span><span class="ot">notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab5/LearningModels.ipynb)</span>, <span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=e509f996-9633-4b75-a48a-e29246a316db)</span>)</span>
<span id="cb3-204"><a href="#cb3-204" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-205"><a href="#cb3-205" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we often have a small sample of a much dataset, and we want to predict the larger data from our sample.</span>
<span id="cb3-206"><a href="#cb3-206" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>this isn't just statistical analysis, as we make models which involve domain knowledge and choices.</span>
<span id="cb3-207"><a href="#cb3-207" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>need to think about whether our sample is in some way representative of the population</span>
<span id="cb3-208"><a href="#cb3-208" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Stochastic noise, i.e randomness</span>
<span id="cb3-209"><a href="#cb3-209" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>systematic error, i.e where the sampling isn't representative, like polling ppl using landlines</span>
<span id="cb3-210"><a href="#cb3-210" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>overfitting: models can 'memrize the the data points in the training set, becoming useless or inaccurate at predicting real world data. With many data sets a more and more complex dataset will keep getting better while getting worse on test/validation data. The best model minimizes test set error, and not training set error.</span>
<span id="cb3-211"><a href="#cb3-211" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>great illustration of variance at 24:30 and 35min in the video</span>
<span id="cb3-212"><a href="#cb3-212" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use <span class="in">`from sklearn.cross_validation import train_test_split`</span> for splitting datasets into a train test split. See <span class="co">[</span><span class="ot">sklearn</span><span class="co">](http://scikit-learn.org/stable/)</span></span>
<span id="cb3-213"><a href="#cb3-213" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sklearn has 3 main features:</span>
<span id="cb3-214"><a href="#cb3-214" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>build and fit models</span>
<span id="cb3-215"><a href="#cb3-215" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>predict</span>
<span id="cb3-216"><a href="#cb3-216" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>transform data.</span>
<span id="cb3-217"><a href="#cb3-217" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sklearn expects days in a 2d array or a matrix of size <span class="in">`[n_samples, n_features]`</span>, so reshape 1d data using <span class="in">`np.reshape(-1,1)`</span></span>
<span id="cb3-218"><a href="#cb3-218" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Validation - keep a chunk of data seperate to check the model after training on the test/train data.</span>
<span id="cb3-219"><a href="#cb3-219" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Cross Validation: randomly split data into K diff train/test splits - so you traion on K-1 partitions and test on 1, so there are a total of K combinations, leading to K risks. This leads to better results then just doing one test/train split.</span>
<span id="cb3-220"><a href="#cb3-220" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>regularization helps with overfitting</span>
<span id="cb3-221"><a href="#cb3-221" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-222"><a href="#cb3-222" aria-hidden="true" tabindex="-1"></a>**Classification** (<span class="co">[</span><span class="ot">notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab5/Classification.ipynb)</span>, <span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=90e73c64-855c-4b06-afb2-94da608ecfbf)</span>)</span>
<span id="cb3-223"><a href="#cb3-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-224"><a href="#cb3-224" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sort data into classes, i.e what kind of fruit</span>
<span id="cb3-225"><a href="#cb3-225" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>most datasets can be projected onto a lower dimensial space, for e.g using PCA</span>
<span id="cb3-226"><a href="#cb3-226" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>read sklearn's PCA docs</span>
<span id="cb3-227"><a href="#cb3-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>kNN:</span>
<span id="cb3-228"><a href="#cb3-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Logistic Regression - use <span class="co">[</span><span class="ot">sklearn</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)</span>, main paramater is C which defines how much to regularize the data. Read <span class="co">[</span><span class="ot">this explanation</span><span class="co">](http://adit.io/posts/2016-03-13-Logistic-Regression.html)</span></span>
<span id="cb3-229"><a href="#cb3-229" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Use sklearns <span class="co">[</span><span class="ot">GridSearchCV</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)</span> to find hyperparameters</span>
<span id="cb3-230"><a href="#cb3-230" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>one way to classify: use <span class="co">[</span><span class="ot">PCA</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)</span> to reduce the feature space, then use logistic regression to classify</span>
<span id="cb3-231"><a href="#cb3-231" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>many datatypes, like images, have tons of features, so important to reduce dimensionality.</span>
<span id="cb3-232"><a href="#cb3-232" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">sklearn PCA</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)</span> returns the principal components in order of how much they explain the data:</span>
<span id="cb3-233"><a href="#cb3-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-234"><a href="#cb3-234" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb3-235"><a href="#cb3-235" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb3-236"><a href="#cb3-236" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA(n_components<span class="op">=</span><span class="dv">60</span>) <span class="co"># PCA with no. of components to return</span></span>
<span id="cb3-237"><a href="#cb3-237" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pca.fit_transform(data)</span>
<span id="cb3-238"><a href="#cb3-238" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(pca.explained_variance_ratio_) <span class="co"># how much of variance is explained</span></span>
<span id="cb3-239"><a href="#cb3-239" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb3-240"><a href="#cb3-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-241"><a href="#cb3-241" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sklearn uses the same interface for all its classifiers, so makes it easy to put a wrapper around gridsearchCV and pass in diff classifiers to compare.</span>
<span id="cb3-242"><a href="#cb3-242" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>discriminative classifier - finds the decision boundary b/w classes</span>
<span id="cb3-243"><a href="#cb3-243" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>maximum-margin classifier - for many classifincation problems, multiplie diff lines can seperate classes. Choose that line where the margin b/w classes is the largest, which makes this model senstive to boundaries b/w classes, rather than to point samples deep inside a class.</span>
<span id="cb3-244"><a href="#cb3-244" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>SVM is a discrimant classier which finds the widest possible margin b/w classes, including some points touching the boundary which are called the support vectors. (since they support or establish the margins.)</span>
<span id="cb3-245"><a href="#cb3-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-246"><a href="#cb3-246" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-247"><a href="#cb3-247" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 10: SVM, Evaluation ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=f21fcc8f-93a8-49f6-9ff8-0f339b0728bd), [slides](https://github.com/cs109/2015/raw/master/Lectures/10-SVMAndEvaluation.pdf))</span></span>
<span id="cb3-248"><a href="#cb3-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-249"><a href="#cb3-249" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>KNN - training is fast, prediction slow since we need to check all the data points to find the nearest neighbours</span>
<span id="cb3-250"><a href="#cb3-250" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>but if we know the decision boundary (the seperating hyperplane) we don't need all the data points</span>
<span id="cb3-251"><a href="#cb3-251" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>w: weight vector defines the orientation of the hyperplane, and bias b.</span>
<span id="cb3-252"><a href="#cb3-252" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>so a new point x is classified by <span class="in">`w(transpose)*x + b`</span></span>
<span id="cb3-253"><a href="#cb3-253" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>this is the mathematical model of a neuron, invented 1957 by Rosenblatt</span>
<span id="cb3-254"><a href="#cb3-254" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>step function vs sigmoid activation</span>
<span id="cb3-255"><a href="#cb3-255" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Support Vector Machines (SVM) are widely used, some consider it best of the shelf classifier. They add a new dimension to help seperate classes and also use maximum margin classification. SVM is called svm becuase of the support vectors defining the max margin lines for the classification boundary.</span>
<span id="cb3-256"><a href="#cb3-256" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>large data is good for training svm as the points on the boundary are rare and svm cares about establishing the boundary</span>
<span id="cb3-257"><a href="#cb3-257" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>since outliers can change the svm boundaries, there is a concept of slack variables - it allows the SVM to missclassify outliers to make a neat decision boundary. sklearn uses the parameter C to define the slack. the lower the number the more the slack.</span>
<span id="cb3-258"><a href="#cb3-258" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>kernel tricks for svm - go to aribitarily mary dimensions with little computational cost. need to think about what kernel to use. Read <span class="co">[</span><span class="ot">What are kernels in machine learning and SVM and why do we need them?</span><span class="co">](https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them)</span>.</span>
<span id="cb3-259"><a href="#cb3-259" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>read <span class="co">[</span><span class="ot">Andrew Ng's cs229 svm notes</span><span class="co">](http://cs229.stanford.edu/notes/cs229-notes3.pdf)</span></span>
<span id="cb3-260"><a href="#cb3-260" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>todo: tale sklearns 'faces' dataset and use svm to predict</span>
<span id="cb3-261"><a href="#cb3-261" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>svm tips:</span>
<span id="cb3-262"><a href="#cb3-262" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>normalize data to 0,1 or -1,1 interval. (check whether the library already normalizes)</span>
<span id="cb3-263"><a href="#cb3-263" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>RBF kernel is a good default</span>
<span id="cb3-264"><a href="#cb3-264" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Read Chich-Wei Hsu practical guide to SVM</span>
<span id="cb3-265"><a href="#cb3-265" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>tune paramaters - which kernel, what parameters for it and what C?</span>
<span id="cb3-266"><a href="#cb3-266" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>ROC curve: plot true positive rate vs false positive rate</span>
<span id="cb3-267"><a href="#cb3-267" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>true +ve is tp/(tp+fn)</span>
<span id="cb3-268"><a href="#cb3-268" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>false +ve is fp/(fp+tn)</span>
<span id="cb3-269"><a href="#cb3-269" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>one useful summary stat is area under the curve</span>
<span id="cb3-270"><a href="#cb3-270" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Precision Recall Curve <span class="co">[</span><span class="ot">sklearn</span><span class="co">](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)</span>, <span class="co">[</span><span class="ot">quora</span><span class="co">](https://www.quora.com/What-is-Precision-Recall-PR-curve)</span></span>
<span id="cb3-271"><a href="#cb3-271" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Precision: <span class="in">`tp/(tp+fp)`</span> - how much are we getting right, or the probability a a random +ve sample is actually +ve (since some +ve samples are false positives).</span>
<span id="cb3-272"><a href="#cb3-272" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Recall <span class="in">`tp/(tp+fn)`</span> - same as in the ROC, how much of the data are we finding. for a random +ve sameple, the probability that it's making a correct prediction. (consider the false negatives.)</span>
<span id="cb3-273"><a href="#cb3-273" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>ideally we want both to be one.</span>
<span id="cb3-274"><a href="#cb3-274" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>good way to compare classifiers</span>
<span id="cb3-275"><a href="#cb3-275" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do you classify multple classes with a SVM, e.g 10 types of fruit</span>
<span id="cb3-276"><a href="#cb3-276" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>One vs all - pick one class, and train it against all the other classes one by one. So you train n classifers for n classes.</span>
<span id="cb3-277"><a href="#cb3-277" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>One vs One - Train n(n-1)/2 classifiers, take majority vote</span>
<span id="cb3-278"><a href="#cb3-278" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>use a confusion matrix of predicted label vs true labels to see classification results</span>
<span id="cb3-279"><a href="#cb3-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-280"><a href="#cb3-280" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 11: Decision Trees and Random Forests ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=8892a8b7-25eb-4bc5-80b6-47b9cf681a05), [slides](https://github.com/khalido/cs109-2015/blob/master/Lectures/11-DecisionTreesAndRandomForest.pdf))</span></span>
<span id="cb3-281"><a href="#cb3-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-282"><a href="#cb3-282" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Books: Read <span class="co">[</span><span class="ot">Elements of Statiscal Learning</span><span class="co">](https://web.stanford.edu/~hastie/ElemStatLearn/)</span> and <span class="co">[</span><span class="ot">Pattern Recognition and Machine Learning</span><span class="co">](https://www.microsoft.com/en-us/research/people/cmbishop/)</span></span>
<span id="cb3-283"><a href="#cb3-283" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decision Tree - fast training and prediction, easy to understand and interpret. DT basically paritions a feature space into cells using a series of decision rules on one feature at a time</span>
<span id="cb3-284"><a href="#cb3-284" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>which feature to query and what thresholds to use?</span>
<span id="cb3-285"><a href="#cb3-285" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>node purity: do splits so cells are pure, i.e have only one class in them</span>
<span id="cb3-286"><a href="#cb3-286" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Gini Impurity gives us the expected error of predicted a class from a random sample</span>
<span id="cb3-287"><a href="#cb3-287" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Gini Index</span>
<span id="cb3-288"><a href="#cb3-288" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Node purity gain: compare gini impurity of parent vs child nodes. Lets us see whether a split has improved classification better than a simple missclassification number.</span>
<span id="cb3-289"><a href="#cb3-289" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Optimal trees - diff ways to get there</span>
<span id="cb3-290"><a href="#cb3-290" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Tree pruning - easy to overfit, so first we make the tree, then go backwards and remove 'bad' decisions, or merge cells etc.</span>
<span id="cb3-291"><a href="#cb3-291" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DT disadvatages: sensitive to small changes in data, overfit, only axis aligned splits</span>
<span id="cb3-292"><a href="#cb3-292" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DT vs SVM:</span>
<span id="cb3-293"><a href="#cb3-293" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Netflix prize winners used an ensemble of over 800 models. somewhat disapointing as they didn't come up with a new method</span>
<span id="cb3-294"><a href="#cb3-294" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>DT doesn't perform well, but what if we use many of them?</span>
<span id="cb3-295"><a href="#cb3-295" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bootstrap is one way to do this. It's a resampling method from statistics, useful to get error bags on estimates.</span>
<span id="cb3-296"><a href="#cb3-296" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Bootstrap lets us generate more sample sets from one dataset, each one slightly different.</span>
<span id="cb3-297"><a href="#cb3-297" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Take N data points and draw N times with replacement, then get an estimate from each bootstrapped samples.</span>
<span id="cb3-298"><a href="#cb3-298" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>bagging: bootstrap aggregrating, where you learn a classifer from each bootstrap sample and average the . (normally uses just one type of classifier)</span>
<span id="cb3-299"><a href="#cb3-299" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>See <span class="co">[</span><span class="ot">bootstrap example notebook</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/Bootstrapping.ipynb)</span></span>
<span id="cb3-300"><a href="#cb3-300" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>bagged DT's perform better than one a single tree</span>
<span id="cb3-301"><a href="#cb3-301" aria-hidden="true" tabindex="-1"></a><span class="ss">        - </span>not useful for linear models</span>
<span id="cb3-302"><a href="#cb3-302" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Bias-Variance trade off: train models with a high variance, then the average might get close</span>
<span id="cb3-303"><a href="#cb3-303" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Random Forest** builds on bagging, builds a tree from each bootstrap sample with node splits selected from random feature subsets. See <span class="co">[</span><span class="ot">1</span><span class="co">](http://blog.yhat.com/posts/random-forests-in-python.html)</span>, or rather <span class="co">[</span><span class="ot">2</span><span class="co">](https://chrisalbon.com/machine-learning/random_forest_classifier_example_scikit.html)</span></span>
<span id="cb3-304"><a href="#cb3-304" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-305"><a href="#cb3-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-306"><a href="#cb3-306" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 7: Machine Learning best practices</span></span>
<span id="cb3-307"><a href="#cb3-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-308"><a href="#cb3-308" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">HW 3 Q1 due</span><span class="co">](https://github.com/khalido/cs109-2015/blob/master/homework/HW3.ipynb)</span>: a lot of pandas manipulation on baseball data</span>
<span id="cb3-309"><a href="#cb3-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-310"><a href="#cb3-310" aria-hidden="true" tabindex="-1"></a>Start the <span class="co">[</span><span class="ot">project</span><span class="co">](http://cs109.github.io/2015/pages/projects.html)</span></span>
<span id="cb3-311"><a href="#cb3-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-312"><a href="#cb3-312" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Towards the end of the course you will work on a month-long data science project. The goal of the project is to go through the complete data science process to answer questions you have about some topic of your own choosing. You will acquire the data, design your visualizations, run statistical analysis, and communicate the results. You will work closely with other classmates in a 3-4 person project team.</span></span>
<span id="cb3-313"><a href="#cb3-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-314"><a href="#cb3-314" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lab 6: Machine Learning 2</span></span>
<span id="cb3-315"><a href="#cb3-315" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-316"><a href="#cb3-316" aria-hidden="true" tabindex="-1"></a>**Classification, Probabilities, ROC curves, and Cost** (<span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=83dfe4c9-fa1b-429c-a84c-839195fbede8)</span>,<span class="co">[</span><span class="ot">notebook</span><span class="co">](https://github.com/cs109/2015lab6/blob/master/lab6-classification-redux.ipynb)</span>)</span>
<span id="cb3-317"><a href="#cb3-317" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-318"><a href="#cb3-318" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Not a very useful lab, essentially better of from sciki-learn understand how to use:</span>
<span id="cb3-319"><a href="#cb3-319" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Logistic Regression</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)</span></span>
<span id="cb3-320"><a href="#cb3-320" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">SVM</span><span class="co">](http://scikit-learn.org/stable/modules/svm.html)</span>, or <span class="co">[</span><span class="ot">swm with kernels</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)</span> - <span class="co">[</span><span class="ot">good tutorial</span><span class="co">](https://www.oreilly.com/learning/intro-to-svm)</span></span>
<span id="cb3-321"><a href="#cb3-321" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span><span class="co">[</span><span class="ot">Confusion Matrix</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)</span></span>
<span id="cb3-322"><a href="#cb3-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-323"><a href="#cb3-323" aria-hidden="true" tabindex="-1"></a>**Comparing Models** (<span class="co">[</span><span class="ot">video</span><span class="co">](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=4676adde-557f-469b-bca8-5ffe868094eb)</span>, <span class="co">[</span><span class="ot">notebook</span><span class="co">](https://github.com/cs109/2015lab6/blob/master/lab6-churn.ipynb)</span>)</span>
<span id="cb3-324"><a href="#cb3-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-325"><a href="#cb3-325" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>learn Bayes classifiers in sklean</span>
<span id="cb3-326"><a href="#cb3-326" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>use <span class="co">[</span><span class="ot">ROC</span><span class="co">](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)</span> curves - often accuracy is the the relevant, the true +ve and false -ve rate is more important. Since False negatives can be costly, you often want to change the threshold probability from the default 0.5. So write your own prediction function as the sklearn one uses 0.5, or with bayes classifiers adjust the prior probability.</span>
<span id="cb3-327"><a href="#cb3-327" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>sklearn has a <span class="co">[</span><span class="ot">roc function</span><span class="co">](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py)</span>, <span class="co">[</span><span class="ot">tutorial</span><span class="co">](http://benalexkeen.com/scoring-classifier-models-using-scikit-learn/)</span></span>
<span id="cb3-328"><a href="#cb3-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-329"><a href="#cb3-329" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 12: Ensemble Methods (([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=4831ebf0-7832-42c5-9339-5b5e08dd3e92), [slides](https://github.com/cs109/2015/blob/master/Lectures/12-Ensemble%20Learning%20and%20Random%20Forests.pdf)))</span></span>
<span id="cb3-330"><a href="#cb3-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-331"><a href="#cb3-331" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>philisophical point: who do ensenble methods work so well? in real life wisdom of the crowds is overrated, but it does a lot better in computerland. Some averaging methods pick up useful stuff in the data, while others cancel out each others errors.</span>
<span id="cb3-332"><a href="#cb3-332" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Decision trees are easy but have poor predictive accuracy, tend to ovefit</span>
<span id="cb3-333"><a href="#cb3-333" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Ensemble learning combines many learners using methods like weighted avg, boosting, etc. See <span class="co">[</span><span class="ot">sklearn's ensemble page</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html)</span></span>
<span id="cb3-334"><a href="#cb3-334" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">random forests</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html#forest)</span> is an extension of bagging of decision trees</span>
<span id="cb3-335"><a href="#cb3-335" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>random forests using sqrt(features) of random predicters to make trees give the best results</span>
<span id="cb3-336"><a href="#cb3-336" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Boosting is another ensemble method like bagging, but better for most applications.</span>
<span id="cb3-337"><a href="#cb3-337" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>tune by num of trees, splits in each tree, weights</span>
<span id="cb3-338"><a href="#cb3-338" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>often using very simple trees, which you adjust the weights as u make more trees to classify better the things u got wrong. At the end u make a weighted avg.</span>
<span id="cb3-339"><a href="#cb3-339" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>most popular &amp; succesful boosting algorithim is <span class="co">[</span><span class="ot">AdaBoost</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)</span></span>
<span id="cb3-340"><a href="#cb3-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-341"><a href="#cb3-341" aria-hidden="true" tabindex="-1"></a>Note: read <span class="co">[</span><span class="ot">this series on machine learning</span><span class="co">](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)</span></span>
<span id="cb3-342"><a href="#cb3-342" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-343"><a href="#cb3-343" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 13: Best Practices ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=b33eec92-d049-4353-a904-5054eb718aff), [slides](https://github.com/cs109/2015/blob/master/Lectures/13-BestPractices_Recommendations.pdf))</span></span>
<span id="cb3-344"><a href="#cb3-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-345"><a href="#cb3-345" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>story telling is important for data scientists - explain whats going on, even in your own notebooks. good presentation is very important.</span>
<span id="cb3-346"><a href="#cb3-346" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Diff b/w bagging and random forest: rf has bagging idea + random feature subsets</span>
<span id="cb3-347"><a href="#cb3-347" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>didn't really find this video that useful, for example:</span>
<span id="cb3-348"><a href="#cb3-348" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>knn and trees can be used for regression too, but why would we?</span>
<span id="cb3-349"><a href="#cb3-349" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>SVM's can be used for regression</span>
<span id="cb3-350"><a href="#cb3-350" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>take care to normalize your data in a sane manner</span>
<span id="cb3-351"><a href="#cb3-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-352"><a href="#cb3-352" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 8: EC2 and Spark</span></span>
<span id="cb3-353"><a href="#cb3-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-354"><a href="#cb3-354" aria-hidden="true" tabindex="-1"></a>HW3 <span class="co">[</span><span class="ot">q2</span><span class="co">](https://nbviewer.jupyter.org/github/khalido/cs109-2015/blob/master/homework/HW3.ipynb#Problem-2)</span> uses the iris data set &amp; <span class="co">[</span><span class="ot">q3</span><span class="co">](https://nbviewer.jupyter.org/github/khalido/cs109-2015/blob/master/homework/HW3.ipynb#Problem-3:)</span> uses sklearn's digits dataset and gridsearchCV to find best parameters for a KNN classifier for two simple datasets.</span>
<span id="cb3-355"><a href="#cb3-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-356"><a href="#cb3-356" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lab 7: Decision Trees, Random Forests, Ensemble Methods ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=0c645a5c-d262-4bb5-a137-3a78db60f3e7), [notebook](https://github.com/khalido/cs109-2015/blob/master/Labs/2015lab7/Lab7-Botany%20and%20Ensemble%20Methods.ipynb))</span></span>
<span id="cb3-357"><a href="#cb3-357" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-358"><a href="#cb3-358" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">decision trees</span><span class="co">](http://scikit-learn.org/stable/modules/tree.html#tree)</span>, but they use their own function on top of sklearn so its a bit annoying</span>
<span id="cb3-359"><a href="#cb3-359" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">random forests</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html#forest)</span>:</span>
<span id="cb3-360"><a href="#cb3-360" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>take a random subsample of the data</span>
<span id="cb3-361"><a href="#cb3-361" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>select a subset of all possible variables (at each node) and build the tree on the best split</span>
<span id="cb3-362"><a href="#cb3-362" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>repeat, then finally take a majority vote</span>
<span id="cb3-363"><a href="#cb3-363" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Ensemble</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html)</span> learning - put together a lot of classifiers to build a better one</span>
<span id="cb3-364"><a href="#cb3-364" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">AdaBoost Classifier</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html#adaboost)</span> uses weights on the training data.</span>
<span id="cb3-365"><a href="#cb3-365" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Gradient Boost Classifier</span><span class="co">](http://scikit-learn.org/stable/modules/ensemble.html#gradient-tree-boosting)</span> is similar to AdaBoost but uses tree as its base classifier, can do regression or classification.</span>
<span id="cb3-366"><a href="#cb3-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-367"><a href="#cb3-367" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 14: Best Practices, Recommendations and MapReduce ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=afee45b9-dcf5-4f29-bc60-871aa78f1cf8), [slides](https://github.com/cs109/2015/raw/master/Lectures/14-Recommendations_MapReduce.pdf))</span></span>
<span id="cb3-368"><a href="#cb3-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-369"><a href="#cb3-369" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>should have started final project by now, if not, start now!</span>
<span id="cb3-370"><a href="#cb3-370" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>**Nesting:** use 5 fold cross validation, take each fold and further apply 5 fold cv to find the right hyperparameters, then use those params in the original fold to train the classifier.</span>
<span id="cb3-371"><a href="#cb3-371" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>think about how and why to normalize data, e.g data already in a range from 0-1 might not need to be normalized, think about normazlization as hyperparameters to your model.</span>
<span id="cb3-372"><a href="#cb3-372" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>get the mean estimates from your training data and use that to normalize training, validation and testing data. these values need to be stored to normalize future data.</span>
<span id="cb3-373"><a href="#cb3-373" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>know your application domain</span>
<span id="cb3-374"><a href="#cb3-374" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>many problems are to do with imbalanced data. Fixes to get balanced data for training:</span>
<span id="cb3-375"><a href="#cb3-375" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>oversample: take bigger samples of the sparse clasess</span>
<span id="cb3-376"><a href="#cb3-376" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>subsampling: take smaller samples of the more frequently occuring classes</span>
<span id="cb3-377"><a href="#cb3-377" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>or weight classes so classifier pays more attention to the sparese classes, see <span class="co">[</span><span class="ot">sklearn</span><span class="co">](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold)</span></span>
<span id="cb3-378"><a href="#cb3-378" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Missing data, where some of the features are missing for certain samples:</span>
<span id="cb3-379"><a href="#cb3-379" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>delete, but can reduce data too much</span>
<span id="cb3-380"><a href="#cb3-380" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>use mean</span>
<span id="cb3-381"><a href="#cb3-381" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>use regression to estimate missing values</span>
<span id="cb3-382"><a href="#cb3-382" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Collobrative filtering</span><span class="co">](https://en.wikipedia.org/wiki/Collaborative_filtering)</span> is a common technique for recommendations or filling in missing values. Can be user based or item based.</span>
<span id="cb3-383"><a href="#cb3-383" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>KNN is widely used for collobrative filtering.</span>
<span id="cb3-384"><a href="#cb3-384" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>SVD is widely used, famously in the netflix contest. watch this <span class="co">[</span><span class="ot">svd video</span><span class="co">](https://www.youtube.com/watch?v=YKmkAoIUxkU)</span> for a refresher if needed.</span>
<span id="cb3-385"><a href="#cb3-385" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-386"><a href="#cb3-386" aria-hidden="true" tabindex="-1"></a>Moving on from Machine Learning...</span>
<span id="cb3-387"><a href="#cb3-387" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-388"><a href="#cb3-388" aria-hidden="true" tabindex="-1"></a>Map reduce is a way to deal with very large data sets by distributing work over many machines. Developed by Google, and Apache Hadoop is a open source implementation.</span>
<span id="cb3-389"><a href="#cb3-389" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-390"><a href="#cb3-390" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>data is in key value pairs and is distributed to however many servers</span>
<span id="cb3-391"><a href="#cb3-391" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we write a map function which does something to a key value pair.</span>
<span id="cb3-392"><a href="#cb3-392" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the mapreduce implementation reaggregrates the key value pairs incoming from all the servers (sorted by keys)</span>
<span id="cb3-393"><a href="#cb3-393" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>we write a reduce which does something else</span>
<span id="cb3-394"><a href="#cb3-394" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>simple mapreduce example:</span>
<span id="cb3-395"><a href="#cb3-395" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>map takes in sentences, sends them off to diff machines to process,</span>
<span id="cb3-396"><a href="#cb3-396" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>those machines send back key:value pairs, say we are just counting words so we get back from machine 1: <span class="in">`"the":1`</span>, <span class="in">`"the":1`</span>, <span class="in">`"a":4`</span> and machine 2 sends back "the":4, "a"10,</span>
<span id="cb3-397"><a href="#cb3-397" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>there can be a combine step here, which takes the output from one mapper and combines it, so the two <span class="in">`the`</span> words from machine 1 become one <span class="in">`"the":2`</span>. this reduces network traffic and makes the work upstream easier. the combine output has to be of the same type as the mapper, since combine is just an optional  optimizatizion step not a DO WORK thing.</span>
<span id="cb3-398"><a href="#cb3-398" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>the mapreduce thingamajig aggregrates all the key:value pairs and sends them to the</span>
<span id="cb3-399"><a href="#cb3-399" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>reduce function, which counts the words and finally we end up with "the":6 and "a":14</span>
<span id="cb3-400"><a href="#cb3-400" aria-hidden="true" tabindex="-1"></a><span class="al">![](/img/mapreduce.png)</span></span>
<span id="cb3-401"><a href="#cb3-401" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>Udacity has a <span class="co">[</span><span class="ot">course on mapreduce with hadoop</span><span class="co">](https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617)</span></span>
<span id="cb3-402"><a href="#cb3-402" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-403"><a href="#cb3-403" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-404"><a href="#cb3-404" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 15: MapReduce Combiners and Spark ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=10964b32-dcdc-49d2-b766-039b707a5c37), [slides](https://github.com/cs109/2015/raw/master/Lectures/15a-MapReduce_Combiner.pdf))</span></span>
<span id="cb3-405"><a href="#cb3-405" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-406"><a href="#cb3-406" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>refresher on mapreduce: it takes away the headache of dealing with machines, clusters, etc</span>
<span id="cb3-407"><a href="#cb3-407" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>be careful on how you use reducers, they shouldn't effect the algorithims effectiveness</span>
<span id="cb3-408"><a href="#cb3-408" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>the reducer should work whether or not the combiner runs</span>
<span id="cb3-409"><a href="#cb3-409" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>you could have a in-mapper combining, but here we are gettting into complex territory</span>
<span id="cb3-410"><a href="#cb3-410" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Why did we switch to map reduce from good old write a function and run it computing to the horror that is map reduce?</span>
<span id="cb3-411"><a href="#cb3-411" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>large large data which looms like a t-rex, scaring our current teeny tiny computers cowering away in fear. luckily the computers gathered in data centers where mapreduce and other such algos can harness them in parallel.</span>
<span id="cb3-412"><a href="#cb3-412" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>many problems are embarrisingly parallel</span>
<span id="cb3-413"><a href="#cb3-413" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Apache Hadoop</span><span class="co">](http://hadoop.apache.org/)</span> is an open source implementation of map reduce, built way back in 2006. It provides us a way to store data on clusters of commodity machines and process it using mapreduce, without having to worry about the rocketscience of managing the clusters and splitting data and processes across all the machines.</span>
<span id="cb3-414"><a href="#cb3-414" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Though these days you probably want to use Hadoop with <span class="co">[</span><span class="ot">Apache Spark</span><span class="co">](http://spark.apache.org/)</span> running on <span class="co">[</span><span class="ot">Amazon's EMR</span><span class="co">](https://aws.amazon.com/emr/)</span> or <span class="co">[</span><span class="ot">Google Cloud</span><span class="co">](https://cloud.google.com/dataproc/)</span>.</span>
<span id="cb3-415"><a href="#cb3-415" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>python has tools like <span class="co">[</span><span class="ot">dask</span><span class="co">](https://ipyparallel.readthedocs.io/en/latest/)</span> and <span class="co">[</span><span class="ot">ipyparallel</span><span class="co">](https://ipyparallel.readthedocs.io/en/latest/)</span> for parallel computing. for many projects python is enough</span>
<span id="cb3-416"><a href="#cb3-416" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Functional programming: where your functions take an input, process it and return an output, without messing around with other functions or data stored elsewhere. this makes so much sense that it shouldn't be a big deal, it sounds like a toyota corolla of the computing world, but its a big deal because ppl manage to write functions which fuck around with your programming world enough that after a few rounds of their unfunctional running around no one has a good idea of whats the actual state of things.</span>
<span id="cb3-417"><a href="#cb3-417" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Functional programming is clean! Watch Joel Grus <span class="co">[</span><span class="ot">Learning Data Science Using Functional Python</span><span class="co">](http://pyvideo.org/pydata-seattle-2015/learning-data-science-using-functional-python.html)</span> or <span class="co">[</span><span class="ot">this one</span><span class="co">](https://www.youtube.com/watch?v=LkpHQL863mw)</span></span>
<span id="cb3-418"><a href="#cb3-418" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Getting back to mapreduce, when you are programming functionally, then it makes it possible to distribute the data and computing across different machines as the functions aren't trying to mess with each other. When a machine dies, its no big deal as we know what data and function we sent it to process, we just resend it to some other machine.</span>
<span id="cb3-419"><a href="#cb3-419" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>aside: this is why pandas always outputs a new dataframe whenever we do something, trying to emulate this ideal of not fucking up existing things. Spark also does similar stuff.</span>
<span id="cb3-420"><a href="#cb3-420" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Apache Spark</span><span class="co">](http://spark.apache.org/)</span> makes all this easy for us. Runs on top of Hadoop and provides nice interface and methods for all kind of stuff. So nicer, faster, shineir than Hadoop.</span>
<span id="cb3-421"><a href="#cb3-421" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Spark stores data on a Resilient distributed dataset (RDD) - a fault tolerant collection of stuff which can be operated on in parallel.</span>
<span id="cb3-422"><a href="#cb3-422" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>the basics of using sparl: write a function, some kind of mapreduce job, spark runs it on the RDD and makes a new processed RDD.</span>
<span id="cb3-423"><a href="#cb3-423" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>a lot of the pandas style commands work on spark</span>
<span id="cb3-424"><a href="#cb3-424" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>spark is lazy - it makes a graph of the stuff we want to do and then runs it from start to end every time we execute. so caching is important so we don't keep computing same stuff over and over again</span>
<span id="cb3-425"><a href="#cb3-425" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>explore spark…. _todo_</span>
<span id="cb3-426"><a href="#cb3-426" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-427"><a href="#cb3-427" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 9: Bayes!</span></span>
<span id="cb3-428"><a href="#cb3-428" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-429"><a href="#cb3-429" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lab 8: Vagrant and VirtualBox, AWS, and Spark ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=6f1deb9e-051b-4019-99ec-19aee14be9d9), [notebook]())</span></span>
<span id="cb3-430"><a href="#cb3-430" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-431"><a href="#cb3-431" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Moving on from sklearn...</span>
<span id="cb3-432"><a href="#cb3-432" aria-hidden="true" tabindex="-1"></a>-</span>
<span id="cb3-433"><a href="#cb3-433" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-434"><a href="#cb3-434" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 16: Bayes Theorem and Bayesian Methods ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=233f6c34-306f-481b-8ea5-be33076eb6a8), [slides](https://github.com/cs109/2015/raw/master/Lectures/16-BayesianMethods.pdf))</span></span>
<span id="cb3-435"><a href="#cb3-435" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-436"><a href="#cb3-436" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>book recommendations from the lecture: _(the one I liked best in bold)_</span>
<span id="cb3-437"><a href="#cb3-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-438"><a href="#cb3-438" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>easy reading: <span class="co">[</span><span class="ot">How Bayes’ Rule Cracked the Enigma Code</span><span class="co">](https://www.amazon.com/Theory-That-Would-Not-Die-ebook/dp/B0050QB3EQ/ref=tmm_kin_swatch_0?_encoding=UTF8&amp;qid=&amp;sr=)</span> also see her <span class="co">[</span><span class="ot">talk at google</span><span class="co">](https://www.youtube.com/watch?v=8oD6eBkjF9o)</span>.</span>
<span id="cb3-439"><a href="#cb3-439" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>simple stuff, in python: <span class="co">[</span><span class="ot">Think Bayes</span><span class="co">](http://greenteapress.com/wp/think-bayes/)</span>, though maybe too basic</span>
<span id="cb3-440"><a href="#cb3-440" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>**[Probabilistic Programming &amp; Bayesian Methods for Hackers](http://camdavidsonpilon.github.io/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/) - text and python code is in [jupyter notebooks](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers) which u can clone to your own pc and go through.**</span>
<span id="cb3-441"><a href="#cb3-441" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>Proper textbooks: <span class="co">[</span><span class="ot">Bayesian Data Analysis</span><span class="co">](http://www.stat.columbia.edu/~gelman/book/)</span> and <span class="co">[</span><span class="ot">Statistical Rethinking</span><span class="co">](http://xcelab.net/rm/statistical-rethinking/)</span> - this comes with <span class="co">[</span><span class="ot">python examples</span><span class="co">](https://github.com/aloctavodia/Statistical-Rethinking-with-Python-and-PyMC3)</span>.</span>
<span id="cb3-442"><a href="#cb3-442" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-443"><a href="#cb3-443" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Bayes rule: P(A|B) = P(B|A)P(A) / P(B)</span>
<span id="cb3-444"><a href="#cb3-444" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-445"><a href="#cb3-445" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>bayes rules tells us how to update our beliefs (the prior) as we get new data, which gives us a new posterior (which is just a fancy word for our new, updated belief). A more <span class="co">[</span><span class="ot">wordy description</span><span class="co">](http://www.nytimes.com/2011/08/07/books/review/the-theory-that-would-not-die-by-sharon-bertsch-mcgrayne-book-review.html)</span>:</span>
<span id="cb3-446"><a href="#cb3-446" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-447"><a href="#cb3-447" aria-hidden="true" tabindex="-1"></a><span class="at">  &gt; The theorem itself can be stated simply. Beginning with a provisional hypothesis about the world (there are, of course, no other kinds), we assign to it an initial probability called the prior probability or simply the prior. After actively collecting or happening upon some potentially relevant evidence, we use Bayes’s theorem to recalculate the probability of the hypothesis in light of the new evidence. This revised probability is called the posterior probability or simply the posterior.</span></span>
<span id="cb3-448"><a href="#cb3-448" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-449"><a href="#cb3-449" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>bayes is controversial becuase traditional stats doesn't like giving numbers to unknown thinks, for example bayes essentially makes up the prior. the prior is often our subject belief about something.</span>
<span id="cb3-450"><a href="#cb3-450" aria-hidden="true" tabindex="-1"></a><span class="ss">    - </span>however, even when starting out with different priors, given that they aren't ridiculously dogmatic, with a large sample size the different priors will converge</span>
<span id="cb3-451"><a href="#cb3-451" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>discriminative model: focus on predicting y given x, generative model: we simulate the entire model, i.e we can generate x and y</span>
<span id="cb3-452"><a href="#cb3-452" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>naive bayes: assumes probablities are conditionally independent of each other, greatly simplifies the calculations. sometimes unrealistic but works well for many scenarios. <span class="co">[</span><span class="ot">sklearn has bayes, of course</span><span class="co">](http://scikit-learn.org/stable/modules/naive_bayes.html)</span>.</span>
<span id="cb3-453"><a href="#cb3-453" aria-hidden="true" tabindex="-1"></a><span class="ss">  -  </span>Conjugate prior says that if u start with a family of distributions, like beta, you stay in the same distribution. simplifies computations</span>
<span id="cb3-454"><a href="#cb3-454" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>one way to think about bayesian</span>
<span id="cb3-455"><a href="#cb3-455" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-456"><a href="#cb3-456" aria-hidden="true" tabindex="-1"></a>More Reading:</span>
<span id="cb3-457"><a href="#cb3-457" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-458"><a href="#cb3-458" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Count Baye's intro to bayesian statistics</span><span class="co">](https://www.countbayesie.com/blog/2016/5/1/a-guide-to-bayesian-statistics)</span></span>
<span id="cb3-459"><a href="#cb3-459" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-460"><a href="#cb3-460" aria-hidden="true" tabindex="-1"></a><span class="fu">### Lecture 17: Bayesian Methods Continued ([video](https://matterhorn.dce.harvard.edu/engage/player/watch.html?id=e63c650e-4cf6-4bee-b57e-f16e927ba25a), [slides](https://github.com/cs109/2015/raw/master/Lectures/17BayesianMethodsContinued.pdf))</span></span>
<span id="cb3-461"><a href="#cb3-461" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-462"><a href="#cb3-462" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>you can estimate your prior from the data, though some bayesians would say you're tainting your priors and the data by doing that, but this is an accepted way to get an acceptable prior.</span>
<span id="cb3-463"><a href="#cb3-463" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the lecture works through examples from a blog, which has collected its bayes posts into this book: <span class="co">[</span><span class="ot">Introduction to Empirical Bayes: Examples from Baseball Statistics</span><span class="co">](http://varianceexplained.org/r/empirical-bayes-book/)</span>. the explanations in the book look great.</span>
<span id="cb3-464"><a href="#cb3-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-465"><a href="#cb3-465" aria-hidden="true" tabindex="-1"></a>Note: Bayes is simple to do yet hard to understand. So read a number of guides/blogs/posts/<span class="co">[</span><span class="ot">youtubes</span><span class="co">](http://pyvideo.org/search.html?q=bayes)</span> till it makes sense. Some talks to see:</span>
<span id="cb3-466"><a href="#cb3-466" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-467"><a href="#cb3-467" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Eric J Ma Bayesian Statistical Analysis with Python PyCon 2017</span><span class="co">](https://www.youtube.com/watch?v=p1IB4zWq9C8)</span> - 30 min talk, uses PyMC3</span>
<span id="cb3-468"><a href="#cb3-468" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Christopher Fonnesbeck Probabilistic Programming with PyMC3 PyCon 2017</span><span class="co">](https://www.youtube.com/watch?v=5TyvJ6jXHYE)</span> - 30min, more PyMC3</span>
<span id="cb3-469"><a href="#cb3-469" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-470"><a href="#cb3-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-471"><a href="#cb3-471" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-472"><a href="#cb3-472" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 10: Text</span></span>
<span id="cb3-473"><a href="#cb3-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-474"><a href="#cb3-474" aria-hidden="true" tabindex="-1"></a>hw5 - did this with my group, so need to redo the hw and commit to my own github.</span>
<span id="cb3-475"><a href="#cb3-475" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-476"><a href="#cb3-476" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 11: Clustering!</span></span>
<span id="cb3-477"><a href="#cb3-477" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-478"><a href="#cb3-478" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 12: Deep Learning</span></span>
<span id="cb3-479"><a href="#cb3-479" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-480"><a href="#cb3-480" aria-hidden="true" tabindex="-1"></a>Used tensorflow and keras. update repo.</span>
<span id="cb3-481"><a href="#cb3-481" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-482"><a href="#cb3-482" aria-hidden="true" tabindex="-1"></a><span class="fu">## Week 13: Final Project &amp; Wrapup</span></span>
<span id="cb3-483"><a href="#cb3-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-484"><a href="#cb3-484" aria-hidden="true" tabindex="-1"></a>My final project was a proof of concept bot which ingested your bank transactions and answered questions about your money. It used wit.ai to parse user queries, machine learning to categorize transactions and straight forward scipy to crunch numbers and make graphs using matplotlib and seaborn. It was a fun learning excercise, to make something which lived briefly live on facebook messenger and could answer questions. using wit.ai made the NLP easy, though with more time writing my own NLP parser or using one of the many offline libraries would be a good learning excercise.</span>
<span id="cb3-485"><a href="#cb3-485" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-486"><a href="#cb3-486" aria-hidden="true" tabindex="-1"></a><span class="fu"># Additional Resources</span></span>
<span id="cb3-487"><a href="#cb3-487" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-488"><a href="#cb3-488" aria-hidden="true" tabindex="-1"></a>Stuff I found useful to understand the class material better.</span>
<span id="cb3-489"><a href="#cb3-489" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-490"><a href="#cb3-490" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Computational and Inferential Thinking</span><span class="co">](https://ds8.gitbooks.io/textbook/content/)</span> - the textbook for UC Berkely's <span class="co">[</span><span class="ot">Foundations of Data Science class</span><span class="co">]</span>(http://data8.org/.</span>
<span id="cb3-491"><a href="#cb3-491" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Pythonb Data Science Handbook</span><span class="co">](https://jakevdp.github.io/PythonDataScienceHandbook/)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>