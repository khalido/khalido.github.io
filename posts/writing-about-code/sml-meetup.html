<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KO">
<meta name="dcterms.date" content="2018-08-20">
<meta name="description" content="notes from the meetup">

<title>khalido.org - SML 2018-08: Deep Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">khalido.org</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">About</a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/khalido"><i class="bi bi-github" role="img">
</i> 
 </a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/KO"><i class="bi bi-twitter" role="img">
</i> 
 </a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">SML 2018-08: Deep Reinforcement Learning</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          notes from the meetup
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">meetup</div>
                <div class="quarto-category">data science</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>KO </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 20, 2018</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sml-2018-08-deep-reinforcement-learning" id="toc-sml-2018-08-deep-reinforcement-learning" class="nav-link active" data-scroll-target="#sml-2018-08-deep-reinforcement-learning">SML 2018-08: Deep Reinforcement Learning</a></li>
  <li><a href="#alasdair-hamilton-reinforcement-learning" id="toc-alasdair-hamilton-reinforcement-learning" class="nav-link" data-scroll-target="#alasdair-hamilton-reinforcement-learning">Alasdair Hamilton: Reinforcement learning</a></li>
  <li><a href="#alex-long-deep-reinforcement-learning-zero-to-ppo-in-20-minutes" id="toc-alex-long-deep-reinforcement-learning-zero-to-ppo-in-20-minutes" class="nav-link" data-scroll-target="#alex-long-deep-reinforcement-learning-zero-to-ppo-in-20-minutes">Alex Long: Deep Reinforcement Learning: Zero to PPO in 20 minutes</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="sml-2018-08-deep-reinforcement-learning" class="level1">
<h1>SML 2018-08: Deep Reinforcement Learning</h1>
<p>SMLâ€™s <a href="https://www.meetup.com/Sydney-Machine-Learning/events/252760610/">2018-08 meetup</a>.</p>
<ul>
<li>launching StarAI Deep Reinforcement Learning Course
<ul>
<li>7 week course to take you from RL near-zero to hero</li>
<li>10-15hrs study a week, meet 6-8pm once a week</li>
</ul></li>
</ul>
<p>the talks:</p>
</section>
<section id="alasdair-hamilton-reinforcement-learning" class="level1">
<h1><a href="https://www.linkedin.com/in/alasdair-hamilton-11852a7b/">Alasdair Hamilton</a>: Reinforcement learning</h1>
<blockquote class="blockquote">
<p>The future of Artificial Intelligence is, at least partly, rooted in Reinforcement Learning. During our presentation, the team from Remi AI will take attendees on a journey through their experiences in Reinforcement Learning. Special attention will be paid to the areas in which the team have been able to apply Reinforcement Learning in a commercial setting and the four mammalian methods of learning, and how they inspire us. Remi AI has applied RL to budget and bid management, to dynamic pricing and web design, to large scale management of power requirements, inventory management, predictive maintenance. The talk will conclude with a discussion on future applications, as well as a roadmap for those looking to start out in the space, then Q&amp;A.</p>
</blockquote>
<ul>
<li>founder of <a href="https://www.linkedin.com/company/remi-pty-ltd/">Remi AI</a>, one of Sydneyâ€™s only RL companies.</li>
<li>Remi AI was founded 5 yrs ago, works on applied AI for companies/biz/ngoâ€™s.</li>
<li>RL is a form of AI that includes motivating the agent toward some goal.</li>
<li>current RL agents are incredibly dumb - even though they excel at certain tasks.</li>
<li>Why RL:
<ul>
<li>its been a very long time since humans invented tools - RL is the framework which enables an artificial agent to invent more tools to solve a problem</li>
<li>RL is the framework which humans use internally to learn</li>
<li>RL covers a lot of domains, from engineering to pyschology</li>
<li>RL is modelled around the dopamine reward delivering system in animals/humans</li>
</ul></li>
<li>some real world RL applications: stunt flight, traffic optimisation, Go (see Deepmind), dynamic pricing</li>
<li>the RL agent can change its environment with its own actions. there is no supervisor, ony a reward mechanism.</li>
<li>unlike supervised methods, feedbck is often delayed, so time is extremely important. e.g in a traffic optimsation problem traffic jams can arise a long time after a decision.</li>
<li>Demoed of a RL agent learning breakout, a remote control helicopter and space invadors.</li>
<li>reward hacking - designers can set wrong or low rewards and punishments - e.g if the punishment for crashing is to high, the best stratgey is to never take off.</li>
<li>RL in robotics is changing the field - what took decades to learn incrementally, RL systems are learning insanely quickly.</li>
<li>rewarding an agent: a reward is a scalar feedback signal - we dictate through rewards how well the agent is doing at step t. the best systems combine prediction and reward.</li>
<li>games are a very useful way to researh RL, as you have clear scores which can be tied to rewards and punishments.</li>
<li>RL is a lot harder in the real world in terms of choosing rewards and punishement.</li>
<li>imitating human reward systems can be both useful and incredibly dangerous.</li>
<li>there is quite a range of RL algos.</li>
<li>RL needs a huge amount of training time - agents initially know nothing so there is a lot of trial and error - ideally in simulation since doing this in the real world is very expensive</li>
<li>a lot of RL is building a simulator to train an agent, then taking it to the real world.</li>
<li>todo: research transfer learning in RL.</li>
<li>research happening in hierarchical learning - break up things so learn individual goals which build to the final goal.</li>
<li>Some real world applications of RL:
<ul>
<li>a few cities have implemented deep RL algos in traffic optimisation. RL is also useful for devising plans on how to handle breakdowns, like a road out of commision.</li>
<li>energy reduction - <a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">Googleâ€™s data center</a></li>
<li>supply chain optimisation - warehousing, logistic.</li>
</ul></li>
<li>RL models are simulation agnostic, but transition from simulation to the real world can be extremely troublesome. So:
<ul>
<li>continually review simulators.</li>
<li>where are the reward predictions going wrong?</li>
</ul></li>
</ul>
<p>q &amp; a</p>
<ul>
<li>How do you reward differnt/multiple objectives?
<ul>
<li>for a wide range of objectives, scale up rewards at different levels - example reward individual contract signings and also bigger level revenue goals.</li>
</ul></li>
<li>is RL limited by the complexity of the simulation?
<ul>
<li>some things can be simulated by very simple things like graphs written in python - sims donâ€™t have to be complex and visual.</li>
</ul></li>
<li>arbitary rewards can cause problems - whats a good fix?
<ul>
<li>its a hot area - ppl are figuring out best utility functions for rewards.</li>
</ul></li>
</ul>
</section>
<section id="alex-long-deep-reinforcement-learning-zero-to-ppo-in-20-minutes" class="level1">
<h1><a href="https://www.linkedin.com/in/alex-long-rl/">Alex Long</a>: Deep Reinforcement Learning: Zero to PPO in 20 minutes</h1>
<blockquote class="blockquote">
<p>Deep Reinforcement Learning (Deep-RL) is the combination of traditional RL algorithms with the high-dimensional function approximation methods of deep learning. This combination allows Deep-RL to eclipse human performance on systems of previously intractable state-spaces and high branching factors, such as the game of GO, Atari arcade games, and heads up limit poker. In this talk I will focus on the intuition behind Deep-RL, how it compares (and differs) to other machine learning methods, as well as discuss some potential commercial applications.</p>
</blockquote>
<ul>
<li>goal is to wow the audience with how general Proximal Policy Optimization (PPO) is.</li>
<li>OpenAI used PPO to learn DOTA and beat professionals, exact same algo can be used to learn/do totally something else.</li>
<li>context: where does RL fit?</li>
<li>general RL: Iâ€™m in situation X and my overall goal is y. What do I need to do?</li>
<li>RL is very general, fits with and takes bits of all parts of AI.</li>
<li>see <a href="http://www.argmin.net/">ben rechtâ€™s blog</a> on RL.</li>
<li>a way to think of any move choosing machine is as a tuneable black box which looks at the state of the world and spits out a probablity for all possible moves.</li>
<li>RL is adjusting this move choosing machine so that the likelihood of good moves is higher and bad moves lower. Thats it. Of course, you need to do this many times over.</li>
<li>one move choosing machines can be a NN, which are nice since they let us generalize inputs we havenâ€™t seen before by learning the underlying data distribution.</li>
<li>the simplest way of recording moves is just making a table of moves and a score, but this doesnâ€™t help us with moves we havenâ€™t seen. A naive answer is to pick the closest neighbours, or interpolate linearly. Neural networks allow us to approximate the underlying distribution, which allows us to reason about states we havenâ€™t seen before.</li>
<li>how do we train our NN to approximate a game globally when we are using the NN itself to collect the data we are using for training?
<ul>
<li>be extremely careful about how we explore the space</li>
<li>be extremely carefeul about how we update our NN</li>
</ul></li>
<li>Policy gradients gives us a safe way to update the NN, but have lots of issues</li>
<li>Proximal Policy Optimization uses a surrogate loss.</li>
<li>key diff: using the ratio of probabilites b/w old and new policies and limit the amount this can be updated. This bounds policy updates and introduces stability.</li>
<li>the engineering behind RL algos is impresisve and important - the algorithims can be straightforward but see all the engineering/computing resources OpenAI is throwing at their RL algos playing computer games like DOTA.</li>
</ul>
<p>q &amp; a</p>
<ul>
<li>problem of local minima?
<ul>
<li>in a multi-dimensional world, the local minima generally disappear, as there are so many directions to move. It does exist, but in a different way from SL.</li>
<li>there is a problem of getting stuck in a bad policy space so never learning â€˜goodâ€™ moves.</li>
</ul></li>
<li>what about overfitting?
<ul>
<li>we canâ€™t overfit - there is no test set. Weâ€™re doing RL in an environment where we want it to win the mostest.</li>
</ul></li>
<li>how many policies do u keep track of?
<ul>
<li>only two - current and one step ahead</li>
</ul></li>
<li>is there a q value in PPO?
<ul>
<li>in policy gradient methods you donâ€™t have a q</li>
</ul></li>
<li>catostrophic policy updates - how often do u have to abondon?
<ul>
<li>vanilla policy gradients, have to restart all the time, then tune hyperparameters.</li>
<li>PPO almost never, it generally figures things out.</li>
</ul></li>
<li>how efficient is RL?
<ul>
<li>RL uses huge amounts of compute but is progressing very fast, in orders of magnitude. At the moment its prohibitvely expensive for normal ppl to train things like DOTA bots but it keeps getting computationaly cheaper as algos improve.</li>
</ul></li>
</ul>


<!-- -->

</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "SML 2018-08: Deep Reinforcement Learning"</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "notes from the meetup"</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2018-08-20</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">- meetup</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">- data science</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu"># SML 2018-08: Deep Reinforcement Learning</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>SML's <span class="co">[</span><span class="ot">2018-08 meetup</span><span class="co">](https://www.meetup.com/Sydney-Machine-Learning/events/252760610/)</span>.</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>launching StarAI Deep Reinforcement Learning Course</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>7 week course to take you from RL near-zero to hero</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>10-15hrs study a week, meet 6-8pm once a week</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>the talks:</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu"># [Alasdair Hamilton](https://www.linkedin.com/in/alasdair-hamilton-11852a7b/): Reinforcement learning</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; The future of Artificial Intelligence is, at least partly, rooted in Reinforcement Learning. During our presentation, the team from Remi AI will take attendees on a journey through their experiences in Reinforcement Learning. Special attention will be paid to the areas in which the team have been able to apply Reinforcement Learning in a commercial setting and the four mammalian methods of learning, and how they inspire us. Remi AI has applied RL to budget and bid management, to dynamic pricing and web design, to large scale management of power requirements, inventory management, predictive maintenance. The talk will conclude with a discussion on future applications, as well as a roadmap for those looking to start out in the space, then Q&amp;A.</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>founder of <span class="co">[</span><span class="ot">Remi AI</span><span class="co">](https://www.linkedin.com/company/remi-pty-ltd/)</span>, one of Sydney's only RL companies.</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Remi AI was founded 5 yrs ago, works on applied AI for companies/biz/ngo's.</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL is a form of AI that includes motivating the agent toward some goal.</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>current RL agents are incredibly dumb - even though they excel at certain tasks.</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Why RL:</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>its been a very long time since humans invented tools - RL is the framework which enables an artificial agent to invent more tools to solve a problem</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>RL is the framework which humans use internally to learn</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>RL covers a lot of domains, from engineering to pyschology</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>RL is modelled around the dopamine reward delivering system in animals/humans</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>some real world RL applications: stunt flight, traffic optimisation, Go (see Deepmind), dynamic pricing</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the RL agent can change its environment with its own actions. there is no supervisor, ony a reward mechanism.</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>unlike supervised methods, feedbck is often delayed, so time is extremely important. e.g in a traffic optimsation problem traffic jams can arise a long time after a decision.</span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Demoed of a RL agent learning breakout, a remote control helicopter and space invadors.</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>reward hacking - designers can set wrong or low rewards and punishments - e.g if the punishment for crashing is to high, the best stratgey is to never take off.</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL in robotics is changing the field - what took decades to learn incrementally, RL systems are learning insanely quickly.</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>rewarding an agent: a reward is a scalar feedback signal - we dictate through rewards how well the agent is doing at step t. the best systems combine prediction and reward.</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>games are a very useful way to researh RL, as you have clear scores which can be tied to rewards and punishments.</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL is a lot harder in the real world in terms of choosing rewards and punishement.</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>imitating human reward systems can be both useful and incredibly dangerous.</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>there is quite a range of RL algos.</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL needs a huge amount of training time - agents initially know nothing so there is a lot of trial and error - ideally in simulation since doing this in the real world is very expensive</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a lot of RL is building a simulator to train an agent, then taking it to the real world.</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>todo: research transfer learning in RL.</span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>research happening in hierarchical learning - break up things so learn individual goals which build to the final goal.</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Some real world applications of RL:</span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>a few cities have implemented deep RL algos in traffic optimisation. RL is also useful for devising plans on how to handle breakdowns, like a road out of commision.</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>energy reduction - <span class="co">[</span><span class="ot">Google's data center</span><span class="co">](https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/)</span></span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>supply chain optimisation - warehousing, logistic.</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL models are simulation agnostic, but transition from simulation to the real world can be extremely troublesome. So:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>continually review simulators.</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>where are the reward predictions going wrong?</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a>q &amp; a</span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>How do you reward differnt/multiple objectives?</span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>for a wide range of objectives, scale up rewards at different levels - example reward individual contract signings and also bigger level revenue goals.</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>is RL limited by the complexity of the simulation?</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>some things can be simulated by very simple things like graphs written in python - sims don't have to be complex and visual.</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>arbitary rewards can cause problems - whats a good fix?</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>its a hot area - ppl are figuring out best utility functions for rewards.</span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a><span class="fu"># [Alex Long](https://www.linkedin.com/in/alex-long-rl/): Deep Reinforcement Learning: Zero to PPO in 20 minutes</span></span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; Deep Reinforcement Learning (Deep-RL) is the combination of traditional RL algorithms with the high-dimensional function approximation methods of deep learning. This combination allows Deep-RL to eclipse human performance on systems of previously intractable state-spaces and high branching factors, such as the game of GO, Atari arcade games, and heads up limit poker. In this talk I will focus on the intuition behind Deep-RL, how it compares (and differs) to other machine learning methods, as well as discuss some potential commercial applications.</span></span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>goal is to wow the audience with how general Proximal Policy Optimization (PPO) is.</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>OpenAI used PPO to learn DOTA and beat professionals, exact same algo can be used to learn/do totally something else.</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>context: where does RL fit?</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>general RL: I'm in situation X and my overall goal is y. What do I need to do?</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL is very general, fits with and takes bits of all parts of AI.</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>see <span class="co">[</span><span class="ot">ben recht's blog</span><span class="co">](http://www.argmin.net/)</span> on RL.</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a way to think of any move choosing machine is as a tuneable black box which looks at the state of the world and spits out a probablity for all possible moves.</span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>RL is adjusting this move choosing machine so that the likelihood of good moves is higher and bad moves lower. Thats it. Of course, you need to do this many times over.</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>one move choosing machines can be a NN, which are nice since they let us generalize inputs we haven't seen before by learning the underlying data distribution.</span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the simplest way of recording moves is just making a table of moves and a score, but this doesn't help us with moves we haven't seen. A naive answer is to pick the closest neighbours, or interpolate linearly. Neural networks allow us to approximate the underlying distribution, which allows us to reason about states we haven't seen before.</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how do we train our NN to approximate a game globally when we are using the NN itself to collect the data we are using for training?</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>be extremely careful about how we explore the space</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>be extremely carefeul about how we update our NN</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Policy gradients gives us a safe way to update the NN, but have lots of issues</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Proximal Policy Optimization uses a surrogate loss.</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>key diff: using the ratio of probabilites b/w old and new policies and limit the amount this can be updated. This bounds policy updates and introduces stability.</span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>the engineering behind RL algos is impresisve and important - the algorithims can be straightforward but see all the engineering/computing resources OpenAI is throwing at their RL algos playing computer games like DOTA.</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>q &amp; a</span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>problem of local minima?</span>
<span id="cb1-91"><a href="#cb1-91" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>in a multi-dimensional world, the local minima generally disappear, as there are so many directions to move. It does exist, but in a different way from SL.</span>
<span id="cb1-92"><a href="#cb1-92" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>there is a problem of getting stuck in a bad policy space so never learning 'good' moves.</span>
<span id="cb1-93"><a href="#cb1-93" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>what about overfitting?</span>
<span id="cb1-94"><a href="#cb1-94" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>we can't overfit - there is no test set. We're doing RL in an environment where we want it to win the mostest.</span>
<span id="cb1-95"><a href="#cb1-95" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how many policies do u keep track of?</span>
<span id="cb1-96"><a href="#cb1-96" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>only two - current and one step ahead</span>
<span id="cb1-97"><a href="#cb1-97" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>is there a q value in PPO?</span>
<span id="cb1-98"><a href="#cb1-98" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>in policy gradient methods you don't have a q</span>
<span id="cb1-99"><a href="#cb1-99" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>catostrophic policy updates - how often do u have to abondon?</span>
<span id="cb1-100"><a href="#cb1-100" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>vanilla policy gradients, have to restart all the time, then tune hyperparameters.</span>
<span id="cb1-101"><a href="#cb1-101" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>PPO almost never, it generally figures things out.</span>
<span id="cb1-102"><a href="#cb1-102" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>how efficient is RL?</span>
<span id="cb1-103"><a href="#cb1-103" aria-hidden="true" tabindex="-1"></a><span class="ss">  - </span>RL uses huge amounts of compute but is progressing very fast, in orders of magnitude. At the moment its prohibitvely expensive for normal ppl to train things like DOTA bots but it keeps getting computationaly cheaper as algos improve.</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>