<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="KO">
<meta name="dcterms.date" content="2017-08-15">
<meta name="description" content="notes from the course">

<title>khalido.org - Udacity ND101: Deep Learning Nanodegree</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">khalido.org</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/khalido" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/KO" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Udacity ND101: Deep Learning Nanodegree</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                  <div>
        <div class="description">
          notes from the course
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">courses</div>
                <div class="quarto-category">deep learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>KO </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">August 15, 2017</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#udacity-nd101-deep-learning-nanodegree" id="toc-udacity-nd101-deep-learning-nanodegree" class="nav-link active" data-scroll-target="#udacity-nd101-deep-learning-nanodegree">Udacity ND101: Deep Learning Nanodegree</a>
  <ul class="collapse">
  <li><a href="#before-the-course-starts" id="toc-before-the-course-starts" class="nav-link" data-scroll-target="#before-the-course-starts">Before the course starts:</a>
  <ul class="collapse">
  <li><a href="#python-resources" id="toc-python-resources" class="nav-link" data-scroll-target="#python-resources">Python resources</a></li>
  <li><a href="#math-resources" id="toc-math-resources" class="nav-link" data-scroll-target="#math-resources">Math resources</a></li>
  <li><a href="#books-to-read" id="toc-books-to-read" class="nav-link" data-scroll-target="#books-to-read">Books to read</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#week-1" id="toc-week-1" class="nav-link" data-scroll-target="#week-1">Week 1</a>
  <ul class="collapse">
  <li><a href="#fast-style-transfer" id="toc-fast-style-transfer" class="nav-link" data-scroll-target="#fast-style-transfer">fast-style transfer</a></li>
  <li><a href="#deep-traffic-simulator" id="toc-deep-traffic-simulator" class="nav-link" data-scroll-target="#deep-traffic-simulator">Deep Traffic simulator</a></li>
  <li><a href="#flappy-bird" id="toc-flappy-bird" class="nav-link" data-scroll-target="#flappy-bird">Flappy Bird</a></li>
  <li><a href="#sirajs-intro-to-linear-regression" id="toc-sirajs-intro-to-linear-regression" class="nav-link" data-scroll-target="#sirajs-intro-to-linear-regression">Sirajâ€™s intro to linear regression</a></li>
  <li><a href="#moving-on-to-neural-nets" id="toc-moving-on-to-neural-nets" class="nav-link" data-scroll-target="#moving-on-to-neural-nets">moving on to neural nets</a></li>
  <li><a href="#backpropagation" id="toc-backpropagation" class="nav-link" data-scroll-target="#backpropagation">Backpropagation</a></li>
  <li><a href="#project-1-predictin-bike-sharing-demand-from-historical-data" id="toc-project-1-predictin-bike-sharing-demand-from-historical-data" class="nav-link" data-scroll-target="#project-1-predictin-bike-sharing-demand-from-historical-data">Project 1: Predictin Bike Sharing demand from historical data</a></li>
  </ul></li>
  <li><a href="#week-2" id="toc-week-2" class="nav-link" data-scroll-target="#week-2">Week 2</a>
  <ul class="collapse">
  <li><a href="#model-evaluation-and-validation" id="toc-model-evaluation-and-validation" class="nav-link" data-scroll-target="#model-evaluation-and-validation">Model Evaluation and Validation</a>
  <ul class="collapse">
  <li><a href="#two-types-of-error" id="toc-two-types-of-error" class="nav-link" data-scroll-target="#two-types-of-error">Two types of error</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#week-3" id="toc-week-3" class="nav-link" data-scroll-target="#week-3">Week 3</a>
  <ul class="collapse">
  <li><a href="#sentiment-analysis-with-andrew-trask" id="toc-sentiment-analysis-with-andrew-trask" class="nav-link" data-scroll-target="#sentiment-analysis-with-andrew-trask">Sentiment Analysis with Andrew Trask</a></li>
  <li><a href="#intro-to-tflearn" id="toc-intro-to-tflearn" class="nav-link" data-scroll-target="#intro-to-tflearn">Intro to TFLearn</a></li>
  <li><a href="#rnn" id="toc-rnn" class="nav-link" data-scroll-target="#rnn">RNN</a></li>
  <li><a href="#using-tflearn-to-do-sentiment-analysis" id="toc-using-tflearn-to-do-sentiment-analysis" class="nav-link" data-scroll-target="#using-tflearn-to-do-sentiment-analysis">using tflearn to do sentiment Analysis</a></li>
  </ul></li>
  <li><a href="#week-4" id="toc-week-4" class="nav-link" data-scroll-target="#week-4">Week 4</a>
  <ul class="collapse">
  <li><a href="#sirajs-math-for-deep-learning" id="toc-sirajs-math-for-deep-learning" class="nav-link" data-scroll-target="#sirajs-math-for-deep-learning">Sirajâ€™s Math for Deep learning</a></li>
  <li><a href="#miniflow" id="toc-miniflow" class="nav-link" data-scroll-target="#miniflow">Miniflow</a></li>
  </ul></li>
  <li><a href="#week-5" id="toc-week-5" class="nav-link" data-scroll-target="#week-5">Week 5</a>
  <ul class="collapse">
  <li><a href="#intro-to-tensorflow" id="toc-intro-to-tensorflow" class="nav-link" data-scroll-target="#intro-to-tensorflow">Intro to Tensorflow</a></li>
  </ul></li>
  <li><a href="#week-6" id="toc-week-6" class="nav-link" data-scroll-target="#week-6">Week 6</a>
  <ul class="collapse">
  <li><a href="#preventing-overfitting" id="toc-preventing-overfitting" class="nav-link" data-scroll-target="#preventing-overfitting">Preventing overfitting</a></li>
  <li><a href="#convolution-networks-or-cnn" id="toc-convolution-networks-or-cnn" class="nav-link" data-scroll-target="#convolution-networks-or-cnn">Convolution Networks, or CNN</a></li>
  <li><a href="#sirajs-image-classification" id="toc-sirajs-image-classification" class="nav-link" data-scroll-target="#sirajs-image-classification">Sirajâ€™s Image Classification</a></li>
  <li><a href="#project-2" id="toc-project-2" class="nav-link" data-scroll-target="#project-2">Project 2</a></li>
  </ul></li>
  <li><a href="#week-7" id="toc-week-7" class="nav-link" data-scroll-target="#week-7">Week 7</a>
  <ul class="collapse">
  <li><a href="#intro-to-recurrent-neural-networks" id="toc-intro-to-recurrent-neural-networks" class="nav-link" data-scroll-target="#intro-to-recurrent-neural-networks">Intro to Recurrent Neural Networks</a></li>
  <li><a href="#aside" id="toc-aside" class="nav-link" data-scroll-target="#aside">aside</a></li>
  </ul></li>
  <li><a href="#week-8" id="toc-week-8" class="nav-link" data-scroll-target="#week-8">Week 8</a>
  <ul class="collapse">
  <li><a href="#embeddings-and-word2vec" id="toc-embeddings-and-word2vec" class="nav-link" data-scroll-target="#embeddings-and-word2vec">Embeddings and Word2vec</a></li>
  </ul></li>
  <li><a href="#week-9" id="toc-week-9" class="nav-link" data-scroll-target="#week-9">Week 9</a>
  <ul class="collapse">
  <li><a href="#tensorboard" id="toc-tensorboard" class="nav-link" data-scroll-target="#tensorboard">TensorBoard</a></li>
  </ul></li>
  <li><a href="#week-10" id="toc-week-10" class="nav-link" data-scroll-target="#week-10">Week 10</a>
  <ul class="collapse">
  <li><a href="#how-to-make-a-text-summarizer" id="toc-how-to-make-a-text-summarizer" class="nav-link" data-scroll-target="#how-to-make-a-text-summarizer">how to make a text summarizer</a></li>
  <li><a href="#weight-initialization" id="toc-weight-initialization" class="nav-link" data-scroll-target="#weight-initialization">Weight Initialization</a>
  <ul class="collapse">
  <li><a href="#weight-initilizaiton-resources" id="toc-weight-initilizaiton-resources" class="nav-link" data-scroll-target="#weight-initilizaiton-resources">weight initilizaiton resources</a></li>
  </ul></li>
  <li><a href="#sentiment-prediction-rnn" id="toc-sentiment-prediction-rnn" class="nav-link" data-scroll-target="#sentiment-prediction-rnn">Sentiment Prediction RNN</a></li>
  <li><a href="#project-3---generate-a-tv-script" id="toc-project-3---generate-a-tv-script" class="nav-link" data-scroll-target="#project-3---generate-a-tv-script">Project 3 - Generate a TV Script</a></li>
  </ul></li>
  <li><a href="#week-11" id="toc-week-11" class="nav-link" data-scroll-target="#week-11">Week 11</a>
  <ul class="collapse">
  <li><a href="#transfer-learning" id="toc-transfer-learning" class="nav-link" data-scroll-target="#transfer-learning">Transfer Learning</a></li>
  <li><a href="#language-translation" id="toc-language-translation" class="nav-link" data-scroll-target="#language-translation">Language Translation</a></li>
  </ul></li>
  <li><a href="#week-12" id="toc-week-12" class="nav-link" data-scroll-target="#week-12">Week 12</a>
  <ul class="collapse">
  <li><a href="#sequence-to-sequence" id="toc-sequence-to-sequence" class="nav-link" data-scroll-target="#sequence-to-sequence">Sequence to Sequence</a></li>
  <li><a href="#how-to-make-a-chatbot" id="toc-how-to-make-a-chatbot" class="nav-link" data-scroll-target="#how-to-make-a-chatbot">How to make a chatbot</a></li>
  <li><a href="#reinforcement-learning" id="toc-reinforcement-learning" class="nav-link" data-scroll-target="#reinforcement-learning">Reinforcement learning</a></li>
  <li><a href="#project-4---translation" id="toc-project-4---translation" class="nav-link" data-scroll-target="#project-4---translation">Project 4 - Translation</a></li>
  <li><a href="#gans" id="toc-gans" class="nav-link" data-scroll-target="#gans">GANS</a>
  <ul class="collapse">
  <li><a href="#batch-normalization" id="toc-batch-normalization" class="nav-link" data-scroll-target="#batch-normalization">Batch normalization</a></li>
  <li><a href="#face-generation-project" id="toc-face-generation-project" class="nav-link" data-scroll-target="#face-generation-project">Face generation project</a></li>
  </ul></li>
  <li><a href="#all-done" id="toc-all-done" class="nav-link" data-scroll-target="#all-done">All done!</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="udacity-nd101-deep-learning-nanodegree" class="level1">
<h1>Udacity ND101: Deep Learning Nanodegree</h1>
<p>My notes &amp; files for <a href="https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101">Udacityâ€™s Deep Learning intro course</a>.</p>
<section id="before-the-course-starts" class="level2">
<h2 class="anchored" data-anchor-id="before-the-course-starts">Before the course starts:</h2>
<ul>
<li>Read <a href="https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.t1zhql96j">Machine learning is fun</a></li>
<li>Watch <a href="https://youtu.be/LFDU2GX4AqM">Andrew NG: Deep Learning in Practice</a> (34 minutes)</li>
<li>Go though <a href="https://classroom.udacity.com/courses/ud730/">UD730 deep learning course</a> on Udacity</li>
<li>Watch <a href="https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd">Learn tensorflow 3 hour video</a></li>
<li>http://rodneybrooks.com/patrick-winston-explains-deep-learning/</li>
</ul>
<section id="python-resources" class="level3">
<h3 class="anchored" data-anchor-id="python-resources">Python resources</h3>
<p>The course recommneds knowing <a href="https://www.udacity.com/course/programming-foundations-with-python--ud036">basic python from here</a>, but I found the following two resources better:</p>
<ul>
<li><a href="https://github.com/jakevdp/WhirlwindTourOfPython">A whirlwind tour of Python</a></li>
<li><a href="https://github.com/jakevdp/PythonDataScienceHandbook">Python Data Science Handbook</a></li>
</ul>
</section>
<section id="math-resources" class="level3">
<h3 class="anchored" data-anchor-id="math-resources">Math resources</h3>
<p>Need to know multivariable calculus &amp; linear algebra.</p>
<ul>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus">Khan Academy Multivariable calculus</a></li>
<li><a href="https://www.youtube.com/playlist?list=PLlXfTHzgMRUKXD88IdzS14F4NxAZudSmv">Linear algebra youtube playlist</a> or over at <a href="https://www.lem.ma/web/#/books/VBS92YDYuscc5-lK/landing">lemma</a></li>
<li><a href="https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/jacobian-prerequisite-knowledge">Khan Academyâ€™s Linear Algebra Intro</a></li>
</ul>
</section>
<section id="books-to-read" class="level3">
<h3 class="anchored" data-anchor-id="books-to-read">Books to read</h3>
<ul>
<li><a href="https://www.manning.com/books/grokking-deep-learning">Grokking Deep Learning by Andrew Trask</a>. This provides a very gentle introduction to Deep Learning and covers the intuition more than the theory.</li>
<li><a href="http://neuralnetworksanddeeplearning.com/">Neural Networks And Deep Learning</a> by Michael Nielsen. This book is more rigorous than Grokking Deep Learning and includes a lot of fun, interactive visualizations to play with.</li>
<li><a href="http://www.deeplearningbook.org/">The Deep Learning Textbook</a> from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This online book has lot of material and is the most rigorous of the three books suggested.</li>
</ul>
</section>
</section>
</section>
<section id="week-1" class="level1">
<h1>Week 1</h1>
<p>The two instructors are [Mat Leonard] &amp; <a href="http://www.sirajraval.com/">Siraj Raval</a>.</p>
<p>Some of the stuff covered in the first week:</p>
<ul>
<li><a href="http://scikit-learn.org/">Scikit-learn</a> - An extremely popular Machine Learning library for python.</li>
<li><a href="https://en.wikipedia.org/wiki/Perceptron">Perceptrons</a> -The simplest form of a neural network.</li>
<li><a href="https://en.wikipedia.org/wiki/Gradient_descent">Gradient Descent</a> - A process by which Machine Learning algorithms learn to improve themselves based on the accuracy of their predictions. Youâ€™ll learn more about this in upcoming lessons.</li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Backpropagation</a> - The process by which neural networks learn how to improve individual parameters. Youâ€™ll learn all about this in the upcoming lessons.</li>
<li><a href="http://www.numpy.org/">Numpy</a> - An extremely popular library for scientific computing in python.</li>
<li><a href="http://tensorflow.org/">Tensorflow</a> - One of the most popular python libraries for creating neural networks. It is maintained by Google.</li>
</ul>
<section id="fast-style-transfer" class="level3">
<h3 class="anchored" data-anchor-id="fast-style-transfer">fast-style transfer</h3>
<p>Looking at<a href="https://github.com/lengstrom/fast-style-transfer">an existing style transfer deep learning script</a> to play around with. Hmm.. interesting to see what can be done but HOW is it done is the q?</p>
</section>
<section id="deep-traffic-simulator" class="level3">
<h3 class="anchored" data-anchor-id="deep-traffic-simulator"><a href="http://selfdrivingcars.mit.edu/deeptrafficjs/">Deep Traffic</a> simulator</h3>
<p>See the <a href="http://selfdrivingcars.mit.edu/deeptraffic/">overview</a> for how to tinker with the inputs to train the simple neural network. Interesting to see how inputs drastically effect the quality of the output.</p>
<blockquote class="blockquote">
<p><a href="http://selfdrivingcars.mit.edu/deeptrafficjs/">DeepTraffic</a> is a gamified simulation of typical highway traffic. Your task is to build a neural agent â€“ more specifically design and train a neural network that performs well on high traffic roads. Your neural network gets to control one of the cars (displayed in red) and has to learn how to navigate efficiently to go as fast as possible. The car already comes with a safety system, so you donâ€™t have to worry about the basic task of driving â€“ the net only has to tell the car if it should accelerate/slow down or change lanes, and it will do so if that is possible without crashing into other cars.</p>
</blockquote>
<p>This neural net uses <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a></p>
</section>
<section id="flappy-bird" class="level3">
<h3 class="anchored" data-anchor-id="flappy-bird"><a href="https://github.com/yenchenlin/DeepLearningFlappyBird">Flappy Bird</a></h3>
<p>This is a deep learning agent which plays flappy bird. Not very useful at this point. Need to know to understand/use/train the flappybird playing agent!</p>
<p>So come back to this later.</p>
</section>
<section id="sirajs-intro-to-linear-regression" class="level3">
<h3 class="anchored" data-anchor-id="sirajs-intro-to-linear-regression">Sirajâ€™s intro to linear regression</h3>
<p>he walks through linear regression, using numpy and then by building a gradient descent model.</p>
</section>
<section id="moving-on-to-neural-nets" class="level3">
<h3 class="anchored" data-anchor-id="moving-on-to-neural-nets">moving on to neural nets</h3>
<ul>
<li>a neural network is an algorithm which identifies patterns in data</li>
<li>Backpropagation trains a neural net by updating weights via gradient descent</li>
<li>deep learning = many layer neural net + big data + big compute</li>
</ul>
</section>
<section id="backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation">Backpropagation</h2>
<p>This is the key to understanding neural nets, so itâ€™s important to understand how Backpropagation works.</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=59Hbtz7XgjM">CS231n Winter 2016 Lecture 4: Backpropagation</a></li>
</ul>
</section>
<section id="project-1-predictin-bike-sharing-demand-from-historical-data" class="level2">
<h2 class="anchored" data-anchor-id="project-1-predictin-bike-sharing-demand-from-historical-data">Project 1: Predictin Bike Sharing demand from historical data</h2>
<p><a href="https://github.com/khalido/nd101-projects/blob/playing-with-numpy/dlnd-your-first-neural-network.ipynb">Final Notebook</a></p>
</section>
</section>
<section id="week-2" class="level1">
<h1>Week 2</h1>
<section id="model-evaluation-and-validation" class="level2">
<h2 class="anchored" data-anchor-id="model-evaluation-and-validation">Model Evaluation and Validation</h2>
<p>Generalization is better than overfitting.</p>
<p>R2 score - simplest possible model is to take the avg of all values and draw a straight line, then calculate the mean square error - the R2 score is 1 minus the error of our regression model divided by the error of the simplest possible model - if we have a good model, the error will be small compared to the simple model, thus R2 will be close to 1 - for a bad model, the ratio of errors will be closer to 1, giving a small R2 values</p>
<p>from sklearn.metrics import r2_score</p>
<section id="two-types-of-error" class="level3">
<h3 class="anchored" data-anchor-id="two-types-of-error">Two types of error</h3>
<ul>
<li>overfitting</li>
<li>underfitting</li>
</ul>
</section>
</section>
</section>
<section id="week-3" class="level1">
<h1>Week 3</h1>
<section id="sentiment-analysis-with-andrew-trask" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-analysis-with-andrew-trask">Sentiment Analysis with Andrew Trask</h2>
<p>This was a good project - built a simple neural network to classify reviews as negative or positive.</p>
</section>
<section id="intro-to-tflearn" class="level2">
<h2 class="anchored" data-anchor-id="intro-to-tflearn">Intro to TFLearn</h2>
<p>Sigmoid activation functions have a max derivate of .25, so errors shrink by 75% or more during backprogation. This means the neural network takes a long time to train. Instead of sigmoid, most DL networks use RLU - which is a supersimple function which outputs max(input, 0). For a +ve inut the output equals the input, and for a -ve input the output is 0. Relu nodes can die if there is a large graident through them, so they are best used with a small learning rate.</p>
<p>For a simple binary classification, the sigmoid function works, but for mulitple outputs, for example reconigizing digits, use the <a href="https://en.wikipedia.org/wiki/Softmax_function">softmax</a> function. A softmax function squashes outputs to be b/w 0 and 1 and divides them such that the total sum of the output equals 1.</p>
<p>one hot encoding means using a simple vector, like <code>y=[0,0,0,0,1,0,0,0,0,0]</code> to represent 4.</p>
<p>http://tflearn.org/</p>
</section>
<section id="rnn" class="level2">
<h2 class="anchored" data-anchor-id="rnn">RNN</h2>
<p>http://colah.github.io/posts/2015-08-Understanding-LSTMs/ http://karpathy.github.io/2015/05/21/rnn-effectiveness/</p>
</section>
<section id="using-tflearn-to-do-sentiment-analysis" class="level2">
<h2 class="anchored" data-anchor-id="using-tflearn-to-do-sentiment-analysis">using tflearn to do sentiment Analysis</h2>
</section>
</section>
<section id="week-4" class="level1">
<h1>Week 4</h1>
<section id="sirajs-math-for-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="sirajs-math-for-deep-learning">Sirajâ€™s Math for Deep learning</h2>
<p>Need to know statistics, linear algebra and calculus. C</p>
</section>
<section id="miniflow" class="level2">
<h2 class="anchored" data-anchor-id="miniflow">Miniflow</h2>
<p>built a simple graph based nn</p>
<p>an alternative to Amazonâ€™s EC2 gpu machines: <a href="https://www.floydhub.com/">floyd</a></p>
</section>
</section>
<section id="week-5" class="level1">
<h1>Week 5</h1>
<section id="intro-to-tensorflow" class="level2">
<h2 class="anchored" data-anchor-id="intro-to-tensorflow">Intro to <a href="https://www.tensorflow.org/">Tensorflow</a></h2>
<p>Deep learning is a family of techniques which adapts to all sorts of data and problems. the basic techiniques of DL apply to a bunch of diff fields. Neural Networks have been around for decades but had pretty much disappread from the CS science. They came back in a bigway in the 2010â€™s with advances in speech reconizition, computer vision and machine translation. This was enabled by lots of data and cheap gpus.</p>
<p>All the hotness is in my <a href="https://github.com/khalido/nd101/blob/master/intro_to_tensorflow.ipynb">intro to tensorflow notebook</a>.</p>
</section>
</section>
<section id="week-6" class="level1">
<h1>Week 6</h1>
<p>Going deeper into tensorflow!</p>
<section id="preventing-overfitting" class="level3">
<h3 class="anchored" data-anchor-id="preventing-overfitting">Preventing overfitting</h3>
<p><strong>Early Termination</strong>: stop training soon as you stop improving</p>
<p><strong>Regulariation</strong> applies constrains - L2 regularization adds another term to the loss which penalizes large weights.</p>
<p><strong>Dropout</strong> is an important technique - it randomly stops half the signals flowing throgh a layer, and multiplies by 2 the remaining signals. This forces the NN to make redundant representations for everything - so with only partial info it can predict the right answer. During testing, you cancel the dropout to maximize the predictive power of the model.</p>
<p><strong>get rid of unnecessary info</strong> For example, when reconigizing letters, the colors donâ€™t matter, so transform R,G,B values into grayscale by (R+G+B)/2.</p>
<p><strong>weight sharing</strong> say you have two kittens in the same image. so it makes sense to train the same part of the network on each kitten. we do this be weight sharing.</p>
<p><strong>statistical invariants</strong> things which donâ€™t change across time and space, like say the word kitten in a text, it always refers to kittens.</p>
</section>
<section id="convolution-networks-or-cnn" class="level2">
<h2 class="anchored" data-anchor-id="convolution-networks-or-cnn">Convolution Networks, or CNN</h2>
<p>A CNN breaks up an image into many pieces and learns to first reconigzie basic shapes, lines, curves, then the more complex objects as combinations of the simpler shapes, then classifies the image by combining the complex objects together. A CNN can have many layers, with each layer capturing a different level of complexity.</p>
<p>Resources:</p>
<ul>
<li>It seems the time is now to read <a href="http://neuralnetworksanddeeplearning.com/">this book on neural networks</a> and go through <a href="http://cs231n.github.io/">cs231</a>.</li>
<li>This <a href="https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.yop1x41dy">article</a> is a nice simple overview of neural networks and builds a simple covnet network using keras.</li>
<li><a href="https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/">A beginers guide to understanding n=cnovlutional networks</a></li>
</ul>
</section>
<section id="sirajs-image-classification" class="level2">
<h2 class="anchored" data-anchor-id="sirajs-image-classification">Sirajâ€™s Image Classification</h2>
</section>
<section id="project-2" class="level2">
<h2 class="anchored" data-anchor-id="project-2">Project 2</h2>
<p>Goal is to Classify images from the <a href="https://www.cs.toronto.edu/~kriz/cifar.html">CIFAR dataset</a>.</p>
<p>A good time to watch this <a href="https://www.youtube.com/watch?v=vq2nnJ4g6N0">video intro to Tensorflow</a>.</p>
<p>My <a href="https://github.com/khalido/nd101-projects/blob/master/dlnd_image_classification.ipynb">final project notebook</a>.</p>
<p>Note: I only got 65% accuracy, but that was at 20 epochs. Running it at a hundred or so shuold bump up the accuracy over 70%, but I got tired of waiting for the model to train.</p>
</section>
</section>
<section id="week-7" class="level1">
<h1>Week 7</h1>
<section id="intro-to-recurrent-neural-networks" class="level2">
<h2 class="anchored" data-anchor-id="intro-to-recurrent-neural-networks">Intro to Recurrent Neural Networks</h2>
<p>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</p>
</section>
<section id="aside" class="level2">
<h2 class="anchored" data-anchor-id="aside">aside</h2>
<p><a href="https://www.youtube.com/watch?v=8UQzJaa0HPU&amp;feature=youtu.be">Deep Learning chat</a> https://deeplearning4j.org/word2vec</p>
</section>
</section>
<section id="week-8" class="level1">
<h1>Week 8</h1>
<section id="embeddings-and-word2vec" class="level2">
<h2 class="anchored" data-anchor-id="embeddings-and-word2vec">Embeddings and Word2vec</h2>
<p><a href="https://en.wikipedia.org/wiki/Word2vec">Word2Vec</a></p>
<p>Notebook: https://github.com/udacity/deep-learning/tree/master/embeddings</p>
</section>
</section>
<section id="week-9" class="level1">
<h1>Week 9</h1>
<section id="tensorboard" class="level2">
<h2 class="anchored" data-anchor-id="tensorboard">TensorBoard</h2>
<p>This is really useful to see the initial model, and then to see what is happening while itâ€™s training.</p>
<ul>
<li><a href="https://youtu.be/eBbEDRsCmv4">tensorboard intro</a></li>
<li><a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">basic tensorboard intro</a></li>
</ul>
<p>Can also be used for <a href="https://github.com/udacity/deep-learning/blob/master/tensorboard/Anna_KaRNNa_Hyperparameters.ipynb">hyperparameter search</a> by selecting diff combinations of parameters, writing them to a logstring and viewing the diff runs in tensorboard all nicely charted out.</p>
<p>todo: make a simple mnist NN with tensorboard summaries of different parameters. something like:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lstm_size <span class="kw">in</span> [<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>]:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num_layers <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> learning_rate <span class="kw">in</span> [<span class="fl">0.002</span>, <span class="fl">0.001</span>]:</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>            log_string <span class="op">=</span> <span class="st">'logs/4/lr=</span><span class="sc">{}</span><span class="st">,rl=</span><span class="sc">{}</span><span class="st">,ru=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(learning_rate, num_layers, lstm_size)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>            writer <span class="op">=</span> tf.summary.FileWriter(log_string)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="week-10" class="level1">
<h1>Week 10</h1>
<p>RNNâ€™s and language generation</p>
<section id="how-to-make-a-text-summarizer" class="level2">
<h2 class="anchored" data-anchor-id="how-to-make-a-text-summarizer">how to make a text summarizer</h2>
<p>â€¦..</p>
</section>
<section id="weight-initialization" class="level2">
<h2 class="anchored" data-anchor-id="weight-initialization">Weight Initialization</h2>
<p>The initial value of weights is very important to how well a NN trains. If all the weights start of the same, it makes it hard to update the weights since they end up all giving similar outputs, making it hard for the NN to learn.</p>
<p>Random weights work much better. Tensorflowâ€™s <a href="https://www.tensorflow.org/api_docs/python/tf/random_uniform">tf.random_uniform()</a> is a good weight initilization function. Be default, tf.random.uniform picks float values uniformly spread b/w 0 and 1, which is decent, but we can do better.</p>
<p>Smart people have tested out different initial weights, and it seems using weights distributed around zero with a std dev around 0.1 works well, with tails cut off. Tensorflow has a built in function:</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/truncated_normal">tf.truncated_normal</a> which generates a random normal distribution around zero with values &gt; 2 std devs from the mean repicked. So making 3 weights would look someething like:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>A normal distribution means that the weights will tend to be closer to the mean rather than uniformly distributed.</p>
<p>todo: run a simple network on MNIST with different weights and see in tensorboard todo: find some approchable resources on weights</p>
<section id="weight-initilizaiton-resources" class="level3">
<h3 class="anchored" data-anchor-id="weight-initilizaiton-resources">weight initilizaiton resources</h3>
<ul>
<li><a href="http://cs231n.github.io/neural-networks-2/#init">cs231n weight initilization</a> - supports the above method of small random weights centered on zero, but warns that smaller numbers arenâ€™t always better.</li>
</ul>
</section>
</section>
<section id="sentiment-prediction-rnn" class="level2">
<h2 class="anchored" data-anchor-id="sentiment-prediction-rnn">Sentiment Prediction RNN</h2>
</section>
<section id="project-3---generate-a-tv-script" class="level2">
<h2 class="anchored" data-anchor-id="project-3---generate-a-tv-script">Project 3 - Generate a TV Script</h2>
<p>This was an interesting project - to generate new text after training a RNN network on a subset of Simpsons scripts, set in Moeâ€™s Cavern.</p>
<p><a href="https://github.com/khalido/nd101-projects/blob/master/dlnd_tv_script_generation.ipynb">Completed project 3 notebook</a></p>
</section>
</section>
<section id="week-11" class="level1">
<h1>Week 11</h1>
<section id="transfer-learning" class="level2">
<h2 class="anchored" data-anchor-id="transfer-learning">Transfer Learning</h2>
<p>Training nerual networks can take a long time so we can take an exisiting pretrained network and use that to extract features or as the initial network to further build upon. Here, we use <a href="https://arxiv.org/pdf/1409.1556.pdf">VGGNet</a>.</p>
<p>Iâ€™m using a <a href="https://github.com/machrisaa/tensorflow-vgg">pretrained VGG network from here</a>.</p>
</section>
<section id="language-translation" class="level2">
<h2 class="anchored" data-anchor-id="language-translation">Language Translation</h2>
</section>
</section>
<section id="week-12" class="level1">
<h1>Week 12</h1>
<section id="sequence-to-sequence" class="level2">
<h2 class="anchored" data-anchor-id="sequence-to-sequence">Sequence to Sequence</h2>
<p>is a kind of RNN network.</p>
<p><a href="https://www.tensorflow.org/api_docs/python/tf/contrib/seq2seq">tf.contrib.seq2seq</a></p>
<p>Useful posts - <a href="https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b">Text Summarization with Amazon Reviews</a> - <a href="https://www.youtube.com/watch?v=G5RY_SUJih4">Seq2Seq talk</a> - <a href="https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6">Stanford lectures on NLP using deep learning</a></p>
</section>
<section id="how-to-make-a-chatbot" class="level2">
<h2 class="anchored" data-anchor-id="how-to-make-a-chatbot">How to make a chatbot</h2>
<p>Train a DNN to learn text and answers. Facebook has some <a href="https://research.fb.com/downloads/babi/">datasets to learn on</a>.</p>
<p>Using Keras itâ€™s relative straightforward. See <a href="https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py">this implementation</a>.</p>
</section>
<section id="reinforcement-learning" class="level2">
<h2 class="anchored" data-anchor-id="reinforcement-learning">Reinforcement learning</h2>
<p>Use rewards when things be done right. Here we look at Q-Learning. Using negative rewards for bad stuff, and postiive rewards for good stuff, you train an agent to seek out the max reward path, using a q-table which stores rewards for each state and action possible from that state.</p>
<p><a href="https://gym.openai.com">OpenAI Gym</a> has lots of implementations.</p>
<p><a href="https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947">basic RL</a></p>
</section>
<section id="project-4---translation" class="level2">
<h2 class="anchored" data-anchor-id="project-4---translation">Project 4 - Translation</h2>
<p>use a rnn to translate english to french.</p>
<p>Project is built on TF 1.0, so see <a href="https://github.com/tensorflow/tensorflow/blob/master/RELEASE.md">release notes</a> for later versions as TF changed some RNN bits.</p>
<p>I had a problem with training times. Even on a GPU it took forever. This is with relatively limited data, with a decently sized data it will take a looong time!</p>
<p>Questions to ponder:</p>
<ul>
<li>should the encoding &amp; decoding embed size be exactly the vocab size?</li>
</ul>
<p><strong>Parameters which should give &gt; 90% accuracy:</strong></p>
<ul>
<li>epochs = 3</li>
<li>batch_size = 256</li>
<li>rnn_size = 128</li>
<li>num_layers = 2</li>
<li>encoding_embedding_size = 256</li>
<li>decoding_embedding_size = 256</li>
<li>learning_rate = 0.005</li>
<li>keep_probability = 0.9</li>
</ul>
</section>
<section id="gans" class="level2">
<h2 class="anchored" data-anchor-id="gans">GANS</h2>
<p>Gans use a differentiable function represented by a NN to transform an output from an input (generally random noise?).</p>
<p>A second network, called the discriminator, which is just a regular NN classifier which has been trained on real images, gives the generated image a probablity of being real or fake. The discriminator receives a mix of real images and generated images.</p>
<p>Over time, the generator gets good at generating images which passes the discrimatorâ€™s test.</p>
<p>Game theory tells us that the two NNâ€™s should come to a equilibrium where neither NN can improve their situation.</p>
<p>We train both networks alternately - look up best practices.</p>
<ul>
<li><a href="https://medium.com/towards-data-science/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0">GAN by Example using Keras</a></li>
<li><a href="https://medium.com/intuitionmachine/deep-adversarial-learning-is-finally-ready-and-will-radically-change-the-game-f0cfda7b91d3">another blog post</a></li>
<li><a href="https://arxiv.org/pdf/1406.2661.pdf">Original GAN paper</a></li>
<li><a href="https://blog.openai.com/generative-models/">OpenAI GAN explainer</a></li>
<li><a href="https://www.quora.com/topic/Generative-Adversarial-Networks-1">Quora GANs topic</a></li>
</ul>
<section id="batch-normalization" class="level3">
<h3 class="anchored" data-anchor-id="batch-normalization">Batch normalization</h3>
<p>This greatly aids deep networks to learn faster, as well so stops them flaking out and killing too many neurons.</p>
</section>
<section id="face-generation-project" class="level3">
<h3 class="anchored" data-anchor-id="face-generation-project">Face generation project</h3>
<p>A GAN which generates mnist images and faces. Relatively straigtforward, but adding in <a href="https://github.com/udacity/deep-learning/blob/master/batch-norm/Batch_Normalization_Lesson.ipynb">batch normalization</a> involves some tensorflow kungfu. Doing this project in TF made me yearn for Keras.</p>
<p>Specifically, the <a href="https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization">batch_norm layer in TF</a> isnâ€™t magical enough, so we have to wrap the train operations inside tf.control_dependencies block so the batch normalization layers get updated. The below is the code from <a href="https://github.com/udacity/deep-learning/blob/master/dcgan-svhn/DCGAN.ipynb">Udacty</a> but since I added batch norm to both the generator and the discriminator this didnâ€™t work for me:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>        d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1<span class="op">=</span>beta1).minimize(d_loss, var_list<span class="op">=</span>d_vars)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1<span class="op">=</span>beta1).minimize(g_loss, var_list<span class="op">=</span>g_vars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>After much googling and help from the forums, this worked:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># need this to make batch normalization work properly during inference</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    update_ops <span class="op">=</span> tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    g_updates  <span class="op">=</span> [opt <span class="cf">for</span> opt <span class="kw">in</span> update_ops <span class="cf">if</span> opt.name.startswith(<span class="st">'generator'</span>)]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.control_dependencies(g_updates):</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1).minimize(d_loss, var_list<span class="op">=</span>d_vars)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1).minimize(g_loss, var_list<span class="op">=</span>g_vars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>Putting this here becuase there must be better/simpler easier way to do batch_norm without getting into the graph bowels of tensorflow.</p>
<p>todo: reimplement in Keras</p>
<p><a href="https://github.com/khalido/nd101-projects/blob/master/dlnd_face_generation.ipynb">Project notebok</a></p>
</section>
</section>
<section id="all-done" class="level2">
<h2 class="anchored" data-anchor-id="all-done">All done!</h2>
<p>Link to the official <a href="https://confirm.udacity.com/FMMYQCGE">course certificate</a>.</p>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb5" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Udacity ND101: Deep Learning Nanodegree"</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> /images/udacity-nd101-cert.png</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> 2017-08-15</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="an">description:</span><span class="co"> "notes from the course"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="an">categories:</span><span class="co"> [courses, deep learning]</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="an">layout:</span><span class="co"> post</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="an">toc:</span><span class="co"> true</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="fu"># Udacity ND101: Deep Learning Nanodegree</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>My notes &amp; files for <span class="co">[</span><span class="ot">Udacity's Deep Learning intro course</span><span class="co">](https://www.udacity.com/course/deep-learning-nanodegree-foundation--nd101)</span>.</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="fu">## Before the course starts:</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Read <span class="co">[</span><span class="ot">Machine learning is fun</span><span class="co">](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471#.t1zhql96j)</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Watch <span class="co">[</span><span class="ot">Andrew NG: Deep Learning in Practice</span><span class="co">](https://youtu.be/LFDU2GX4AqM)</span> (34 minutes)</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Go though <span class="co">[</span><span class="ot">UD730 deep learning course</span><span class="co">](https://classroom.udacity.com/courses/ud730/)</span> on Udacity</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Watch <span class="co">[</span><span class="ot">Learn tensorflow 3 hour video</span><span class="co">](https://cloud.google.com/blog/big-data/2017/01/learn-tensorflow-and-deep-learning-without-a-phd)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>http://rodneybrooks.com/patrick-winston-explains-deep-learning/</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a><span class="fu">### Python resources</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>The course recommneds knowing <span class="co">[</span><span class="ot">basic python from here</span><span class="co">](https://www.udacity.com/course/programming-foundations-with-python--ud036)</span>, but I found the following two resources better:</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">A whirlwind tour of Python</span><span class="co">](https://github.com/jakevdp/WhirlwindTourOfPython)</span></span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Python Data Science Handbook</span><span class="co">](https://github.com/jakevdp/PythonDataScienceHandbook)</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a><span class="fu">### Math resources</span></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>Need to know multivariable calculus &amp; linear algebra.</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Khan Academy Multivariable calculus</span><span class="co">](https://www.khanacademy.org/math/multivariable-calculus)</span></span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Linear algebra youtube playlist</span><span class="co">](https://www.youtube.com/playlist?list=PLlXfTHzgMRUKXD88IdzS14F4NxAZudSmv)</span> or over at <span class="co">[</span><span class="ot">lemma</span><span class="co">](https://www.lem.ma/web/#/books/VBS92YDYuscc5-lK/landing)</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Khan Academy's Linear Algebra Intro</span><span class="co">](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/jacobian/v/jacobian-prerequisite-knowledge)</span></span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="fu">### Books to read</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Grokking Deep Learning by Andrew Trask</span><span class="co">](https://www.manning.com/books/grokking-deep-learning)</span>. This provides a very gentle introduction to Deep Learning and covers the intuition more than the theory.</span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Neural Networks And Deep Learning</span><span class="co">](http://neuralnetworksanddeeplearning.com/)</span> by Michael Nielsen. This book is more rigorous than Grokking Deep Learning and includes a lot of fun, interactive visualizations to play with.</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">The Deep Learning Textbook</span><span class="co">](http://www.deeplearningbook.org/)</span> from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This online book has lot of material and is the most rigorous of the three books suggested.</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 1</span></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>The two instructors are <span class="co">[</span><span class="ot">Mat Leonard</span><span class="co">]</span> &amp; <span class="co">[</span><span class="ot">Siraj Raval</span><span class="co">](http://www.sirajraval.com/)</span>.</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a>Some of the stuff covered in the first week:</span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Scikit-learn</span><span class="co">](http://scikit-learn.org/)</span> - An extremely popular Machine Learning library for python.</span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Perceptrons</span><span class="co">](https://en.wikipedia.org/wiki/Perceptron)</span> -The simplest form of a neural network.</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Gradient Descent</span><span class="co">](https://en.wikipedia.org/wiki/Gradient_descent)</span> - A process by which Machine Learning algorithms learn to improve themselves based on the accuracy of their predictions. You'll learn more about this in upcoming lessons.</span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Backpropagation</span><span class="co">](http://neuralnetworksanddeeplearning.com/chap2.html)</span> - The process by which neural networks learn how to improve individual parameters. You'll learn all about this in the upcoming lessons.</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Numpy</span><span class="co">](http://www.numpy.org/)</span> - An extremely popular library for scientific computing in python.</span>
<span id="cb5-55"><a href="#cb5-55" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">Tensorflow</span><span class="co">](http://tensorflow.org/)</span> - One of the most popular python libraries for creating neural networks. It is maintained by Google.</span>
<span id="cb5-56"><a href="#cb5-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-57"><a href="#cb5-57" aria-hidden="true" tabindex="-1"></a><span class="fu">### fast-style transfer</span></span>
<span id="cb5-58"><a href="#cb5-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-59"><a href="#cb5-59" aria-hidden="true" tabindex="-1"></a>Looking at<span class="co">[</span><span class="ot">an existing style transfer deep learning script</span><span class="co">](https://github.com/lengstrom/fast-style-transfer)</span> to play around with. Hmm.. interesting to see what can be done but HOW is it done is the q?</span>
<span id="cb5-60"><a href="#cb5-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-61"><a href="#cb5-61" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Deep Traffic](http://selfdrivingcars.mit.edu/deeptrafficjs/) simulator</span></span>
<span id="cb5-62"><a href="#cb5-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-63"><a href="#cb5-63" aria-hidden="true" tabindex="-1"></a>See the <span class="co">[</span><span class="ot">overview</span><span class="co">](http://selfdrivingcars.mit.edu/deeptraffic/)</span> for how to tinker with the inputs to train the simple neural network. Interesting to see how inputs drastically effect the quality of the output.</span>
<span id="cb5-64"><a href="#cb5-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-65"><a href="#cb5-65" aria-hidden="true" tabindex="-1"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">DeepTraffic</span><span class="co">](http://selfdrivingcars.mit.edu/deeptrafficjs/)</span><span class="at"> is a gamified simulation of typical highway traffic. Your task is to build a neural agent â€“ more specifically design and train a neural network that performs well on high traffic roads. Your neural network gets to control one of the cars (displayed in red) and has to learn how to navigate efficiently to go as fast as possible. The car already comes with a safety system, so you donâ€™t have to worry about the basic task of driving â€“ the net only has to tell the car if it should accelerate/slow down or change lanes, and it will do so if that is possible without crashing into other cars.</span></span>
<span id="cb5-66"><a href="#cb5-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-67"><a href="#cb5-67" aria-hidden="true" tabindex="-1"></a>This neural net uses <span class="co">[</span><span class="ot">reinforcement learning</span><span class="co">](https://en.wikipedia.org/wiki/Reinforcement_learning)</span></span>
<span id="cb5-68"><a href="#cb5-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-69"><a href="#cb5-69" aria-hidden="true" tabindex="-1"></a><span class="fu">### [Flappy Bird](https://github.com/yenchenlin/DeepLearningFlappyBird)</span></span>
<span id="cb5-70"><a href="#cb5-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-71"><a href="#cb5-71" aria-hidden="true" tabindex="-1"></a>This is a deep learning agent which plays flappy bird. Not very useful at this point. Need to know to understand/use/train the flappybird playing agent!</span>
<span id="cb5-72"><a href="#cb5-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-73"><a href="#cb5-73" aria-hidden="true" tabindex="-1"></a>So come back to this later.</span>
<span id="cb5-74"><a href="#cb5-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-75"><a href="#cb5-75" aria-hidden="true" tabindex="-1"></a><span class="fu">### Siraj's intro to linear regression</span></span>
<span id="cb5-76"><a href="#cb5-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-77"><a href="#cb5-77" aria-hidden="true" tabindex="-1"></a>he walks through linear regression, using numpy and then by building a gradient descent model.</span>
<span id="cb5-78"><a href="#cb5-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-79"><a href="#cb5-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-80"><a href="#cb5-80" aria-hidden="true" tabindex="-1"></a><span class="fu">### moving on to neural nets</span></span>
<span id="cb5-81"><a href="#cb5-81" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-82"><a href="#cb5-82" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>a neural network is an algorithm which identifies patterns in data</span>
<span id="cb5-83"><a href="#cb5-83" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>Backpropagation trains a neural net by updating weights via gradient descent</span>
<span id="cb5-84"><a href="#cb5-84" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>deep learning = many layer neural net + big data + big compute</span>
<span id="cb5-85"><a href="#cb5-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-86"><a href="#cb5-86" aria-hidden="true" tabindex="-1"></a><span class="fu">## Backpropagation</span></span>
<span id="cb5-87"><a href="#cb5-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-88"><a href="#cb5-88" aria-hidden="true" tabindex="-1"></a>This is the key to understanding neural nets, so it's important to understand how Backpropagation works.</span>
<span id="cb5-89"><a href="#cb5-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-90"><a href="#cb5-90" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">CS231n Winter 2016 Lecture 4: Backpropagation</span><span class="co">](https://www.youtube.com/watch?v=59Hbtz7XgjM)</span></span>
<span id="cb5-91"><a href="#cb5-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-92"><a href="#cb5-92" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-93"><a href="#cb5-93" aria-hidden="true" tabindex="-1"></a><span class="fu">## Project 1: Predictin Bike Sharing demand from historical data</span></span>
<span id="cb5-94"><a href="#cb5-94" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-95"><a href="#cb5-95" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Final Notebook</span><span class="co">](https://github.com/khalido/nd101-projects/blob/playing-with-numpy/dlnd-your-first-neural-network.ipynb)</span></span>
<span id="cb5-96"><a href="#cb5-96" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-97"><a href="#cb5-97" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 2</span></span>
<span id="cb5-98"><a href="#cb5-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-99"><a href="#cb5-99" aria-hidden="true" tabindex="-1"></a><span class="fu">## Model Evaluation and Validation</span></span>
<span id="cb5-100"><a href="#cb5-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-101"><a href="#cb5-101" aria-hidden="true" tabindex="-1"></a>Generalization is better than overfitting.</span>
<span id="cb5-102"><a href="#cb5-102" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-103"><a href="#cb5-103" aria-hidden="true" tabindex="-1"></a>R2 score</span>
<span id="cb5-104"><a href="#cb5-104" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>simplest possible model is to take the avg of all values and draw a straight line, then calculate the mean square error</span>
<span id="cb5-105"><a href="#cb5-105" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>the R2 score is 1 minus the error of our regression model divided by the error of the simplest possible model</span>
<span id="cb5-106"><a href="#cb5-106" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>if we have a good model, the error will be small compared to the simple model, thus R2 will be close to 1</span>
<span id="cb5-107"><a href="#cb5-107" aria-hidden="true" tabindex="-1"></a><span class="ss"> - </span>for a bad model, the ratio of errors will be closer to 1, giving a small R2 values</span>
<span id="cb5-108"><a href="#cb5-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-109"><a href="#cb5-109" aria-hidden="true" tabindex="-1"></a> from sklearn.metrics import r2_score</span>
<span id="cb5-110"><a href="#cb5-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-111"><a href="#cb5-111" aria-hidden="true" tabindex="-1"></a><span class="fu">### Two types of error</span></span>
<span id="cb5-112"><a href="#cb5-112" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-113"><a href="#cb5-113" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>overfitting</span>
<span id="cb5-114"><a href="#cb5-114" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>underfitting</span>
<span id="cb5-115"><a href="#cb5-115" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-116"><a href="#cb5-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-117"><a href="#cb5-117" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 3</span></span>
<span id="cb5-118"><a href="#cb5-118" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-119"><a href="#cb5-119" aria-hidden="true" tabindex="-1"></a><span class="fu">## Sentiment Analysis with Andrew Trask</span></span>
<span id="cb5-120"><a href="#cb5-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-121"><a href="#cb5-121" aria-hidden="true" tabindex="-1"></a>This was a good project - built a simple neural network to classify reviews as negative or positive.</span>
<span id="cb5-122"><a href="#cb5-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-123"><a href="#cb5-123" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intro to TFLearn</span></span>
<span id="cb5-124"><a href="#cb5-124" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-125"><a href="#cb5-125" aria-hidden="true" tabindex="-1"></a>Sigmoid activation functions have a max derivate of .25, so errors shrink by 75% or more during backprogation. This means the neural network takes a long time to train. Instead of sigmoid, most DL networks use RLU - which is a supersimple function which outputs max(input, 0). For a +ve inut the output equals the input, and for a -ve input the output is 0. Relu nodes can die if there is a large graident through them, so they are best used with a small learning rate.</span>
<span id="cb5-126"><a href="#cb5-126" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-127"><a href="#cb5-127" aria-hidden="true" tabindex="-1"></a>For a simple binary classification, the sigmoid function works, but for mulitple outputs, for example reconigizing digits, use the <span class="co">[</span><span class="ot">softmax</span><span class="co">](https://en.wikipedia.org/wiki/Softmax_function)</span> function. A softmax function squashes outputs to be b/w 0 and 1 and divides them such that the total sum of the output equals 1.</span>
<span id="cb5-128"><a href="#cb5-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-129"><a href="#cb5-129" aria-hidden="true" tabindex="-1"></a>one hot encoding means using a simple vector, like <span class="in">`y=[0,0,0,0,1,0,0,0,0,0]`</span> to represent 4.</span>
<span id="cb5-130"><a href="#cb5-130" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-131"><a href="#cb5-131" aria-hidden="true" tabindex="-1"></a>http://tflearn.org/</span>
<span id="cb5-132"><a href="#cb5-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-133"><a href="#cb5-133" aria-hidden="true" tabindex="-1"></a><span class="fu">## RNN</span></span>
<span id="cb5-134"><a href="#cb5-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-135"><a href="#cb5-135" aria-hidden="true" tabindex="-1"></a>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</span>
<span id="cb5-136"><a href="#cb5-136" aria-hidden="true" tabindex="-1"></a>http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span>
<span id="cb5-137"><a href="#cb5-137" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-138"><a href="#cb5-138" aria-hidden="true" tabindex="-1"></a><span class="fu">## using tflearn to do sentiment Analysis</span></span>
<span id="cb5-139"><a href="#cb5-139" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-140"><a href="#cb5-140" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-141"><a href="#cb5-141" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 4</span></span>
<span id="cb5-142"><a href="#cb5-142" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-143"><a href="#cb5-143" aria-hidden="true" tabindex="-1"></a><span class="fu">## Siraj's Math for Deep learning</span></span>
<span id="cb5-144"><a href="#cb5-144" aria-hidden="true" tabindex="-1"></a>Need to know statistics, linear algebra and calculus.</span>
<span id="cb5-145"><a href="#cb5-145" aria-hidden="true" tabindex="-1"></a>C</span>
<span id="cb5-146"><a href="#cb5-146" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-147"><a href="#cb5-147" aria-hidden="true" tabindex="-1"></a><span class="fu">## Miniflow</span></span>
<span id="cb5-148"><a href="#cb5-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-149"><a href="#cb5-149" aria-hidden="true" tabindex="-1"></a>built a simple graph based nn</span>
<span id="cb5-150"><a href="#cb5-150" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-151"><a href="#cb5-151" aria-hidden="true" tabindex="-1"></a>an alternative to Amazon's EC2 gpu machines: <span class="co">[</span><span class="ot">floyd</span><span class="co">](https://www.floydhub.com/)</span></span>
<span id="cb5-152"><a href="#cb5-152" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-153"><a href="#cb5-153" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 5</span></span>
<span id="cb5-154"><a href="#cb5-154" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-155"><a href="#cb5-155" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intro to [Tensorflow](https://www.tensorflow.org/)</span></span>
<span id="cb5-156"><a href="#cb5-156" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-157"><a href="#cb5-157" aria-hidden="true" tabindex="-1"></a>Deep learning is a family of techniques which adapts to all sorts of data and problems. the basic techiniques of DL apply to a bunch of diff fields. Neural Networks have been around for decades but had pretty much disappread from the CS science. They came back in a bigway in the 2010's with advances in speech reconizition, computer vision and machine translation. This was enabled by lots of data and cheap gpus.</span>
<span id="cb5-158"><a href="#cb5-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-159"><a href="#cb5-159" aria-hidden="true" tabindex="-1"></a>All the hotness is in my <span class="co">[</span><span class="ot">intro to tensorflow notebook</span><span class="co">](https://github.com/khalido/nd101/blob/master/intro_to_tensorflow.ipynb)</span>.</span>
<span id="cb5-160"><a href="#cb5-160" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-161"><a href="#cb5-161" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 6</span></span>
<span id="cb5-162"><a href="#cb5-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-163"><a href="#cb5-163" aria-hidden="true" tabindex="-1"></a>Going deeper into tensorflow!</span>
<span id="cb5-164"><a href="#cb5-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-165"><a href="#cb5-165" aria-hidden="true" tabindex="-1"></a><span class="fu">### Preventing overfitting</span></span>
<span id="cb5-166"><a href="#cb5-166" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-167"><a href="#cb5-167" aria-hidden="true" tabindex="-1"></a>**Early Termination**: stop training soon as you stop improving</span>
<span id="cb5-168"><a href="#cb5-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-169"><a href="#cb5-169" aria-hidden="true" tabindex="-1"></a>**Regulariation** applies constrains - L2 regularization adds another term to the loss which penalizes large weights.</span>
<span id="cb5-170"><a href="#cb5-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-171"><a href="#cb5-171" aria-hidden="true" tabindex="-1"></a>**Dropout** is an important technique - it randomly stops half the signals flowing throgh a layer, and multiplies by 2 the remaining signals. This forces the NN to make redundant representations for everything - so with only partial info it can predict the right answer.</span>
<span id="cb5-172"><a href="#cb5-172" aria-hidden="true" tabindex="-1"></a>During testing, you cancel the dropout to maximize the predictive power of the model.</span>
<span id="cb5-173"><a href="#cb5-173" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-174"><a href="#cb5-174" aria-hidden="true" tabindex="-1"></a>**get rid of unnecessary info** For example, when reconigizing letters, the colors don't matter, so transform R,G,B values into grayscale by (R+G+B)/2.</span>
<span id="cb5-175"><a href="#cb5-175" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-176"><a href="#cb5-176" aria-hidden="true" tabindex="-1"></a>**weight sharing** say you have two kittens in the same image. so it makes sense to train the same part of the network on each kitten. we do this be weight sharing.</span>
<span id="cb5-177"><a href="#cb5-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-178"><a href="#cb5-178" aria-hidden="true" tabindex="-1"></a>**statistical invariants** things which don't change across time and space, like say the word kitten in a text, it always refers to kittens.</span>
<span id="cb5-179"><a href="#cb5-179" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-180"><a href="#cb5-180" aria-hidden="true" tabindex="-1"></a><span class="fu">## Convolution Networks, or CNN</span></span>
<span id="cb5-181"><a href="#cb5-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-182"><a href="#cb5-182" aria-hidden="true" tabindex="-1"></a>A CNN breaks up an image into many pieces and learns to first reconigzie basic shapes, lines, curves, then the more complex objects as combinations of the simpler shapes, then classifies the image by combining the complex objects together. A CNN can have many layers, with each layer capturing a different level of complexity.</span>
<span id="cb5-183"><a href="#cb5-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-184"><a href="#cb5-184" aria-hidden="true" tabindex="-1"></a>Resources:</span>
<span id="cb5-185"><a href="#cb5-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-186"><a href="#cb5-186" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>It seems the time is now to read <span class="co">[</span><span class="ot">this book on neural networks</span><span class="co">](http://neuralnetworksanddeeplearning.com/)</span> and go through <span class="co">[</span><span class="ot">cs231</span><span class="co">](http://cs231n.github.io/)</span>.</span>
<span id="cb5-187"><a href="#cb5-187" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span>This <span class="co">[</span><span class="ot">article</span><span class="co">](https://hackernoon.com/learning-ai-if-you-suck-at-math-p5-deep-learning-and-convolutional-neural-nets-in-plain-english-cda79679bbe3#.yop1x41dy)</span> is a nice simple overview of neural networks and builds a simple covnet network using keras.</span>
<span id="cb5-188"><a href="#cb5-188" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">A beginers guide to understanding n=cnovlutional networks</span><span class="co">](https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/)</span></span>
<span id="cb5-189"><a href="#cb5-189" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-190"><a href="#cb5-190" aria-hidden="true" tabindex="-1"></a><span class="fu">## Siraj's Image Classification</span></span>
<span id="cb5-191"><a href="#cb5-191" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-192"><a href="#cb5-192" aria-hidden="true" tabindex="-1"></a><span class="fu">## Project 2</span></span>
<span id="cb5-193"><a href="#cb5-193" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-194"><a href="#cb5-194" aria-hidden="true" tabindex="-1"></a>Goal is to Classify images from the <span class="co">[</span><span class="ot">CIFAR dataset</span><span class="co">](https://www.cs.toronto.edu/~kriz/cifar.html)</span>.</span>
<span id="cb5-195"><a href="#cb5-195" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-196"><a href="#cb5-196" aria-hidden="true" tabindex="-1"></a>A good time to watch this <span class="co">[</span><span class="ot">video intro to Tensorflow</span><span class="co">](https://www.youtube.com/watch?v=vq2nnJ4g6N0)</span>.</span>
<span id="cb5-197"><a href="#cb5-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-198"><a href="#cb5-198" aria-hidden="true" tabindex="-1"></a>My <span class="co">[</span><span class="ot">final project notebook</span><span class="co">](https://github.com/khalido/nd101-projects/blob/master/dlnd_image_classification.ipynb)</span>.</span>
<span id="cb5-199"><a href="#cb5-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-200"><a href="#cb5-200" aria-hidden="true" tabindex="-1"></a>Note: I only got 65% accuracy, but that was at 20 epochs. Running it at a hundred or so shuold bump up the accuracy over 70%, but I got tired of waiting for the model to train.</span>
<span id="cb5-201"><a href="#cb5-201" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-202"><a href="#cb5-202" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 7</span></span>
<span id="cb5-203"><a href="#cb5-203" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-204"><a href="#cb5-204" aria-hidden="true" tabindex="-1"></a><span class="fu">## Intro to Recurrent Neural Networks</span></span>
<span id="cb5-205"><a href="#cb5-205" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-206"><a href="#cb5-206" aria-hidden="true" tabindex="-1"></a>http://colah.github.io/posts/2015-08-Understanding-LSTMs/</span>
<span id="cb5-207"><a href="#cb5-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-208"><a href="#cb5-208" aria-hidden="true" tabindex="-1"></a><span class="fu">## aside</span></span>
<span id="cb5-209"><a href="#cb5-209" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-210"><a href="#cb5-210" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Deep Learning chat</span><span class="co">](https://www.youtube.com/watch?v=8UQzJaa0HPU&amp;feature=youtu.be)</span></span>
<span id="cb5-211"><a href="#cb5-211" aria-hidden="true" tabindex="-1"></a>https://deeplearning4j.org/word2vec</span>
<span id="cb5-212"><a href="#cb5-212" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-213"><a href="#cb5-213" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 8</span></span>
<span id="cb5-214"><a href="#cb5-214" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-215"><a href="#cb5-215" aria-hidden="true" tabindex="-1"></a><span class="fu">## Embeddings and Word2vec</span></span>
<span id="cb5-216"><a href="#cb5-216" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-217"><a href="#cb5-217" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">Word2Vec</span><span class="co">](https://en.wikipedia.org/wiki/Word2vec)</span></span>
<span id="cb5-218"><a href="#cb5-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-219"><a href="#cb5-219" aria-hidden="true" tabindex="-1"></a>Notebook: https://github.com/udacity/deep-learning/tree/master/embeddings</span>
<span id="cb5-220"><a href="#cb5-220" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-221"><a href="#cb5-221" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 9</span></span>
<span id="cb5-222"><a href="#cb5-222" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-223"><a href="#cb5-223" aria-hidden="true" tabindex="-1"></a><span class="fu">## TensorBoard</span></span>
<span id="cb5-224"><a href="#cb5-224" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-225"><a href="#cb5-225" aria-hidden="true" tabindex="-1"></a>This is really useful to see the initial model, and then to see what is happening while it's training.</span>
<span id="cb5-226"><a href="#cb5-226" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-227"><a href="#cb5-227" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">tensorboard intro</span><span class="co">](https://youtu.be/eBbEDRsCmv4)</span></span>
<span id="cb5-228"><a href="#cb5-228" aria-hidden="true" tabindex="-1"></a><span class="ss">- </span><span class="co">[</span><span class="ot">basic tensorboard intro</span><span class="co">](https://www.tensorflow.org/get_started/summaries_and_tensorboard)</span></span>
<span id="cb5-229"><a href="#cb5-229" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-230"><a href="#cb5-230" aria-hidden="true" tabindex="-1"></a>Can also be used for <span class="co">[</span><span class="ot">hyperparameter search</span><span class="co">](https://github.com/udacity/deep-learning/blob/master/tensorboard/Anna_KaRNNa_Hyperparameters.ipynb)</span> by selecting diff combinations of parameters, writing them to a logstring and viewing the diff runs in tensorboard all nicely charted out.</span>
<span id="cb5-231"><a href="#cb5-231" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-232"><a href="#cb5-232" aria-hidden="true" tabindex="-1"></a>todo: make a simple mnist NN with tensorboard summaries of different parameters. something like:</span>
<span id="cb5-233"><a href="#cb5-233" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-234"><a href="#cb5-234" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-235"><a href="#cb5-235" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> lstm_size <span class="kw">in</span> [<span class="dv">128</span>,<span class="dv">256</span>,<span class="dv">512</span>]:</span>
<span id="cb5-236"><a href="#cb5-236" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num_layers <span class="kw">in</span> [<span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb5-237"><a href="#cb5-237" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> learning_rate <span class="kw">in</span> [<span class="fl">0.002</span>, <span class="fl">0.001</span>]:</span>
<span id="cb5-238"><a href="#cb5-238" aria-hidden="true" tabindex="-1"></a>            log_string <span class="op">=</span> <span class="st">'logs/4/lr=</span><span class="sc">{}</span><span class="st">,rl=</span><span class="sc">{}</span><span class="st">,ru=</span><span class="sc">{}</span><span class="st">'</span>.<span class="bu">format</span>(learning_rate, num_layers, lstm_size)</span>
<span id="cb5-239"><a href="#cb5-239" aria-hidden="true" tabindex="-1"></a>            writer <span class="op">=</span> tf.summary.FileWriter(log_string)</span>
<span id="cb5-240"><a href="#cb5-240" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb5-241"><a href="#cb5-241" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-242"><a href="#cb5-242" aria-hidden="true" tabindex="-1"></a><span class="fu"># Week 10</span></span>
<span id="cb5-243"><a href="#cb5-243" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-244"><a href="#cb5-244" aria-hidden="true" tabindex="-1"></a>RNN's and language generation</span>
<span id="cb5-245"><a href="#cb5-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-246"><a href="#cb5-246" aria-hidden="true" tabindex="-1"></a><span class="fu">## how to make a text summarizer</span></span>
<span id="cb5-247"><a href="#cb5-247" aria-hidden="true" tabindex="-1"></a>.....</span>
<span id="cb5-248"><a href="#cb5-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-249"><a href="#cb5-249" aria-hidden="true" tabindex="-1"></a><span class="fu">## Weight Initialization</span></span>
<span id="cb5-250"><a href="#cb5-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-251"><a href="#cb5-251" aria-hidden="true" tabindex="-1"></a>The initial value of weights is very important to how well a NN trains. If all the weights start of the same, it makes it hard to update the weights since they end up all giving similar outputs, making it hard for the NN to learn.</span>
<span id="cb5-252"><a href="#cb5-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-253"><a href="#cb5-253" aria-hidden="true" tabindex="-1"></a>Random weights work much better. Tensorflow's <span class="co">[</span><span class="ot">tf.random_uniform()</span><span class="co">](https://www.tensorflow.org/api_docs/python/tf/random_uniform)</span> is a good weight initilization function. Be default, tf.random.uniform picks float values uniformly spread b/w 0 and 1, which is decent, but we can do better.</span>
<span id="cb5-254"><a href="#cb5-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-255"><a href="#cb5-255" aria-hidden="true" tabindex="-1"></a>Smart people have tested out different initial weights, and it seems using weights distributed around zero with a std dev around 0.1 works well, with tails cut off. Tensorflow has a built in function:</span>
<span id="cb5-256"><a href="#cb5-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-257"><a href="#cb5-257" aria-hidden="true" tabindex="-1"></a><span class="co">[</span><span class="ot">tf.truncated_normal</span><span class="co">](https://www.tensorflow.org/api_docs/python/tf/truncated_normal)</span> which generates a random normal distribution around zero with values &gt; 2 std devs from the mean repicked. So making 3 weights would look someething like:</span>
<span id="cb5-258"><a href="#cb5-258" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-259"><a href="#cb5-259" aria-hidden="true" tabindex="-1"></a><span class="in">```python</span></span>
<span id="cb5-260"><a href="#cb5-260" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> [</span>
<span id="cb5-261"><a href="#cb5-261" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb5-262"><a href="#cb5-262" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>)),</span>
<span id="cb5-263"><a href="#cb5-263" aria-hidden="true" tabindex="-1"></a>    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev<span class="op">=</span><span class="fl">0.1</span>))</span>
<span id="cb5-264"><a href="#cb5-264" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-265"><a href="#cb5-265" aria-hidden="true" tabindex="-1"></a>A normal distribution means that the weights will tend to be closer to the mean rather than uniformly distributed.</span>
<span id="cb5-266"><a href="#cb5-266" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-267"><a href="#cb5-267" aria-hidden="true" tabindex="-1"></a>todo: run a simple network on MNIST <span class="cf">with</span> different weights <span class="kw">and</span> see <span class="kw">in</span> tensorboard</span>
<span id="cb5-268"><a href="#cb5-268" aria-hidden="true" tabindex="-1"></a>todo: find some approchable resources on weights</span>
<span id="cb5-269"><a href="#cb5-269" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-270"><a href="#cb5-270" aria-hidden="true" tabindex="-1"></a><span class="co">### weight initilizaiton resources</span></span>
<span id="cb5-271"><a href="#cb5-271" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-272"><a href="#cb5-272" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [cs231n weight initilization](http:<span class="op">//</span>cs231n.github.io<span class="op">/</span>neural<span class="op">-</span>networks<span class="op">-</span><span class="dv">2</span><span class="op">/</span><span class="co">#init) - supports the above method of small random weights centered on zero, but warns that smaller numbers aren't always better.</span></span>
<span id="cb5-273"><a href="#cb5-273" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-274"><a href="#cb5-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-275"><a href="#cb5-275" aria-hidden="true" tabindex="-1"></a><span class="co">## Sentiment Prediction RNN</span></span>
<span id="cb5-276"><a href="#cb5-276" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-277"><a href="#cb5-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-278"><a href="#cb5-278" aria-hidden="true" tabindex="-1"></a><span class="co">## Project 3 - Generate a TV Script</span></span>
<span id="cb5-279"><a href="#cb5-279" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-280"><a href="#cb5-280" aria-hidden="true" tabindex="-1"></a>This was an interesting project <span class="op">-</span> to generate new text after training a RNN network on a subset of Simpsons scripts, <span class="bu">set</span> <span class="kw">in</span> Moe<span class="st">'s Cavern.</span></span>
<span id="cb5-281"><a href="#cb5-281" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-282"><a href="#cb5-282" aria-hidden="true" tabindex="-1"></a><span class="er">[Completed project 3 notebook](https://github.com/khalido/nd101-projects/blob/master/dlnd_tv_script_generation.ipynb)</span></span>
<span id="cb5-283"><a href="#cb5-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-284"><a href="#cb5-284" aria-hidden="true" tabindex="-1"></a><span class="er"># Week 11</span></span>
<span id="cb5-285"><a href="#cb5-285" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-286"><a href="#cb5-286" aria-hidden="true" tabindex="-1"></a><span class="co">## Transfer Learning</span></span>
<span id="cb5-287"><a href="#cb5-287" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-288"><a href="#cb5-288" aria-hidden="true" tabindex="-1"></a>Training nerual networks can take a <span class="bu">long</span> time so we can take an exisiting pretrained network <span class="kw">and</span> use that to extract features <span class="kw">or</span> <span class="im">as</span> the initial network to further build upon. Here, we use [VGGNet](https:<span class="op">//</span>arxiv.org<span class="op">/</span>pdf<span class="op">/</span><span class="fl">1409.1556</span>.pdf).</span>
<span id="cb5-289"><a href="#cb5-289" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-290"><a href="#cb5-290" aria-hidden="true" tabindex="-1"></a>I<span class="st">'m using a [pretrained VGG network from here](https://github.com/machrisaa/tensorflow-vgg).</span></span>
<span id="cb5-291"><a href="#cb5-291" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-292"><a href="#cb5-292" aria-hidden="true" tabindex="-1"></a><span class="er">## Language Translation</span></span>
<span id="cb5-293"><a href="#cb5-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-294"><a href="#cb5-294" aria-hidden="true" tabindex="-1"></a><span class="co"># Week 12</span></span>
<span id="cb5-295"><a href="#cb5-295" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-296"><a href="#cb5-296" aria-hidden="true" tabindex="-1"></a><span class="co">## Sequence to Sequence</span></span>
<span id="cb5-297"><a href="#cb5-297" aria-hidden="true" tabindex="-1"></a><span class="kw">is</span> a kind of RNN network.</span>
<span id="cb5-298"><a href="#cb5-298" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-299"><a href="#cb5-299" aria-hidden="true" tabindex="-1"></a>[tf.contrib.seq2seq](https:<span class="op">//</span>www.tensorflow.org<span class="op">/</span>api_docs<span class="op">/</span>python<span class="op">/</span>tf<span class="op">/</span>contrib<span class="op">/</span>seq2seq)</span>
<span id="cb5-300"><a href="#cb5-300" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-301"><a href="#cb5-301" aria-hidden="true" tabindex="-1"></a>Useful posts</span>
<span id="cb5-302"><a href="#cb5-302" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [Text Summarization <span class="cf">with</span> Amazon Reviews](https:<span class="op">//</span>medium.com<span class="op">/</span>towards<span class="op">-</span>data<span class="op">-</span>science<span class="op">/</span>text<span class="op">-</span>summarization<span class="op">-</span><span class="cf">with</span><span class="op">-</span>amazon<span class="op">-</span>reviews<span class="op">-</span><span class="dv">41801</span><span class="er">c2210b</span>)</span>
<span id="cb5-303"><a href="#cb5-303" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [Seq2Seq talk](https:<span class="op">//</span>www.youtube.com<span class="op">/</span>watch?v<span class="op">=</span>G5RY_SUJih4)</span>
<span id="cb5-304"><a href="#cb5-304" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [Stanford lectures on NLP using deep learning](https:<span class="op">//</span>www.youtube.com<span class="op">/</span>playlist?<span class="bu">list</span><span class="op">=</span>PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6)</span>
<span id="cb5-305"><a href="#cb5-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-306"><a href="#cb5-306" aria-hidden="true" tabindex="-1"></a><span class="co">## How to make a chatbot</span></span>
<span id="cb5-307"><a href="#cb5-307" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-308"><a href="#cb5-308" aria-hidden="true" tabindex="-1"></a>Train a DNN to learn text <span class="kw">and</span> answers. Facebook has some [datasets to learn on](https:<span class="op">//</span>research.fb.com<span class="op">/</span>downloads<span class="op">/</span>babi<span class="op">/</span>).</span>
<span id="cb5-309"><a href="#cb5-309" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-310"><a href="#cb5-310" aria-hidden="true" tabindex="-1"></a>Using Keras it<span class="st">'s relative straightforward. See [this implementation](https://github.com/fchollet/keras/blob/master/examples/babi_rnn.py).</span></span>
<span id="cb5-311"><a href="#cb5-311" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-312"><a href="#cb5-312" aria-hidden="true" tabindex="-1"></a><span class="er">## Reinforcement learning</span></span>
<span id="cb5-313"><a href="#cb5-313" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-314"><a href="#cb5-314" aria-hidden="true" tabindex="-1"></a>Use rewards when things be done right. Here we look at Q<span class="op">-</span>Learning.</span>
<span id="cb5-315"><a href="#cb5-315" aria-hidden="true" tabindex="-1"></a>Using negative rewards <span class="cf">for</span> bad stuff, <span class="kw">and</span> postiive rewards <span class="cf">for</span> good stuff, you train an agent to seek out the <span class="bu">max</span> reward path, using a q<span class="op">-</span>table which stores rewards <span class="cf">for</span> each state <span class="kw">and</span> action possible <span class="im">from</span> that state.</span>
<span id="cb5-316"><a href="#cb5-316" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-317"><a href="#cb5-317" aria-hidden="true" tabindex="-1"></a>[OpenAI Gym](https:<span class="op">//</span>gym.openai.com) has lots of implementations.</span>
<span id="cb5-318"><a href="#cb5-318" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-319"><a href="#cb5-319" aria-hidden="true" tabindex="-1"></a>[basic RL](https:<span class="op">//</span>medium.com<span class="op">/@</span>tuzzer<span class="op">/</span>cart<span class="op">-</span>pole<span class="op">-</span>balancing<span class="op">-</span><span class="cf">with</span><span class="op">-</span>q<span class="op">-</span>learning<span class="op">-</span>b54c6068d947)</span>
<span id="cb5-320"><a href="#cb5-320" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-321"><a href="#cb5-321" aria-hidden="true" tabindex="-1"></a><span class="co">## Project 4 - Translation</span></span>
<span id="cb5-322"><a href="#cb5-322" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-323"><a href="#cb5-323" aria-hidden="true" tabindex="-1"></a>use a rnn to translate english to french.</span>
<span id="cb5-324"><a href="#cb5-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-325"><a href="#cb5-325" aria-hidden="true" tabindex="-1"></a>Project <span class="kw">is</span> built on TF <span class="fl">1.0</span>, so see [release notes](https:<span class="op">//</span>github.com<span class="op">/</span>tensorflow<span class="op">/</span>tensorflow<span class="op">/</span>blob<span class="op">/</span>master<span class="op">/</span>RELEASE.md) <span class="cf">for</span> later versions <span class="im">as</span> TF changed some RNN bits.</span>
<span id="cb5-326"><a href="#cb5-326" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-327"><a href="#cb5-327" aria-hidden="true" tabindex="-1"></a>I had a problem <span class="cf">with</span> training times. Even on a GPU it took forever. This <span class="kw">is</span> <span class="cf">with</span> relatively limited data, <span class="cf">with</span> a decently sized data it will take a looong time<span class="op">!</span></span>
<span id="cb5-328"><a href="#cb5-328" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-329"><a href="#cb5-329" aria-hidden="true" tabindex="-1"></a>Questions to ponder:</span>
<span id="cb5-330"><a href="#cb5-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-331"><a href="#cb5-331" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> should the encoding <span class="op">&amp;</span> decoding embed size be exactly the vocab size?</span>
<span id="cb5-332"><a href="#cb5-332" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-333"><a href="#cb5-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-334"><a href="#cb5-334" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-335"><a href="#cb5-335" aria-hidden="true" tabindex="-1"></a><span class="op">**</span>Parameters which should give <span class="op">&gt;</span> <span class="dv">90</span><span class="op">%</span> accuracy:<span class="op">**</span></span>
<span id="cb5-336"><a href="#cb5-336" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-337"><a href="#cb5-337" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> epochs <span class="op">=</span> <span class="dv">3</span></span>
<span id="cb5-338"><a href="#cb5-338" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> batch_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb5-339"><a href="#cb5-339" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> rnn_size <span class="op">=</span> <span class="dv">128</span></span>
<span id="cb5-340"><a href="#cb5-340" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> num_layers <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb5-341"><a href="#cb5-341" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> encoding_embedding_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb5-342"><a href="#cb5-342" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> decoding_embedding_size <span class="op">=</span> <span class="dv">256</span></span>
<span id="cb5-343"><a href="#cb5-343" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> learning_rate <span class="op">=</span> <span class="fl">0.005</span></span>
<span id="cb5-344"><a href="#cb5-344" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> keep_probability <span class="op">=</span> <span class="fl">0.9</span></span>
<span id="cb5-345"><a href="#cb5-345" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-346"><a href="#cb5-346" aria-hidden="true" tabindex="-1"></a><span class="co">## GANS</span></span>
<span id="cb5-347"><a href="#cb5-347" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-348"><a href="#cb5-348" aria-hidden="true" tabindex="-1"></a>Gans use a differentiable function represented by a NN to transform an output <span class="im">from</span> an <span class="bu">input</span> (generally random noise?).</span>
<span id="cb5-349"><a href="#cb5-349" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-350"><a href="#cb5-350" aria-hidden="true" tabindex="-1"></a>A second network, called the discriminator, which <span class="kw">is</span> just a regular NN classifier which has been trained on real images, gives the generated image a probablity of being real <span class="kw">or</span> fake. The discriminator receives a mix of real images <span class="kw">and</span> generated images.</span>
<span id="cb5-351"><a href="#cb5-351" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-352"><a href="#cb5-352" aria-hidden="true" tabindex="-1"></a>Over time, the generator gets good at generating images which passes the discrimator<span class="st">'s test.</span></span>
<span id="cb5-353"><a href="#cb5-353" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-354"><a href="#cb5-354" aria-hidden="true" tabindex="-1"></a><span class="er">Game theory tells us that the two NN's should come to a equilibrium where neither NN can improve their situation.</span></span>
<span id="cb5-355"><a href="#cb5-355" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-356"><a href="#cb5-356" aria-hidden="true" tabindex="-1"></a>We train both networks alternately <span class="op">-</span> look up best practices.</span>
<span id="cb5-357"><a href="#cb5-357" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb5-358"><a href="#cb5-358" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-359"><a href="#cb5-359" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [GAN by Example using Keras](https:<span class="op">//</span>medium.com<span class="op">/</span>towards<span class="op">-</span>data<span class="op">-</span>science<span class="op">/</span>gan<span class="op">-</span>by<span class="op">-</span>example<span class="op">-</span>using<span class="op">-</span>keras<span class="op">-</span>on<span class="op">-</span>tensorflow<span class="op">-</span>backend<span class="op">-</span><span class="dv">1</span><span class="er">a6d515a60d0</span>)</span>
<span id="cb5-360"><a href="#cb5-360" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [another blog post](https:<span class="op">//</span>medium.com<span class="op">/</span>intuitionmachine<span class="op">/</span>deep<span class="op">-</span>adversarial<span class="op">-</span>learning<span class="op">-</span><span class="kw">is</span><span class="op">-</span><span class="cf">finally</span><span class="op">-</span>ready<span class="op">-</span><span class="kw">and</span><span class="op">-</span>will<span class="op">-</span>radically<span class="op">-</span>change<span class="op">-</span>the<span class="op">-</span>game<span class="op">-</span>f0cfda7b91d3)</span>
<span id="cb5-361"><a href="#cb5-361" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [Original GAN paper](https:<span class="op">//</span>arxiv.org<span class="op">/</span>pdf<span class="op">/</span><span class="fl">1406.2661</span>.pdf)</span>
<span id="cb5-362"><a href="#cb5-362" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [OpenAI GAN explainer](https:<span class="op">//</span>blog.openai.com<span class="op">/</span>generative<span class="op">-</span>models<span class="op">/</span>)</span>
<span id="cb5-363"><a href="#cb5-363" aria-hidden="true" tabindex="-1"></a><span class="op">-</span> [Quora GANs topic](https:<span class="op">//</span>www.quora.com<span class="op">/</span>topic<span class="op">/</span>Generative<span class="op">-</span>Adversarial<span class="op">-</span>Networks<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb5-364"><a href="#cb5-364" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-365"><a href="#cb5-365" aria-hidden="true" tabindex="-1"></a><span class="co">### Batch normalization</span></span>
<span id="cb5-366"><a href="#cb5-366" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-367"><a href="#cb5-367" aria-hidden="true" tabindex="-1"></a>This greatly aids deep networks to learn faster, <span class="im">as</span> well so stops them flaking out <span class="kw">and</span> killing too many neurons.</span>
<span id="cb5-368"><a href="#cb5-368" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-369"><a href="#cb5-369" aria-hidden="true" tabindex="-1"></a><span class="co">### Face generation project</span></span>
<span id="cb5-370"><a href="#cb5-370" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-371"><a href="#cb5-371" aria-hidden="true" tabindex="-1"></a>A GAN which generates mnist images <span class="kw">and</span> faces.</span>
<span id="cb5-372"><a href="#cb5-372" aria-hidden="true" tabindex="-1"></a>Relatively straigtforward, but adding <span class="kw">in</span> [batch normalization](https:<span class="op">//</span>github.com<span class="op">/</span>udacity<span class="op">/</span>deep<span class="op">-</span>learning<span class="op">/</span>blob<span class="op">/</span>master<span class="op">/</span>batch<span class="op">-</span>norm<span class="op">/</span>Batch_Normalization_Lesson.ipynb) involves some tensorflow kungfu. Doing this project <span class="kw">in</span> TF made me yearn <span class="cf">for</span> Keras.</span>
<span id="cb5-373"><a href="#cb5-373" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-374"><a href="#cb5-374" aria-hidden="true" tabindex="-1"></a>Specifically, the [batch_norm layer <span class="kw">in</span> TF](https:<span class="op">//</span>www.tensorflow.org<span class="op">/</span>api_docs<span class="op">/</span>python<span class="op">/</span>tf<span class="op">/</span>layers<span class="op">/</span>batch_normalization) isn<span class="st">'t magical enough, so we have to wrap the train operations inside tf.control_dependencies block so the batch normalization layers get updated. The below is the code from [Udacty](https://github.com/udacity/deep-learning/blob/master/dcgan-svhn/DCGAN.ipynb) but since I added batch norm to both the generator and the discriminator this didn'</span>t work <span class="cf">for</span> me:</span>
<span id="cb5-375"><a href="#cb5-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-376"><a href="#cb5-376" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb5-377"><a href="#cb5-377" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):</span>
<span id="cb5-378"><a href="#cb5-378" aria-hidden="true" tabindex="-1"></a>        d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1<span class="op">=</span>beta1).minimize(d_loss, var_list<span class="op">=</span>d_vars)</span>
<span id="cb5-379"><a href="#cb5-379" aria-hidden="true" tabindex="-1"></a>        g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1<span class="op">=</span>beta1).minimize(g_loss, var_list<span class="op">=</span>g_vars)</span>
<span id="cb5-380"><a href="#cb5-380" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-381"><a href="#cb5-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-382"><a href="#cb5-382" aria-hidden="true" tabindex="-1"></a>After much googling <span class="kw">and</span> <span class="bu">help</span> <span class="im">from</span> the forums, this worked:</span>
<span id="cb5-383"><a href="#cb5-383" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-384"><a href="#cb5-384" aria-hidden="true" tabindex="-1"></a>```python</span>
<span id="cb5-385"><a href="#cb5-385" aria-hidden="true" tabindex="-1"></a><span class="co"># need this to make batch normalization work properly during inference</span></span>
<span id="cb5-386"><a href="#cb5-386" aria-hidden="true" tabindex="-1"></a>    update_ops <span class="op">=</span> tf.get_collection(tf.GraphKeys.UPDATE_OPS)</span>
<span id="cb5-387"><a href="#cb5-387" aria-hidden="true" tabindex="-1"></a>    g_updates  <span class="op">=</span> [opt <span class="cf">for</span> opt <span class="kw">in</span> update_ops <span class="cf">if</span> opt.name.startswith(<span class="st">'generator'</span>)]</span>
<span id="cb5-388"><a href="#cb5-388" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb5-389"><a href="#cb5-389" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> tf.control_dependencies(g_updates):</span>
<span id="cb5-390"><a href="#cb5-390" aria-hidden="true" tabindex="-1"></a>        d_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1).minimize(d_loss, var_list<span class="op">=</span>d_vars)</span>
<span id="cb5-391"><a href="#cb5-391" aria-hidden="true" tabindex="-1"></a>        g_train_opt <span class="op">=</span> tf.train.AdamOptimizer(learning_rate, beta1).minimize(g_loss, var_list<span class="op">=</span>g_vars)</span>
<span id="cb5-392"><a href="#cb5-392" aria-hidden="true" tabindex="-1"></a>```</span>
<span id="cb5-393"><a href="#cb5-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-394"><a href="#cb5-394" aria-hidden="true" tabindex="-1"></a>Putting this here becuase there must be better<span class="op">/</span>simpler easier way to do batch_norm without getting into the graph bowels of tensorflow.</span>
<span id="cb5-395"><a href="#cb5-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-396"><a href="#cb5-396" aria-hidden="true" tabindex="-1"></a>todo: reimplement <span class="kw">in</span> Keras</span>
<span id="cb5-397"><a href="#cb5-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-398"><a href="#cb5-398" aria-hidden="true" tabindex="-1"></a>[Project notebok](https:<span class="op">//</span>github.com<span class="op">/</span>khalido<span class="op">/</span>nd101<span class="op">-</span>projects<span class="op">/</span>blob<span class="op">/</span>master<span class="op">/</span>dlnd_face_generation.ipynb)</span>
<span id="cb5-399"><a href="#cb5-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-400"><a href="#cb5-400" aria-hidden="true" tabindex="-1"></a><span class="co">## All done!</span></span>
<span id="cb5-401"><a href="#cb5-401" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-402"><a href="#cb5-402" aria-hidden="true" tabindex="-1"></a>Link to the official [course certificate](https:<span class="op">//</span>confirm.udacity.com<span class="op">/</span>FMMYQCGE).</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>