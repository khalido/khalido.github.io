[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This here is possibly a blog. A minor tweak to test CI."
  },
  {
    "objectID": "posts/reading/getting_to_zero.html",
    "href": "posts/reading/getting_to_zero.html",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "",
    "text": "This QE gently and clearly explains the current scientific consensus on global warming and that it is an urgent crisis which Australia needs to tackle. It then lays out specific ways using today’s tech (solar, wind, storage and gas) by which the transition can occur."
  },
  {
    "objectID": "posts/reading/getting_to_zero.html#global-warming",
    "href": "posts/reading/getting_to_zero.html#global-warming",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "global warming",
    "text": "global warming\n\nGlobal warming is happening, and the science is incontrovertible.\nLots of charts from CSIRO’s State of the Climate - video summary\nmethane clathrate (concentrated methane gas trapped in a crystal structure of water) could be released as permafrost warms\n\nmethane is 28 times more potent than CO2 and the energy equivalent is more than the known reserves of oil, coal and shale combined.\n\nA slow response: So far, in most countries, energy from fossil fuels has failed by approx 5-6% e.g Germany from 84 to 78, USA from 86 to 80, Aus 96 to 91."
  },
  {
    "objectID": "posts/reading/getting_to_zero.html#the-technology",
    "href": "posts/reading/getting_to_zero.html#the-technology",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "the technology",
    "text": "the technology\n\nApprox 82% of Australia’s emissions come from burning fossil fuels for electricity, stationary energy and transport. These can be replaced with electric sources, the remaining 18% is more difficult.\n\n\nTech we can’t use:\n\nNuclear is zero carbon but costly and politically unviable. SMR reactors still far away, and fusion a dream.\nHydroelectric dams are great but politically difficult, Australia hasn’t had any large ones in over 50 years\nbiomass isn’t practical at scale, currently close to 1% of total energy production, but scaling it puts pressure on agriculture and native forests\nwaste to electricity - can contribute only a small portion\ngeothermal energy - short supply in most of the world\nwave power and tidal energy - decades of effort has led to 0.005% of global electricity production in 2018.\nExotics like solar panels in space beaming energy are still science fiction\n\n\n\nSolar and Wind\nThis leaves us with solar and wind, which are very different from traditional generators in that fossil fuel generators are synchronous, while solar and wind are non-synchronous. Electric grids were designed for synchronous generators.\n\nSolar panels output a constant current which inverters turn into AC.\nWind turbines output AC but the frequency varies with the wind, so they need mechanical controllers or inverters and sometime synchronous condensers\nsolar has no rotational inertia, wind has very little useful inertia. So you need batteries or capacitor to get to the required inertia and frequency output.\n99% of new capacity in Australia has been solar and wind, globally about 50%.\nIn 2019 Australia had the highest per cap solar in the world at 644 watts/person.\nRBA estimates that solar with 6 hr pumped hydro storage would cost $100 MW/hr vs a new black coal cost of $150 MW/hr.\nCoal can generate at 90% capacity, solar ranges from 10-25%, wind 30-50%.\n\nAEMO estimates a 13GW decline in coal capacity requires 34GW in solar and win.\n\nSolar panels available commercially in 2021 operate at 22%, theoretical limit is about 33%.\nWind turbines extract 35-45% of the wind energy passing through the swept rotor area, rising to 50% in strong winds. The theoretical max is 59%.\nHave to consider the efficiency of how the energy is used - there is a lot of improvements possible.\nRooftop solar could theoretically supply 20-50% of daytime demand, but this requires sophisticated management of all the domestic solar rooftops and batteries.\n\n\n\nStorage\nBatteries!\n\nAustralia is the world’s largest exporter of lithium, with 1/3 the reserves of Chile. (51K tons in 2018)\nfor grid scale batteries, it’s the power - megawatts and the megawatt hours which tell you how long it can operate for at full power.\nround trip efficiency is 85-95%.\n\nPumped Hydro\n\npump water up a gravity potential, release water to convert back to electricity.\nround trip efficiency is 70-80%.\n94% of grid storage globally is pumped hydro\nANU estimates that Aus has 22K potential sites for pumped hydro\n\nHydrogen\n\nHydrogen is the first element on the periodic table and makes up 94% of all atoms in the universe. On earth it is bound to other elements, mostly water.\nElectrolysis: separate water into hydrogen and oxygen by passing an electric current through water with a bit of salt.\nMcKinsey projects clean hydrogen going from USD6 to USD2.6 in 2030.\nround trip efficiency is only 35% so only viable when electricity is essentially free, e.g rooftop solar during the day\ncan store underground in natural salt caverns for long term storage\n\nNatural gas\n\nexists, good for firming, use will lower over time as renewables and storage ramp up\nnat gas allows the use of increasing wind and solar now and closing down coal plants faster than waiting for future storage to be built. Alan argues that some gas is necessary, don’t let the perfect be the enemy of the good\n31% lower total carbon emissions than coal\ncan ue a blend of clean hydrogen and natural gas"
  },
  {
    "objectID": "posts/reading/getting_to_zero.html#other-emitters",
    "href": "posts/reading/getting_to_zero.html#other-emitters",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "Other Emitters",
    "text": "Other Emitters\nstationary energy\n\nmanufacturing, energy, building, mining, agriculture, forestry, fishing sectors are the next biggest emitters after electcity.\nall thse sectors have clear ways to decarbonize, e.g using clean electricity for heat vs burning fossil fuels, replacing\n\nMobility sector\n\nAus: 18% of all emissions, in the US its 28%\nBattery Electric Vehicles will win\nhydrogen isn’t as energy efficient as batteries - for the same initial electricity an electric car would drive 2.5x further than a hydrogen powered car. But hydrogen is easy to store in bulk so is suitable for heavy transport."
  },
  {
    "objectID": "posts/reading/getting_to_zero.html#energy-export",
    "href": "posts/reading/getting_to_zero.html#energy-export",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "Energy export",
    "text": "Energy export\n\nAustralia is well positioned to export green energy, in the form of hydrogen or ammonia, or as embodied in products like steel, aluminum and fertilizers produced using renewable energy,"
  },
  {
    "objectID": "posts/reading/getting_to_zero.html#summary",
    "href": "posts/reading/getting_to_zero.html#summary",
    "title": "Quarterly Essay 81: Getting to Zero - Australia’s Energy Transition",
    "section": "Summary",
    "text": "Summary\nRenewables are here and change can happen very quickly. Australia risks being left behind if it isn’t ambitious.\nAlan ends on the hopeful note that Australia is well positioned to both transition itself to renewables and export clean energy."
  },
  {
    "objectID": "posts/reading/indian-summer.html",
    "href": "posts/reading/indian-summer.html",
    "title": "Indian Summer: The Secret History of the End of an Empire",
    "section": "",
    "text": "A good, easy to read book on how the British exited their Indian empire and left behind a fractured subcontinent.\nThere are a lot of words on the end of the British empire in India, but many of them suffer from lack of access or a bias towards some of the actors in the bloody saga.\nWhy are there two countries in the first place? Pakistani’s would have you think its because muslims couldn’t\n\nWhat was created, then, in East Bengal, soon to be East Pakistan and afterward Bangladesh, was a nation deliberately designed to be incapable of supporting itself. At least one half of Pakistan was set up to fail."
  },
  {
    "objectID": "posts/reading/termination-shock.html",
    "href": "posts/reading/termination-shock.html",
    "title": "Termination Shock",
    "section": "",
    "text": "This is a book about climate change, all hung about on characters. Now Neal is great at learning things and telling them to you, and I’ve enjoyed his earlier works, info dumps and all. I like fantastical characters, like Jack, Eliza and Enoch Root in the Baroque Cycle, but this book… has a lot of ideas and interesting exposition on how a thing could practically happen, but don’t read it for the ppl.\nI did mostly enjoy and finish the book but I’m not even sure it is a book, really this should have been a series of long form articles with a lot of the stuff dropped out of it.\nNeal Stephenson is a great thinker, and really does put the research into science fiction in terms of projecting what is actually possible into the future BUT I just wish he would… just go easy with the too many fantastical characters which don’t really do a good job of being.. characters. No one is going to read this book for the literary value, its for his ideas, but for the near future of climate ideas you’re better off reading The Ministry for the Future by KSR.\nHear the man speak himself:"
  },
  {
    "objectID": "posts/reading/the-key-man.html",
    "href": "posts/reading/the-key-man.html",
    "title": "The Key Man",
    "section": "",
    "text": "The rise and fall of Arif Naqvi is fascinating, but I found this book a bit wanting.\nFirst, the story itself:\nThe basic story is as old as stories go - man does well, gets delusions of granduer and tries to live way above his station, and in doing so falls all the way down to earth.\nThe really sad thing in the whole story is that initially Arif Naqvi was good at investing - not great, but good enough - which is why so many famous names invested in his funds, but fund managers work hard for their cut and most of them never become as rich as their investors.\nIf you are worth 40-60 million dollars, don’t try to live like a multi-billionaire and end up eating your clients money because you are delusional enough to think you will make billions any day now.\nIt is especially tragic that Arif Naqvi failed, becuase it does seem that he had enough good ideas that in a actual fund where people worked to do make their investments better instead of just living the jetset life, enough would have worked out to effect the world for the better.\n3 stars:\nThere is a bit too much moralizing and not enough finance thoughout the book. Similar books about other financiers who crashed and burned tell more and let you make your own judgements.\nWhen you’re talking about ppl managing billions of dollars its besides the point to that they spend money on travel and expensive dinners, and to keep comparing the cost of hosting a prime minster to dinner to the yearly income of 20 poor ppl. Didn’t want to bring up the racism word, but in the dozens of other similar books about white financiers, this doesn’t happen after every few pages.\nYou don’t learn much about Abraaj and how it did stuff - too much of the detail is basically a summary of Arif’s numerous speeches around the world.\nI feel the journalists hit upon a super hot story - that a big PE fund was a failing, and concentrated all their firepower into getting out that story. This book - is just an extended version of that, and doesn’t do the actual rise any justice, and thus misses out on an exciting story.\nAbraaj invested in the developing world - which is where obviously a good chunk of the story happened, but the authors spend most of the book following Arif from one Davos conference to the next."
  },
  {
    "objectID": "posts/reading/a-passage-north.html",
    "href": "posts/reading/a-passage-north.html",
    "title": "A Passage North",
    "section": "",
    "text": "A beautifully written at places book - takes us into the heartbreak of the Sri Lankan civil war.\nThere are some beautiful lines\n\nTHE PRESENT, WE assume, is eternally before us, one of the few things in life from which we cannot be parted.\n\nOn women in public spaces:\n\nIn Delhi and many of the Hindi-speaking states more generally male stares were different, were intensely unselfconscious and intensely unrelenting, so that even when you weren’t being harassed in more explicit verbal or physical ways you still had to use all of your psychological resources to resist these gazes over the course of each day, to prevent these men from trying to enter your soul through your eyes, like strangers who enter the privacy of your house without permission and without even bothering to take off their shoes. You had to employ these psychological resources so constantly over the course of the day, losing even the freedom to think autonomously in your own mind, that by the time you returned home you were always utterly exhausted."
  },
  {
    "objectID": "posts/reading/the_fifth_risk.html",
    "href": "posts/reading/the_fifth_risk.html",
    "title": "The Fifth Risk",
    "section": "",
    "text": "This is a awesome read. In many countries, many people have long stopped wondering or caring about how their country actually functions, and this book takes us through a bunch of really useful government functions, from providing food and shelter to the needy to… just about everything you can imagine.\nThan along comes a man (Trump) and his political appointee who could care less about reality and are busy throwing spanners in the very system that runs America - the many agencies which DO ALL THE THINGS.\nIn a broader sense, this book is about more than just Trump and his bunch of inept political appointees running places they have no business running, but rather society as a whole and its relation to the the government.\nThis quote stuck with me:\n\nThe relationship between the people and their government troubled her. The government was the mission of an entire society: why was the society undermining it? “I’m routinely appalled by how profoundly ignorant even highly educated people are when it comes to the structure and function of our government,” she said. “The sense of identity as Citizen has been replaced by Consumer. The idea that government should serve the citizens like a waiter or concierge, rather than in a ‘collective good’ sense.”\n\nThis book is specifically about America, but this malaise of ignoring reality can be seem across much of the world…"
  },
  {
    "objectID": "posts/reading/the-naked-dont-fear-the-water.md.html",
    "href": "posts/reading/the-naked-dont-fear-the-water.md.html",
    "title": "The Naked Don’t Fear the Water",
    "section": "",
    "text": "Matthieu Aikins follows a brown refugee from Afghanistan through to Europe. It gives you an inside view of a refugees journey, starting from the why, to the how.\nI would have preferred to read an account written by an actual migrant first hand, but they are obviously too busy with a ton of real life issues to be writing books on a very dangerous journey.\n\nImagine the cities of the world connected by a network of paths that measure not physical distance but danger: the risk of getting arrested, stuck in transit, scammed, kidnapped, or killed.\n\nThis book is a must read in a world where the disparity between the truly poor and the first world middle class seems to keep increasing and forcing more and more people to seek a better life elsewhere.\nHis twtr, electriclit interview, another one at thebaffler and a talk about the book:"
  },
  {
    "objectID": "posts/vs_code.html",
    "href": "posts/vs_code.html",
    "title": "Visual Studio Code",
    "section": "",
    "text": "VS Code has so many extensions and stuff, that this is an evoloving list of useful things I’ve found for it.\n[Sanddance](https://marketplace.visualstudio.com/items?itemName=msrvida.vscode-sanddance - this is great, point it at a csv file and boom! you can see your data. website\n\nBy using easy-to-understand views, SandDance helps you find insights about your data, which in turn help you tell stories supported by data, build cases based on evidence, test hypotheses, dig deeper into surface explanations, support decisions for purchases, or relate data into a wider, real world context.\n\n\nSandDance uses unit visualizations, which apply a one-to-one mapping between rows in your database and marks on the screen. Smooth animated transitions between views help you to maintain context as you interact with your data.\n\nVisual Studio IntelliCode - provides AI assisted autocomplete for python and typescript/javascript. Its trained on high quality github code and suggests the most relevant suggestion first.\nMarkdown All-in-One - Adds useful markdown tools like making lists with sublists, adding a table of contents and so on.\nPyLance - type hint enabled language server for python."
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html",
    "href": "posts/code/quarto_notebook_test.html",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#polar-axis",
    "href": "posts/code/quarto_notebook_test.html#polar-axis",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "href": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "title": "Quarto Jupyter notebook test",
    "section": "Lets try some seaborn…",
    "text": "Lets try some seaborn…\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#testing",
    "href": "posts/code/quarto_notebook_test.html#testing",
    "title": "Quarto Jupyter notebook test",
    "section": "testing…",
    "text": "testing…\nwhy isn’t quarto creating a _freeze directory? What stops it?"
  },
  {
    "objectID": "posts/windows.html",
    "href": "posts/windows.html",
    "title": "Windows 10",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC.\n\n\n\ninstall scoop by running this in powershell:\niwr -useb get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable.\n\n\n\n\nFollow the real wsl instructions, my clif notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nNow install the latest linux kernel update package.\nNow install debian from the windows store."
  },
  {
    "objectID": "posts/windows.html#factory-reset",
    "href": "posts/windows.html#factory-reset",
    "title": "Windows 10",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC."
  },
  {
    "objectID": "posts/windows.html#setup",
    "href": "posts/windows.html#setup",
    "title": "Windows 10",
    "section": "",
    "text": "install scoop by running this in powershell:\niwr -useb get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable."
  },
  {
    "objectID": "posts/windows.html#wsl-2",
    "href": "posts/windows.html#wsl-2",
    "title": "Windows 10",
    "section": "",
    "text": "Follow the real wsl instructions, my clif notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nNow install the latest linux kernel update package.\nNow install debian from the windows store."
  },
  {
    "objectID": "posts/recipes/vegan-shami-kebab.html",
    "href": "posts/recipes/vegan-shami-kebab.html",
    "title": "Vegan Shami kebab",
    "section": "",
    "text": "Makes kebabs for frying or grilling. Goes well in burgers."
  },
  {
    "objectID": "posts/recipes/vegan-shami-kebab.html#ingredients",
    "href": "posts/recipes/vegan-shami-kebab.html#ingredients",
    "title": "Vegan Shami kebab",
    "section": "Ingredients",
    "text": "Ingredients\n\n1 can chickpeas, drained, well-rinsed and mashed\n1 can black beans, drained, well-rinsed and mashed\n1 red onion, finely diced\n1 zucchini, grated (around 1-1.5 cups grated)\n1 beetroot, grated\n2-3 brussel sprouts, finely chopped (optional)\n1 bunch finely chopped coriander\n2 spicy chillis finely chopped\n3 tbsp red wine vinegar\n1 tbsp sriracha sauce\n2 tbsp smooth peanut butter\n1 tsp cumin\n2 tsp garlic (crushed from bottle)\n2 tsp black pepper\n1 tsp sea salt (or 1/2 for salt haters)\n1 cup quick oats or rolled oats\n2 tbsp extra virgin olive oil"
  },
  {
    "objectID": "posts/recipes/vegan-shami-kebab.html#directions",
    "href": "posts/recipes/vegan-shami-kebab.html#directions",
    "title": "Vegan Shami kebab",
    "section": "Directions",
    "text": "Directions\n\nAfter draining and rinsing the chickpeas and/or black beans, place them in a bowl and mash them with a super strong masher or use a food processor. Don’t over fine them. Need some texture.\nAdd all the other ingredients to the bowl.\nUse your hands to mix very well.\nMake 7-8 patties and place on oven paper.\nCook on the BBQ for 5-8 minutes each side or until looking awesomely grilled.\nYou can also fry these in a pan with some oil for 3-5 minutes a side.\n\nMake patties and freeze for later, leave out to thaw for 2 hrs before cooking."
  },
  {
    "objectID": "posts/recipes/aloo-sabzi.html",
    "href": "posts/recipes/aloo-sabzi.html",
    "title": "Aloo Sabzi",
    "section": "",
    "text": "The basic aloo sabzi. If made right, its awesome. This is the staple breakfast/lunch/camping/everything Pakistani dish."
  },
  {
    "objectID": "posts/recipes/aloo-sabzi.html#ingredients",
    "href": "posts/recipes/aloo-sabzi.html#ingredients",
    "title": "Aloo Sabzi",
    "section": "Ingredients",
    "text": "Ingredients\n\n4-6 potatoes\n2-3 tomatoes chopped\n1-2 tsp cumin seeds\ncoriander, chopped\noptional: two fresh chillis\n\n\nSpice\n\n1/2 tsp chilli or better yes 1/2 tsp fresh indian chilli flakes\n1 tsp salt (1 tsp per cup of daal)\n1/4 tsp haldi"
  },
  {
    "objectID": "posts/recipes/aloo-sabzi.html#directions",
    "href": "posts/recipes/aloo-sabzi.html#directions",
    "title": "Aloo Sabzi",
    "section": "Directions",
    "text": "Directions\n\nheat some oil, fry cumin seeds till sizzling and turning brown\noptional: slice a chilli down the middle and toss it in\nadd potatoes, stir, add spices and stir throughly\ntoss in tomatoes, stir, then add a tbsp or two of water, lower heat, cover and leave to simmer\ndish is done once potatoes are nice and soft but not falling apart. anywhere from 15-25 minutes.\nonce done, take of heat and sprinkle coriander"
  },
  {
    "objectID": "posts/recipes/aloo-sabzi.html#variations",
    "href": "posts/recipes/aloo-sabzi.html#variations",
    "title": "Aloo Sabzi",
    "section": "Variations",
    "text": "Variations\n\nhttps://www.thespicespoon.com/blog/aloo/"
  },
  {
    "objectID": "posts/recipes/urud-dhal.html",
    "href": "posts/recipes/urud-dhal.html",
    "title": "Dhal Fry",
    "section": "",
    "text": "Recipe from Lahore. Needs some more tips from the original cook."
  },
  {
    "objectID": "posts/recipes/urud-dhal.html#ingredients",
    "href": "posts/recipes/urud-dhal.html#ingredients",
    "title": "Dhal Fry",
    "section": "Ingredients",
    "text": "Ingredients\n\n1 cup urud dhal (split black mung beans) - mash ki dhal\n2 tomatoes diced\n1 onion sliced and cut a bit more\n1-2 green chilis, chopped\n1/2-1 tsp garlic crushed\n1/2-1 tsp ginger minced\n1/2 tsp tamarind paste\ncoriander, half bunch\nfresh ginger, cut into strips\n\n\nSpices\n\n1/2-1 tsp salt\n1/4 tsp haldi (tumeric)\n1/4 tsp cumin powder\n1/4 tsp coriander powder - check if needed\n1/4 tsp zeera (cumin seeds) - make this 1/2 tsp I think\n1/4 to 1/2 tsp chilli powder"
  },
  {
    "objectID": "posts/recipes/urud-dhal.html#steps",
    "href": "posts/recipes/urud-dhal.html#steps",
    "title": "Dhal Fry",
    "section": "Steps",
    "text": "Steps\n\nrinse 2-4 times, then soak dhal for 30-60 minutes\nheat oil, fry onions for 5-6 minutes\nAdd garlic, ginger, 1 chilli, stir, add tamarind paste , fry 2-3 minutes\nadd spices, stir\nAdd tomotoes, fry 5-10 minutes until oil seperates, stir frequently\ndrain water from dhal and add, fry for 2-3 minutes\nadd enough water (1/2-1 cup) to cover dhal, then put a lid on, lower heat once boiling\ncheck if dhal is soft enough to eat, then serve (boil on high while stirring at end if too much water)\nWhen dhal is looking ready to eat, add ginger, 1 chilli and fry for a minute, then stir through coriander.\nAdd coriander, stir, garnish with a bit of the chillis, ginger and coriander\n\nOptional: Serve with lemon or lime wedges."
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html",
    "href": "posts/recipes/Aloo Anda ka salan.html",
    "title": "Aloo Anda ka salan",
    "section": "",
    "text": "Shamsad ki recipe, punjabi style. This is what she makes at home for her family. Its the first time I ever had this dish in Pakistan."
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html#ingredients",
    "href": "posts/recipes/Aloo Anda ka salan.html#ingredients",
    "title": "Aloo Anda ka salan",
    "section": "Ingredients",
    "text": "Ingredients\n\n2-3 eggs1\n3 small potatoes, diced into biggish pieces\n1 tomatoes\n1 onion\n2 green chillis\ncoriander, chopped"
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html#spice",
    "href": "posts/recipes/Aloo Anda ka salan.html#spice",
    "title": "Aloo Anda ka salan",
    "section": "Spice",
    "text": "Spice\n\n1/2 tsp red chilli\n1/2 tsp salt\n1/4 tsp tumeric/haldi\n1/4 tsp cumin powder\n1/4 tsp coriander powder"
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html#directions",
    "href": "posts/recipes/Aloo Anda ka salan.html#directions",
    "title": "Aloo Anda ka salan",
    "section": "Directions",
    "text": "Directions\n\nblend onion, tomato and green chillis and add to hot oil\nAdd all the spices and fry till the oil seperates\nAdd potatoes and mix for 5-6 minutes\nCrack in the eggs eggs and keep mixing for 3-4 minutes till eggs are cooked enough to not stick.\nAdd enough water to cover potatoes a little bit, once it starts boiling, then lower heat, cover with a lid for 10-15 minutes.\n\nServe with a bit of coriander and a pinch of garama masala on top."
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html#notes",
    "href": "posts/recipes/Aloo Anda ka salan.html#notes",
    "title": "Aloo Anda ka salan",
    "section": "Notes",
    "text": "Notes\nThe traditional anda aloo ka salan is more of a salan with boiled eggs floating in it. Never liked it heaps, this one is just plain tastier.\nA more traditional recipe looks like Spice Spoon’s Ami’s Aloo Anday."
  },
  {
    "objectID": "posts/recipes/Aloo Anda ka salan.html#footnotes",
    "href": "posts/recipes/Aloo Anda ka salan.html#footnotes",
    "title": "Aloo Anda ka salan",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDesi eggs preferred. You could also call them organic, free range eggs.↩︎"
  },
  {
    "objectID": "posts/recipes/spicychickpeas.html",
    "href": "posts/recipes/spicychickpeas.html",
    "title": "Chana Masala aka Spicy Chickpeas",
    "section": "",
    "text": "aka spicy chickpeas with a ton of ginger."
  },
  {
    "objectID": "posts/recipes/spicychickpeas.html#spices-put-aside-in-small-bowl",
    "href": "posts/recipes/spicychickpeas.html#spices-put-aside-in-small-bowl",
    "title": "Chana Masala aka Spicy Chickpeas",
    "section": "Spices (put aside in small bowl)",
    "text": "Spices (put aside in small bowl)\n\n1 tsp red chilli powder\n1 tsp cumin powder\n1/2 tsp turmeric powder\nSalt 1 tsp"
  },
  {
    "objectID": "posts/recipes/spicychickpeas.html#ingredients",
    "href": "posts/recipes/spicychickpeas.html#ingredients",
    "title": "Chana Masala aka Spicy Chickpeas",
    "section": "Ingredients",
    "text": "Ingredients\n\n4-5 medium tomatoes, diced\n4 cloves of garlic, minced\n2 cans organic chickpeas (400g).\n7-8cm (3in) ginger, cut into thin strips\n2-4 green thai bird chillies (or any other fresh chilli), chopped.\nCoriander chopped\nlime or lemon wedges"
  },
  {
    "objectID": "posts/recipes/spicychickpeas.html#preparation",
    "href": "posts/recipes/spicychickpeas.html#preparation",
    "title": "Chana Masala aka Spicy Chickpeas",
    "section": "Preparation:",
    "text": "Preparation:\n\nPlace a saucepan on medium-high heat and add oil and garlic. When the garlic begins to smell fragrant (make sure it doesn’t start to brown), add spices. Saute for 1-2 minutes.\nAdd chickpeas with the liquid in the can and stir.\nAdd diced tomatoes, turn flame to medium-low and cover with lid for 10 minutes and simmer.\nRemove lid, add more salt if needed, and the ginger, then chillies and stir a bit. The dish is done, maybe cook for 1-2 more minutes, take of heat and add coriander.\nServe with the lime or lemon wedges.\n\nSlice up a cucumber, maybe some yogurt, and serve with one of:\n\nbread rolls, slice and butter and toast in a frying pan. Don’t squish the bread!\nrice (but a bit boring)\nroti! naan! or regular sliced bread. anything goes."
  },
  {
    "objectID": "posts/recipes/spicychickpeas.html#the-story",
    "href": "posts/recipes/spicychickpeas.html#the-story",
    "title": "Chana Masala aka Spicy Chickpeas",
    "section": "the story",
    "text": "the story\nThe ginger (and chilli) makes the dish. Don’t skimp. And don’t burn the garlic! This dish only works with real desi chilli powder."
  },
  {
    "objectID": "posts/recipes/daal-sprouts.html",
    "href": "posts/recipes/daal-sprouts.html",
    "title": "Sprouted Daal (lentils)",
    "section": "",
    "text": "A faster daal recipe. 30 min, good for 3 ppl, 4 with a small side dish."
  },
  {
    "objectID": "posts/recipes/daal-sprouts.html#ingredients",
    "href": "posts/recipes/daal-sprouts.html#ingredients",
    "title": "Sprouted Daal (lentils)",
    "section": "Ingredients",
    "text": "Ingredients\n\n200 grams mung beans sprouts (rinse)\n400 grams sprouted lentils (rinse)\n2-3 tomatoes, diced.\n1 red onion, sliced\n2 tsp garlic paste (not heaped)\n3-4 chilli, chopped\n1.5 lemons, squeeze the juice\ncoriander, chopped\n\n\nSpice\n\n1 tsp cumin powder\n1.5 tsp salt"
  },
  {
    "objectID": "posts/recipes/daal-sprouts.html#directions",
    "href": "posts/recipes/daal-sprouts.html#directions",
    "title": "Sprouted Daal (lentils)",
    "section": "Directions",
    "text": "Directions\n\nfry onion for 5 minutes in a large pan\nadd lemon juice and fry 5 more\nadd chilli and garlic and few 3-4 minutes\nchuck in tomatoes and a bit of water, cook till jammy, stir frequently\nadd spice and stir, then add the washed sprouts and mix well. Cook 3-5 minutes.\ntake of heat, mix in coriander\n\nServe with… anything really. Chapati, soft tortilla, toast, dahi, whatever. Goes well on toasted sourdough."
  },
  {
    "objectID": "posts/recipes/kheera-raita.html",
    "href": "posts/recipes/kheera-raita.html",
    "title": "Kheera ka raita",
    "section": "",
    "text": "cucumber + mint raita"
  },
  {
    "objectID": "posts/recipes/kheera-raita.html#ingredients",
    "href": "posts/recipes/kheera-raita.html#ingredients",
    "title": "Kheera ka raita",
    "section": "Ingredients",
    "text": "Ingredients\n\n1-2 tsp cumin seeds\n1 big cucmber or 2 smallish\n1-2 cup greek yoghurt (or more, whatever fits in your bowl)\nhalf a bunch worth of fresh mint leaves, chopped fine\n1/2 tsp salt\n1 chilli, deseeded and cut (optional, cause a good raita isn’t spicy, the food is.)"
  },
  {
    "objectID": "posts/recipes/kheera-raita.html#steps",
    "href": "posts/recipes/kheera-raita.html#steps",
    "title": "Kheera ka raita",
    "section": "Steps",
    "text": "Steps\n\ntoast cumin seeds for 3-4 minutes in a hot pan, then grind with a mortar and pestle\ngrate half the cucumber and place in a bowl, dice the other half\nadd yoghurt, salt, and almost all the mint, cucumber, chilli and cumin and mix well\nsprinkle remaining mint, cucumber, chilli and cumin on top and serve.\n\nGoes well with biryani. Don’t use any non-fresh powders in raita, a wise cook once told me. Simple is good. Resist the urge to add more things."
  },
  {
    "objectID": "posts/recipes/bhaigan-ka-bhurta.html",
    "href": "posts/recipes/bhaigan-ka-bhurta.html",
    "title": "Bhaigan ka burta",
    "section": "",
    "text": "Recipe from Lahore. Smokey yummy goodness."
  },
  {
    "objectID": "posts/recipes/bhaigan-ka-bhurta.html#ingredients",
    "href": "posts/recipes/bhaigan-ka-bhurta.html#ingredients",
    "title": "Bhaigan ka burta",
    "section": "Ingredients",
    "text": "Ingredients\n\n1kg eggplants\n2 tomatoes\n2 onions\n2 green chilli\n2-3 tbsp desi yoghurt\n1 lemon\ncoriander, chopped\n\n\nSpices\n\n1 tsp red chilli\n1/2 tsp haldi\n1/2 tsp salt"
  },
  {
    "objectID": "posts/recipes/bhaigan-ka-bhurta.html#steps",
    "href": "posts/recipes/bhaigan-ka-bhurta.html#steps",
    "title": "Bhaigan ka burta",
    "section": "Steps",
    "text": "Steps\n\nHeat eggplants 5-10 minutes on stove, place in bowl of tap water. Then peel skin off and dice.\nBlend the tomotoes and onions and fry in oil.\nAdd spices, fry till oil seperates\nAdd eggplants, mix well and fry for 5-6 minutes.\nAdd yoghurt and lemon, mix for 2-3 minutes\nlower heat, add coriander and cover for 5 minutes\n\nSprinkle some coriander and serve."
  },
  {
    "objectID": "posts/recipes/bhaigan-ka-bhurta.html#notes",
    "href": "posts/recipes/bhaigan-ka-bhurta.html#notes",
    "title": "Bhaigan ka burta",
    "section": "Notes",
    "text": "Notes\nCheck if this recipe needs garlic or ginger."
  },
  {
    "objectID": "posts/writing-about-code/ms_openhack_2018.html",
    "href": "posts/writing-about-code/ms_openhack_2018.html",
    "title": "Microsoft OpenHack 2018 Sydney",
    "section": "",
    "text": "My first ever hackathon! Or so it was called. Notes:\n\n\nFirst up, we learned about and how to use Microsoft Azure. I hadn’t used it and it seems to have rebuilt AWS but with a nicer interface. These massive clouds are all so wide and deep that just clicking around in the interface can take days since things keep coming up.\nOur team spun up a Ubuntu Data Science Virtual Machine (DSVM) on Azure, which comes with JupyterHub ready to run out. It was suprinsingly easy. Still, JupyterHub needs to work on a multiuser interface, if not a full GoogleDocs thingamajig, at least one where other users can hop onto your notebook as you drive it.\nThe hackathon was suprisingly straightforward - learned various bits and bobs, one main one being the Microsoft Custom Vision Service which looks somewhat similar Amazon Rekognition or Google Cloud Vision.\nThe python api is a little bit tricky since every cloud provider has their own vision of how their api’s should function, so it takes getting used to a bit, but once thats done its suprisingly easy to upload training images to custom vision, train the model (it automagics whatever its transfer learning from) and then send it images to predict on. Presto, barely a page of code and what used to be rocket science is now being rolled out all the world.\nAlso covered preprocessing images using the Pillow image libary. Its slower then opencv and others, but its so much nicer to write.\nAnyways, with all this fancy deep learning in the clouds and using apis and what not, I liked this bit of code which preprocessed all the training images:\ndef preprocess_all_images_in_dir(img_dir=\"gear_images/\", \n                              processed_dir=\"gear_images_processed/\", \n                              show_images=False):\n    \"\"\"processes all the images in the subdirectories of the passed in img_dir\n       and saves them in subdirectories of the same name in the processed_dir\"\"\"\n\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    for d in os.listdir(img_dir):\n        cur_dir = img_dir + d\n        new_dir = processed_dir + d\n\n        if not os.path.exists(new_dir):\n            os.makedirs(new_dir)\n\n        for img in os.listdir(cur_dir):\n            im = pre_process_image(cur_dir +\"/\"+ img)\n            if show_images:\n                clear_output(wait=True)\n                display(im)\n            im.save(new_dir + \"/\" + img)\n\npreprocess_all_images_in_dir()\nThe hard parts are being all out sourced, like deep learning, so it makes sense that the simple plumbing work is what many coders end up coding. It’s amazing how quickly now you can get things done by plumbing bits together. And Ideally, services is where its all at now."
  },
  {
    "objectID": "posts/writing-about-code/ms_openhack_2018.html#day-one-azure-basics",
    "href": "posts/writing-about-code/ms_openhack_2018.html#day-one-azure-basics",
    "title": "Microsoft OpenHack 2018 Sydney",
    "section": "",
    "text": "First up, we learned about and how to use Microsoft Azure. I hadn’t used it and it seems to have rebuilt AWS but with a nicer interface. These massive clouds are all so wide and deep that just clicking around in the interface can take days since things keep coming up.\nOur team spun up a Ubuntu Data Science Virtual Machine (DSVM) on Azure, which comes with JupyterHub ready to run out. It was suprinsingly easy. Still, JupyterHub needs to work on a multiuser interface, if not a full GoogleDocs thingamajig, at least one where other users can hop onto your notebook as you drive it.\nThe hackathon was suprisingly straightforward - learned various bits and bobs, one main one being the Microsoft Custom Vision Service which looks somewhat similar Amazon Rekognition or Google Cloud Vision.\nThe python api is a little bit tricky since every cloud provider has their own vision of how their api’s should function, so it takes getting used to a bit, but once thats done its suprisingly easy to upload training images to custom vision, train the model (it automagics whatever its transfer learning from) and then send it images to predict on. Presto, barely a page of code and what used to be rocket science is now being rolled out all the world.\nAlso covered preprocessing images using the Pillow image libary. Its slower then opencv and others, but its so much nicer to write.\nAnyways, with all this fancy deep learning in the clouds and using apis and what not, I liked this bit of code which preprocessed all the training images:\ndef preprocess_all_images_in_dir(img_dir=\"gear_images/\", \n                              processed_dir=\"gear_images_processed/\", \n                              show_images=False):\n    \"\"\"processes all the images in the subdirectories of the passed in img_dir\n       and saves them in subdirectories of the same name in the processed_dir\"\"\"\n\n    if not os.path.exists(processed_dir):\n        os.makedirs(processed_dir)\n\n    for d in os.listdir(img_dir):\n        cur_dir = img_dir + d\n        new_dir = processed_dir + d\n\n        if not os.path.exists(new_dir):\n            os.makedirs(new_dir)\n\n        for img in os.listdir(cur_dir):\n            im = pre_process_image(cur_dir +\"/\"+ img)\n            if show_images:\n                clear_output(wait=True)\n                display(im)\n            im.save(new_dir + \"/\" + img)\n\npreprocess_all_images_in_dir()\nThe hard parts are being all out sourced, like deep learning, so it makes sense that the simple plumbing work is what many coders end up coding. It’s amazing how quickly now you can get things done by plumbing bits together. And Ideally, services is where its all at now."
  },
  {
    "objectID": "posts/writing-about-code/sml-meetup-2018-09.html",
    "href": "posts/writing-about-code/sml-meetup-2018-09.html",
    "title": "SML 2018-09: Deep Reinforcement Learning",
    "section": "",
    "text": "SML’s 2018-09 meetup.\n\nSTEM Careers expo on Thu 6th Sep\n\nthe talks:\n\n\n\nKosuke, Takenaka Corporation is one of the top 5 largest construction companies in Japan. Their site supervisors perform site visits, record the progress of ongoing construction as well as assign appropriate resourcing from their workforce by writing up the report every evening. This involves taking a lot of photos and organizing them based on the construction progress.\n\n\nIn this talk, I walk you through an effort by Takenaka with help from Microsoft to automatically organize photos utilizing Keras and Transfer Learning to make workstyle innovation at construction sites possible.\n\n\ngithub\nTakanaka is a huge consturction company in Japan\nso much demand that only 5.7% of employees get 2 days a week, plus working population is decreasing\n\nneed to make work more efficent\n\nlots of photo related tasks - construction management relies on photos for order, evidence & education\n\nmanually organised, takes lots of time\n100 millions photos taken yearly\n\nUsed transfer learning with 10K photos split: train/test/valid split: 7,2,1 for a trial\nYou don’t need heaps of photos for transfer learning, the pretrained models have already been trained on heaps of data.\nKeras has a bunch of pretrained models ready to use.\nWe have to run a bunch of experiments to figure out stuff. Azure Machine Learning Services has lots of tools for experimentation.\nsee https://github.com/fujikosu/aml-keras-image-recognition\nthink about data augmentation - Keras’s built in functions for this are great\n\nThe company’s example was really powerful - a very small amount of deep learning solved a very big problem, since transfer learning and the cloud is making vision problems much easier to deal with.\n\n\n\n\nCurrently, SSG.COM, the subsidiary of Shinsegae Group the largest retailer in South Korea, is developing an AI chatbot service based on deep learning. It was decided that the internet as as service-based architecture is not efficient because it requires additional resources if the operation production scale is increased.\nWe will discuss why the AI model training layer does not have to be running at all times but the GPU-supported VM should be used continuously, to reduce costs to SSG.COM.\n\n\n@kritcs18, github\nssg.com is the largest retailer in South Korea, was working on a AI chatbot based on deep learning\npretty complex and expensive architecture, wanted to migrate to the cloud (PaaS)\nshifed to a serverless model with API gateways\nMS Azure Batch AI takes care of all the hardware, you focus on the work which needs to happen. Looks pretty pimp. The era of spinning up your own VM is fast ending.\n\nsuited for bigCo for sure, but for personal experiments there is too much configuration code (for now).\n\n\n\nBatch AI is a managed service that enables data scientists and AI researchers to train AI and other machine learning models on clusters of Azure virtual machines, including VMs with GPU support. You describe the requirements of your job, where to find the inputs and store the outputs, and Batch AI handles the rest.\n\nInteresting becuase companies are moving from running their own hardware, to hardware in the cloud, and now on to running apps themselves with the hardware spin up spin down somewhat abstracted (or in the process of getting so).\n\n\n\n\nHaving the right data is often as important as having the right model for building effective computer vision systems. Even if you have enough data, what do you do when don’t have the right labels? Common problems arising in image understanding include only having partially labelled data, have the wrong type of labels and noisy labels. Broadly these problems can be addressed by an area of research called weakly supervised learning. I will give an overview of some strategies coming from weakly supervised learning to help train good predictive models in the presence of these difficulties. The tools used will include ConvNets combined with some interesting applications of familiar friends such as the EM algorithm and graph-based clustering."
  },
  {
    "objectID": "posts/writing-about-code/sml-meetup-2018-09.html#kosuke-fujimoto-how-can-we-innovate-our-workstyle-at-construction-sites-with-image-classification",
    "href": "posts/writing-about-code/sml-meetup-2018-09.html#kosuke-fujimoto-how-can-we-innovate-our-workstyle-at-construction-sites-with-image-classification",
    "title": "SML 2018-09: Deep Reinforcement Learning",
    "section": "",
    "text": "Kosuke, Takenaka Corporation is one of the top 5 largest construction companies in Japan. Their site supervisors perform site visits, record the progress of ongoing construction as well as assign appropriate resourcing from their workforce by writing up the report every evening. This involves taking a lot of photos and organizing them based on the construction progress.\n\n\nIn this talk, I walk you through an effort by Takenaka with help from Microsoft to automatically organize photos utilizing Keras and Transfer Learning to make workstyle innovation at construction sites possible.\n\n\ngithub\nTakanaka is a huge consturction company in Japan\nso much demand that only 5.7% of employees get 2 days a week, plus working population is decreasing\n\nneed to make work more efficent\n\nlots of photo related tasks - construction management relies on photos for order, evidence & education\n\nmanually organised, takes lots of time\n100 millions photos taken yearly\n\nUsed transfer learning with 10K photos split: train/test/valid split: 7,2,1 for a trial\nYou don’t need heaps of photos for transfer learning, the pretrained models have already been trained on heaps of data.\nKeras has a bunch of pretrained models ready to use.\nWe have to run a bunch of experiments to figure out stuff. Azure Machine Learning Services has lots of tools for experimentation.\nsee https://github.com/fujikosu/aml-keras-image-recognition\nthink about data augmentation - Keras’s built in functions for this are great\n\nThe company’s example was really powerful - a very small amount of deep learning solved a very big problem, since transfer learning and the cloud is making vision problems much easier to deal with."
  },
  {
    "objectID": "posts/writing-about-code/sml-meetup-2018-09.html#krit-kamtuo-using-batch-ai-to-train-nlp-based-on-tensorflow-and-cntk-in-the-retail-industry",
    "href": "posts/writing-about-code/sml-meetup-2018-09.html#krit-kamtuo-using-batch-ai-to-train-nlp-based-on-tensorflow-and-cntk-in-the-retail-industry",
    "title": "SML 2018-09: Deep Reinforcement Learning",
    "section": "",
    "text": "Currently, SSG.COM, the subsidiary of Shinsegae Group the largest retailer in South Korea, is developing an AI chatbot service based on deep learning. It was decided that the internet as as service-based architecture is not efficient because it requires additional resources if the operation production scale is increased.\nWe will discuss why the AI model training layer does not have to be running at all times but the GPU-supported VM should be used continuously, to reduce costs to SSG.COM.\n\n\n@kritcs18, github\nssg.com is the largest retailer in South Korea, was working on a AI chatbot based on deep learning\npretty complex and expensive architecture, wanted to migrate to the cloud (PaaS)\nshifed to a serverless model with API gateways\nMS Azure Batch AI takes care of all the hardware, you focus on the work which needs to happen. Looks pretty pimp. The era of spinning up your own VM is fast ending.\n\nsuited for bigCo for sure, but for personal experiments there is too much configuration code (for now).\n\n\n\nBatch AI is a managed service that enables data scientists and AI researchers to train AI and other machine learning models on clusters of Azure virtual machines, including VMs with GPU support. You describe the requirements of your job, where to find the inputs and store the outputs, and Batch AI handles the rest.\n\nInteresting becuase companies are moving from running their own hardware, to hardware in the cloud, and now on to running apps themselves with the hardware spin up spin down somewhat abstracted (or in the process of getting so)."
  },
  {
    "objectID": "posts/writing-about-code/sml-meetup-2018-09.html#matt-gibson-learning-with-not-quite-the-right-labels",
    "href": "posts/writing-about-code/sml-meetup-2018-09.html#matt-gibson-learning-with-not-quite-the-right-labels",
    "title": "SML 2018-09: Deep Reinforcement Learning",
    "section": "",
    "text": "Having the right data is often as important as having the right model for building effective computer vision systems. Even if you have enough data, what do you do when don’t have the right labels? Common problems arising in image understanding include only having partially labelled data, have the wrong type of labels and noisy labels. Broadly these problems can be addressed by an area of research called weakly supervised learning. I will give an overview of some strategies coming from weakly supervised learning to help train good predictive models in the presence of these difficulties. The tools used will include ConvNets combined with some interesting applications of familiar friends such as the EM algorithm and graph-based clustering."
  },
  {
    "objectID": "posts/writing-about-code/dss-meetup.html",
    "href": "posts/writing-about-code/dss-meetup.html",
    "title": "DSS-2018-07: Michael Allwright and Inna Kolyshkina",
    "section": "",
    "text": "DSS-2018-07: Michael Allwright and Inna Kolyshkina\nData Science Sydney’s 2018-07 meetup.\n\ndata literacy is important for senior execs who data scientists are workng for - there are courses out there for them\n\n\n\nMichael Allwright: “DATA FOR BAD OR GOOD? THE MINERVA COLLECTIVE”\n\nMichael is Co-Founder and CEO of The Minerva Collective, a not for profit which uses data and data sharing for social good, and runs a data analytics consulting company focussed on using data for good.\n\n\nwe all have tons of data being collected about us, by shops, banks, facebook, health records, etc\nits scary - which is why we have talks about data privacy etc\nsee black mirror episode nosedive and China’s social media score for people\nbut there is lots of data being used for good\n\nin developing countries blindness is being data treated\nits helping cancer researchers detect early stage cancer using machine learning\nneuroscience generates big big data - ML has the potential to deal with it\n\ngetting back to Minerva\n98K kids in the EU go missing every year - the police used telco data to analyze and break up kidnapping rings\nMike studied pure math and did data work in the UK, like modelling building usage and which ones to shut down to save money\nonto corp work at PWC and an identity crisis - which led to travel, and a decision to focus on humans, became a hipster at Bondi beach talking about using data to change soceity… then… drumroll… the data science breakfast meetup was born (looks interesting, should go)\nwanted to do something good, but lots of issues about getting access to data to do stuff with, so joined up with Data republic to form the Minerva Collective. DR provides a secure platform and Minerva gets data and analysts on it to do data for good.\nrun 6 events and many projects, a few examples:\nDomestive violence - effects 1/6 Aus women\n\nran several workshops with experts and groups.\nlearned perps want to change their behaviour but there are long waiting lists, so is there an opportunity to intervence earlier?\nUni of Melbourne is doing a lot of work here on collecting data, tracking ppl etc\nlearned can look at biometric data, telco and bank data of perps using machine learning to predict interventions, also provide awarness etc\n\nFinancial Stress - 64% adults face financial stress in AU\n\ndeveloped a getting by index of financial stress hotspots\nlooking at bank data to build real time models to develop risk scores for a number of risk factors and use this for early interventions\n\nSolar power in Africa - ppl have more cellphones than power\n\nthere is lots of data, using this to predict best churn/agents/solutions etc\n\nand there is a lot more stuff they are doing\nthink about: what is my mission in data science?\n\nq&a:\n\nwhat tools to use?\n\nmost important is the ability to frame the problem and deliver a simple solution visually.\n\ndomestic violence data sources are very private, how did they get?\n\nthis was done as part of a NSW initiative where they incentivzed preps to opt-in to share their data\n\nDV is often under-reported, so is the data reliable?\n\nthe Uni of Melbourne was tracking perps\n\nhow do you go from proof of concept to actually ruling out?\n\nbiggest challange is this, to move on from concepts and hackathons to something operational.\nhas gotten a lot of traction in the consulting side\n\ntake on AI ethics\n\nnot sure, but there is a broad spectrum - from WMD’s to targetting ppl to help them\n\n\n\n\nInna Kolyshkina: “USING DATA SCIENCE METHODS FOR PREDICTIVE ASSET MANAGEMENT FOR A LARGE AUSTRALIAN UTILITIES COMPANY. USE CASE.”\n\nInna Kolyshkina is Director of Data Science in Analytikk Consulting Services, a Data Analytics consultancy.\n\n\nThis talk describes a recent project delivered to a large Australian utilities company [..] it enabled the organisation to proactively identify problematic or costly assets prior to asset failure and to use predicted future costs to inform annual maintenance planning.\n\n\nfounded IAPA, first data scientist at a big4 in Australia\nPredictive Asset Management analytics - asset owning companies want to proactively optimize maintainence planning and prevent failures\nin the past, client used common sense - this doesn’t work to well. Of the shelf solutions developed for other countries didn’t work too well either, and regulators wanted better/local ones (company had to justify funding to regulators)\nhad 12 years of data, which they wanted to use to forecast the next 10 years of asset maintainence cost, failure occurence and duration for several types of assets\nsolution had to be transparent and auditable, easy to implement and compliant with regulation\nData was huge and untidy - this is normal. millions of records, 500 variables, different types\n\nonly 15 years of data and they wanted 10 years of prediction\nit was sparse, high levels of random variation, lots of zeros, and so on\n\nconvential view was a bathtub curve - new asset increases reliability, decreases cost, enters mature phase then cost and failures go up with age. this was wrong\nafter initial data eval, had to go back to tell them data wasn’t enough, added more data like climate etc\nused standard data science models like decision trees, boosting, random forests (more) to data science the data. three models were fit for each data type\nfinally, expressed models as decision treer, combined with classic techniques\n\nq&a\n\nhow to optimize model?\n\nfirst did feature selection - so already knew all the important factors / significant predictors - looked at deviance, r-squared, lasso regression, then fit a GLM model\n\nhow did you get all these data sciencey things across to the old school company?\nkey was that clients were engineers, they could understand.\nHave to build trust and gain respect - did a proper requirements collection, understood what they had already done, explained where else the data science techniques worked\nbuild a proof of concept, walked them through it. had several meetings around the proof of concept\nobjective measures - how did the DS forecasts compare to their engineering judgements?\nasked stakeholders - when will you believe the model resutls?\nregulators hire econmetricians who just use linear models, don’t understand data science - so the final linear model makes sense to them\ntested model on data they hadn’t seen\nmain interest of the client was to find the most expensive assets\nenergy space is tough as regulator is more innumerate than the models - how did they percieve this?\n\nthey used data science methods for feature selection and then choose a linear model - so it was understandable\nclient played regulator with the consultant and asked difficult questions, etc\n\ndata enrichment - how did they know what/which more to get?\n\nasked the client what other data would be useful/relevant to the question - they already knew what could be relevant\n\ndata cleanup\n\naudited the data, got business rules, basic checks\nthe zero data - e.g some assets have very low failure rates so there isn’t much failure data - used special stats techniques there\n\nwhat would they say to the regulator?\n\nsend them to a data literacy course\ntalk about whats happening in the world, mainstream methods data scienctists are using, stats reference\n\nwhat did you spend the most time on?\n\ndata audit, checks and cleaning\n\ndid you consider other approaches like interpretative model?\n\nregulators hire consultants, who are generally econometrician, who accept linear models, otherwise they would have used decision trees or something like that"
  },
  {
    "objectID": "posts/writing-about-code/setup-crostini-chromebook.html",
    "href": "posts/writing-about-code/setup-crostini-chromebook.html",
    "title": "Setting up Linux on a Chromebook with Crostini",
    "section": "",
    "text": "ChromeOS supports a built in Linux - running inside a container running insde a VM. Turn it on by going to settings and pressing the on button for Linux (beta). This gives us a bare bones Linux install running Debian 11. So here goes all the things I did to customize it.\nMost of these tips are from the reddit crostini wiki, I’ve put all the ones I’m using in one page for a handy reference.\nBy default, the linux user has no password. Some installations require you to enter a password so xix this by switching to root sudo su and then running passwd username.\n\n\n\n\nthe debian repos often have older packages, so for really new versions use flatpak.\nInstall flatpak: sudo apt install flatpak\nAdd the flatpak repo:\nsudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\nInstall apps by going to the app page on flathub and copying the install line at the bottom - for example to install Uberwriter its:\nsudo flatpak install flathub de.wolfvollprecht.UberWriter\nNote: you need sudo to install and update flatpak apps in crostini.\n\n\n\nAnaconda can automatically install VS Code, but to install it directly:\ncurl -L https://go.microsoft.com/fwlink/?LinkID=760868 &gt; vscode.deb\nsudo apt install ./vscode.deb\nUseful plugins:\n\nMarkdown All in One\nPython\n\n\n\n\nCurrently I’m using Caret - a chromeos text editor app which runs lightening fast.\nvs code can handle markdown nicely, but I find it too slow on Crostini. I prefer Typora.io or Caret.io for markdown and vs code is for coding. BUT all these apps are based on electron, and currently run slow on Crostini, and will continue being slow until Crostini gets GPU accleration.\nInstall Caret by downloading the latest beta release .deb and:\nsudo apt install ./caret-beta.deb\nInstall Typora - haven’t tested this out, especially adding repo. Apparently you have to sudo apt install software-properties-common first before adding a repo.\n# add Typora's repository\nsudo add-apt-repository 'deb https://typora.io/linux ./'\nsudo apt update\n\n# install typora\nsudo apt install typora\n\n\n\n\nNote: install miniconda instead to save space if needed.\ngo to the Anaconda linux download page and copy the url, then download it:\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh - check for update url here.\nInstall by bash Anaconda3-2020.11-Linux-x86_64.sh - change filename to whatever the downloaded file name is.\n\n\nWorks out of the box with Anaconda. If using miniconda, install by\nconda install -c conda-forge jupyterlab\nOther useful stuff for Jupyter:\nTo install extensions, first install nodejs:\nStep 1: install nvm, a script to install nodejs\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.37.2/install.sh | bash\nstep 2: install nodejs using volta:\n# install Volta\ncurl https://get.volta.sh | bash\n\n# install Node\nvolta install node\n\n# start using Node\nnode\nSome seful jupyterlab extensions:\nSee https://github.com/mauhai/awesome-jupyterlab\n\nshow a toc from markdown headings: jupyter labextension install @jupyterlab/toc\n\ntodo: try out nteract - a react based desktop front end for jupyter.\n\n\n\nSometimes you need R. So here goes:\nconda create -n R r-essentials r-base\nPresto! now R should show up as a kernel in jupyter.\nIntall r packages inside the r environment by using the conda r channel - note that many packages seems to be prefaced with r-:\nconda install -c r r-plotly\n\n\n\n\nthis is really important, cause if the terminal doesn’t look like something out of a movie, are you really doing something?\nI’m using Tilix, install by\nsudo apt install tilix\nthough as of ChromeOS 77 the built in terminal is pretty good, so no need for this.\n\n\nFirst up, the native terminal gobbles up some of my fav chromeos shortcuts, namely:\n\nMaximize Alt + =\nMinimize Alt + -\nDock window left/right Alt + [, Alt + ]\nClose window Ctrl+Shift+W - this is actually Ctrl-W normally, but terminal might need that hence shift.\n\nfix this by pressing ctrl-shift-p inside terminal, then scroll down to keyboard preferences and enter\n{\n  \"Alt-187\": \"PASS\",\n  \"Alt-189\": \"PASS\",\n  \"Alt-219\": \"PASS\",\n  \"Alt-221\": \"PASS\",\n  \"Ctrl-Shift-W\": \"PASS\"\n}\nThis has to be done only once, since ChromeOS remembers the terminal settings across devices. So handy after a powerwash.\n\n\n\nIf using Tilix, no need to do this, but for the native terminal, install tmux:\nsudo apt install tmux\nthe only thing I really do with tmux is to split the terminal horizontally, then splitting one horizontal terminal vertically, for a total of three windows. Now there is a lot more about sessions and whats not, but the bare basics are:\nstart tmux by typing tmux, then press ctrl+b to enter command mode. \" splits the window horizontally and % splits it vertically. To move around, press ctrl-b arrow-key\nFor more customizatoin, make a .tmux.conf in the home directory and add:\n# Enable mouse mode (tmux 2.1 and above)\nset -g mouse on\n\n\n\nrumour has it that crostini can have a heart attack if you change the shell, so stick with bash. Consider oh-my-bash or bash-it for hacker level coding.\nalso install powerline-fonts and select a powerline font for the terminal.\nsudo apt install fonts-powerline\nUseful shell tools:\ntldr shows a short and useful help page for commands, e.g type tldr curl to get a synopsis of how to use curl.\n\nPreferred install: npm install -g tldr\nIf node not installed: pip install tldr\n\nbat a replacement for cat, displays files with syntax highlighting in the terminal. Right now as rendering problems in crostini, but hopefully will improve. Install by downloading the .deb and sudo apt install ./bat_file.deb.\n\n\n\ninstall plugin manager for vim - I went with vim-plug:\nedit ~/.vimrc so it has this stuff:\n\" install vim-plug if not installed\nif empty(glob('~/.vim/autoload/plug.vim'))\n  silent !curl -fLo ~/.vim/autoload/plug.vim --create-dirs\n    \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n  autocmd VimEnter * PlugInstall --sync | source $MYVIMRC\nendif\n\n\" plugin directory, don't use standard Vim names like 'plugin'\ncall plug#begin('~/.vim/plugged')\n\n\" plugins go here\n\" ---------------\n\" highlights code problems: https://github.com/w0rp/ale\nPlug 'w0rp/ale'\n\" adds a status bar: https://github.com/itchyny/lightline.vim\nPlug 'itchyny/lightline.vim'\n\" distraction free writing: https://github.com/junegunn/goyo.vim\nPlug 'junegunn/goyo.vim'\n\" for python auto completion: https://github.com/davidhalter/jedi-vim\nPlug 'davidhalter/jedi-vim'\n\n\" Initialize plugin system\ncall plug#end()\n\n\" don't break mid word\nset linebreak\nrun :PlugInstall in vim to install plugs (if needed).\n\n\n\n\nThis command will grab json output of the first (or last?) 200 repos in my github and git clone them all one by one into the directory this command was run.\ncurl -s https://api.github.com/users/khalido/repos?per_page=200 | grep \\\"clone_url\\\" | awk '{print $2}' | sed -e 's/\"//g' -e 's/,//g' | xargs -n1 git clone"
  },
  {
    "objectID": "posts/writing-about-code/setup-crostini-chromebook.html#install-all-the-apps",
    "href": "posts/writing-about-code/setup-crostini-chromebook.html#install-all-the-apps",
    "title": "Setting up Linux on a Chromebook with Crostini",
    "section": "",
    "text": "the debian repos often have older packages, so for really new versions use flatpak.\nInstall flatpak: sudo apt install flatpak\nAdd the flatpak repo:\nsudo flatpak remote-add --if-not-exists flathub https://flathub.org/repo/flathub.flatpakrepo\nInstall apps by going to the app page on flathub and copying the install line at the bottom - for example to install Uberwriter its:\nsudo flatpak install flathub de.wolfvollprecht.UberWriter\nNote: you need sudo to install and update flatpak apps in crostini.\n\n\n\nAnaconda can automatically install VS Code, but to install it directly:\ncurl -L https://go.microsoft.com/fwlink/?LinkID=760868 &gt; vscode.deb\nsudo apt install ./vscode.deb\nUseful plugins:\n\nMarkdown All in One\nPython\n\n\n\n\nCurrently I’m using Caret - a chromeos text editor app which runs lightening fast.\nvs code can handle markdown nicely, but I find it too slow on Crostini. I prefer Typora.io or Caret.io for markdown and vs code is for coding. BUT all these apps are based on electron, and currently run slow on Crostini, and will continue being slow until Crostini gets GPU accleration.\nInstall Caret by downloading the latest beta release .deb and:\nsudo apt install ./caret-beta.deb\nInstall Typora - haven’t tested this out, especially adding repo. Apparently you have to sudo apt install software-properties-common first before adding a repo.\n# add Typora's repository\nsudo add-apt-repository 'deb https://typora.io/linux ./'\nsudo apt update\n\n# install typora\nsudo apt install typora"
  },
  {
    "objectID": "posts/writing-about-code/setup-crostini-chromebook.html#install-anaconda-for-a-better-python",
    "href": "posts/writing-about-code/setup-crostini-chromebook.html#install-anaconda-for-a-better-python",
    "title": "Setting up Linux on a Chromebook with Crostini",
    "section": "",
    "text": "Note: install miniconda instead to save space if needed.\ngo to the Anaconda linux download page and copy the url, then download it:\ncurl -O https://repo.anaconda.com/archive/Anaconda3-2020.11-Linux-x86_64.sh - check for update url here.\nInstall by bash Anaconda3-2020.11-Linux-x86_64.sh - change filename to whatever the downloaded file name is.\n\n\nWorks out of the box with Anaconda. If using miniconda, install by\nconda install -c conda-forge jupyterlab\nOther useful stuff for Jupyter:\nTo install extensions, first install nodejs:\nStep 1: install nvm, a script to install nodejs\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.37.2/install.sh | bash\nstep 2: install nodejs using volta:\n# install Volta\ncurl https://get.volta.sh | bash\n\n# install Node\nvolta install node\n\n# start using Node\nnode\nSome seful jupyterlab extensions:\nSee https://github.com/mauhai/awesome-jupyterlab\n\nshow a toc from markdown headings: jupyter labextension install @jupyterlab/toc\n\ntodo: try out nteract - a react based desktop front end for jupyter.\n\n\n\nSometimes you need R. So here goes:\nconda create -n R r-essentials r-base\nPresto! now R should show up as a kernel in jupyter.\nIntall r packages inside the r environment by using the conda r channel - note that many packages seems to be prefaced with r-:\nconda install -c r r-plotly"
  },
  {
    "objectID": "posts/writing-about-code/setup-crostini-chromebook.html#make-the-terminal-nicer-to-use",
    "href": "posts/writing-about-code/setup-crostini-chromebook.html#make-the-terminal-nicer-to-use",
    "title": "Setting up Linux on a Chromebook with Crostini",
    "section": "",
    "text": "this is really important, cause if the terminal doesn’t look like something out of a movie, are you really doing something?\nI’m using Tilix, install by\nsudo apt install tilix\nthough as of ChromeOS 77 the built in terminal is pretty good, so no need for this.\n\n\nFirst up, the native terminal gobbles up some of my fav chromeos shortcuts, namely:\n\nMaximize Alt + =\nMinimize Alt + -\nDock window left/right Alt + [, Alt + ]\nClose window Ctrl+Shift+W - this is actually Ctrl-W normally, but terminal might need that hence shift.\n\nfix this by pressing ctrl-shift-p inside terminal, then scroll down to keyboard preferences and enter\n{\n  \"Alt-187\": \"PASS\",\n  \"Alt-189\": \"PASS\",\n  \"Alt-219\": \"PASS\",\n  \"Alt-221\": \"PASS\",\n  \"Ctrl-Shift-W\": \"PASS\"\n}\nThis has to be done only once, since ChromeOS remembers the terminal settings across devices. So handy after a powerwash.\n\n\n\nIf using Tilix, no need to do this, but for the native terminal, install tmux:\nsudo apt install tmux\nthe only thing I really do with tmux is to split the terminal horizontally, then splitting one horizontal terminal vertically, for a total of three windows. Now there is a lot more about sessions and whats not, but the bare basics are:\nstart tmux by typing tmux, then press ctrl+b to enter command mode. \" splits the window horizontally and % splits it vertically. To move around, press ctrl-b arrow-key\nFor more customizatoin, make a .tmux.conf in the home directory and add:\n# Enable mouse mode (tmux 2.1 and above)\nset -g mouse on\n\n\n\nrumour has it that crostini can have a heart attack if you change the shell, so stick with bash. Consider oh-my-bash or bash-it for hacker level coding.\nalso install powerline-fonts and select a powerline font for the terminal.\nsudo apt install fonts-powerline\nUseful shell tools:\ntldr shows a short and useful help page for commands, e.g type tldr curl to get a synopsis of how to use curl.\n\nPreferred install: npm install -g tldr\nIf node not installed: pip install tldr\n\nbat a replacement for cat, displays files with syntax highlighting in the terminal. Right now as rendering problems in crostini, but hopefully will improve. Install by downloading the .deb and sudo apt install ./bat_file.deb.\n\n\n\ninstall plugin manager for vim - I went with vim-plug:\nedit ~/.vimrc so it has this stuff:\n\" install vim-plug if not installed\nif empty(glob('~/.vim/autoload/plug.vim'))\n  silent !curl -fLo ~/.vim/autoload/plug.vim --create-dirs\n    \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\n  autocmd VimEnter * PlugInstall --sync | source $MYVIMRC\nendif\n\n\" plugin directory, don't use standard Vim names like 'plugin'\ncall plug#begin('~/.vim/plugged')\n\n\" plugins go here\n\" ---------------\n\" highlights code problems: https://github.com/w0rp/ale\nPlug 'w0rp/ale'\n\" adds a status bar: https://github.com/itchyny/lightline.vim\nPlug 'itchyny/lightline.vim'\n\" distraction free writing: https://github.com/junegunn/goyo.vim\nPlug 'junegunn/goyo.vim'\n\" for python auto completion: https://github.com/davidhalter/jedi-vim\nPlug 'davidhalter/jedi-vim'\n\n\" Initialize plugin system\ncall plug#end()\n\n\" don't break mid word\nset linebreak\nrun :PlugInstall in vim to install plugs (if needed)."
  },
  {
    "objectID": "posts/writing-about-code/setup-crostini-chromebook.html#download-all-my-git-repos",
    "href": "posts/writing-about-code/setup-crostini-chromebook.html#download-all-my-git-repos",
    "title": "Setting up Linux on a Chromebook with Crostini",
    "section": "",
    "text": "This command will grab json output of the first (or last?) 200 repos in my github and git clone them all one by one into the directory this command was run.\ncurl -s https://api.github.com/users/khalido/repos?per_page=200 | grep \\\"clone_url\\\" | awk '{print $2}' | sed -e 's/\"//g' -e 's/,//g' | xargs -n1 git clone"
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html",
    "href": "posts/writing-about-code/building_this_blog.html",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "There are many excellent blog engines out there, but to customise any of them takes so much understanding of how they work, the template and theme engines they use, that its easier to just use them exactly as is with an existing theme.\nI wanted my own custom static blog, which played well with jupyter notebooks and markdown files, as well as a reason to do some python coding, so here goes yet another python blogging engine.\nThis post documents the process of building this blog. The main goal is to put markdown and jupyter notebooks in a folder, and build a static site which gets autoupdated on github pages or netlify. Just like hugo, jekyll, gatsby and so many others!\n\n\nIts straightforward to read a set of markdown posts and convert to python. I am using python to read the posts with python-markdown to parse them into html, complete with inline syntax highlighting.\nKey tools used:\n\nwrite: markdown docs using any editor and jupyter notebooks having yaml front matter.\n\nobsidian to edit markdown\nvs code for jupyter notebooks. Jupyter lab is ok in a pinch but it causes me more problems than not. My fav cloud alternative is Deepnote.\n\nmake the blog:\n\nnbconvert to parse jupyter to markdown.\ntried nbdev but had too many problems, though it has a lot more blog friendly features.\npython to read all the markdown files using python markdown and yaml.\nfinally, writing html pages for index, tags and posts using mako for python friendly templates.\n\nsearch: fusejs to make a in browser search engine\nbuild site: every time I commit to my blog repo, a github action is triggered which rebuilds the sites and saves the output to a public folder.\nhosting: the site is hosted on the gh-pages of my blog repository, which github pages auto republishes. I am using a github action to deploy output files from the public folder to gh-pages on every commit to the main branch.\nlocal sever: running the script with --serve flag starts a local python server.\n\nBelow are notes for the specifics used.\n\n\nPython markdown\n\nhttps://python-markdown.github.io/extensions/\nhttps://facelessuser.github.io/pymdown-extensions/\n\nStart a server from cli:\npython -m http.server\n\n\n\nI started with nbdev to convert notebooks to markdown, but it slowed down rebuilding the blog a lot, and its pretty complex. So in the end I’ve stuck with nbconvert. Some useful tips:\n\nspecify templates\n\nI need to customize nbconvert so it implements some of the features from fastpages, namely:\n\nrenders output differently\ncollapses code cells if #hide is at the top\n\n\n\n\npython markdown has pygments built in, which has a bunch of styles. To generate the css:\npygmentize -S default -f html -a .codehilite &gt; codestyles.css\nBut on second thought decided to can this and go with highlightjs for now as it speeds up builds and keeps the html clean (at the cost of loading more javascript).\nOne thing to investigate is how to make the output cells of jupyter notebooks blend into the main website. This seems to require some css trickery.\n\n\n\n\nI’m no longer familiar with html, even though I build my first weblog on geocities way back in 1997/8. So we have now reached html5.\n\nwhat to put in the head of a html page\nhttps://htmldom.dev/\n\n\n\n\nCSS is hard. So I want a simple to use framework, ended up looking at:\n\nhttps://tachyons.io/\nhttps://tailwindcss - at first sight it looked horrible, with style mixed in with html, but once I thought about it some more, its beautiful. Everything is there visible in the one file and I hate css files anyways. So leaning towards using this, the only downside being is that you need npm to generate the final production tailwind css file. More to follow once I actually implement it…\nPure.css\nwater.css\nmilligram\nnewcss - awesome, simple, super easy to use - basically just write html and it it makes it look nice and clean. Best for simple things like this blog. Only reason to switch to a more complex css file is cause even simple posts like this need small text and slide-outs for post meta-data like tags and date info etc.\nlit\nconcrete.css - very minimal\nhttps://csslayout.io/ - examples of using css directly\n\ntodo: decide on one.\nThings to implement using css:\n\na floating toc, like so many websites have it these days. I like tocs.\n\n\n\n\nSearch. I want search. This is pretty straightforward, we need a list of content and some javascript to do the searching. Jupyter notebooks mess this up as the converted markdown files have a bunch of js and other cruft.\nTo just search post titles is pretty easy, from direct js to algolias autocomplete library.\nIdeally I want to search across all the content is well, which takes some thinking as the output of jupyter notebooks can be huge, with all kinds of js embedded.\nSome things to look at:\n\nfusejs - blog post implementing this in hugo - used this first. It works and is pretty straightforward but has no python integration and I would like better examples as a js newbie.\nminisearch\nlunr.js as well as lunr.py to pre generate the index.\n\nSo step one is to build a search index - which my script does as a json file containing all the post attributes I want searched.\n\n\nGithub actions add superpowers to a repo - they can be set to be triggered at a time interval or on every code push to a branch. To make a github action: Save a github approved formatted yaml file to .github/workflows folder and it should run on every push. For this blog my actions:\n\ncopys the contents of the repo to the github runner\nsets up python - python versions available on github actions\ninstall dependencies as defined in requirements.txt\n\n\n\n\n\n\n\nThe oEmbed specifies how companies like youtube, twitter provide information about their content.\nCalling youtube like so: https://www.youtube.com/oembed?url=https://youtu.be/48A-7GBxZco returns a json like:\n{\"title\":\"Anthony Hyman Memorial Lecture 2022: The Refugee Crisis and Afghanistan\",\"author_name\":\"SOAS University of London\",\"author_url\":\"https://www.youtube.com/c/SoasAcUk\",\"type\":\"video\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/48A-7GBxZco/hqdefault.jpg\",\"html\":\"\\u003ciframe width=\\u0022200\\u0022 height=\\u0022113\\u0022 src=\\u0022https://www.youtube.com/embed/48A-7GBxZco?feature=oembed\\u0022 frameborder=\\u00220\\u0022 allow=\\u0022accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\u0022 allowfullscreen\\u003e\\u003c/iframe\\u003e\"}\nThe html key is the html code to embed the video.\nTwitter also has an oembed api, called like so:\ncurl --request GET --url 'https://publish.twitter.com/oembed?url=https%3A%2F%2Ftwitter.com%2FInterior%2Fstatus%2F507185938620219395'\nand this too returns a json, with the html key being the embed code.\nSadly all the python wrappers are old and abandoned, so I need a tiny wrapper to do this myself.\n\n\n\nSome unicode fonts have emojis built in, so ways to enable emoji is:\n\nuse a unicode font and investigate python-markdown emoji parser to deal with them properly\nuse a script to replace :smile: with emoji incons - nope, don’t want images scattered all over the shop.\n\nEmoji test:\n\nsmiley face test copy pasted from some website: 😅\nsmiley face written using markdown code :smile:​ :smile:\n\ndang, this works in the markdown editor but not in the browser\n\n\n\n\n\nThis should show a twitter tweet embedded inside the page:\n\n\nSunsets don't get much better than this one over @GrandTetonNPS. #nature #sunset pic.twitter.com/YuKy2rcjyU\n\n— US Department of the Interior (@Interior) May 5, 2014\n\n\n\n\n\nfrom mako.template import Template\nfrom mako.lookup import TemplateLookup\nlookup = TemplateLookup(directories=[\"templates\"])\n\n# make the big picture templates\nfor tmpl in [\"index.html\"]:\n    template = lookup.get_template(tmpl)\n    html = template.render(posts=posts).strip()\n    path = path_publish / tmpl\n    path.write_text(html)\n    print(f\"wrote {tmpl} to {path}\")\n\n# write all the posts\ntemplate = lookup.get_template(\"post.html\")\nfor post in posts:\n    html = template.render(post=post).strip()\n    path = path_publish / f\"{post.slug}.html\"\n    path.write_text(html)\n    print(f\"wrote {post.slug} to {path}\")"
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html#blog-engine",
    "href": "posts/writing-about-code/building_this_blog.html#blog-engine",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "Its straightforward to read a set of markdown posts and convert to python. I am using python to read the posts with python-markdown to parse them into html, complete with inline syntax highlighting.\nKey tools used:\n\nwrite: markdown docs using any editor and jupyter notebooks having yaml front matter.\n\nobsidian to edit markdown\nvs code for jupyter notebooks. Jupyter lab is ok in a pinch but it causes me more problems than not. My fav cloud alternative is Deepnote.\n\nmake the blog:\n\nnbconvert to parse jupyter to markdown.\ntried nbdev but had too many problems, though it has a lot more blog friendly features.\npython to read all the markdown files using python markdown and yaml.\nfinally, writing html pages for index, tags and posts using mako for python friendly templates.\n\nsearch: fusejs to make a in browser search engine\nbuild site: every time I commit to my blog repo, a github action is triggered which rebuilds the sites and saves the output to a public folder.\nhosting: the site is hosted on the gh-pages of my blog repository, which github pages auto republishes. I am using a github action to deploy output files from the public folder to gh-pages on every commit to the main branch.\nlocal sever: running the script with --serve flag starts a local python server.\n\nBelow are notes for the specifics used.\n\n\nPython markdown\n\nhttps://python-markdown.github.io/extensions/\nhttps://facelessuser.github.io/pymdown-extensions/\n\nStart a server from cli:\npython -m http.server\n\n\n\nI started with nbdev to convert notebooks to markdown, but it slowed down rebuilding the blog a lot, and its pretty complex. So in the end I’ve stuck with nbconvert. Some useful tips:\n\nspecify templates\n\nI need to customize nbconvert so it implements some of the features from fastpages, namely:\n\nrenders output differently\ncollapses code cells if #hide is at the top\n\n\n\n\npython markdown has pygments built in, which has a bunch of styles. To generate the css:\npygmentize -S default -f html -a .codehilite &gt; codestyles.css\nBut on second thought decided to can this and go with highlightjs for now as it speeds up builds and keeps the html clean (at the cost of loading more javascript).\nOne thing to investigate is how to make the output cells of jupyter notebooks blend into the main website. This seems to require some css trickery."
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html#html",
    "href": "posts/writing-about-code/building_this_blog.html#html",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "I’m no longer familiar with html, even though I build my first weblog on geocities way back in 1997/8. So we have now reached html5.\n\nwhat to put in the head of a html page\nhttps://htmldom.dev/"
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html#css",
    "href": "posts/writing-about-code/building_this_blog.html#css",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "CSS is hard. So I want a simple to use framework, ended up looking at:\n\nhttps://tachyons.io/\nhttps://tailwindcss - at first sight it looked horrible, with style mixed in with html, but once I thought about it some more, its beautiful. Everything is there visible in the one file and I hate css files anyways. So leaning towards using this, the only downside being is that you need npm to generate the final production tailwind css file. More to follow once I actually implement it…\nPure.css\nwater.css\nmilligram\nnewcss - awesome, simple, super easy to use - basically just write html and it it makes it look nice and clean. Best for simple things like this blog. Only reason to switch to a more complex css file is cause even simple posts like this need small text and slide-outs for post meta-data like tags and date info etc.\nlit\nconcrete.css - very minimal\nhttps://csslayout.io/ - examples of using css directly\n\ntodo: decide on one.\nThings to implement using css:\n\na floating toc, like so many websites have it these days. I like tocs."
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html#search",
    "href": "posts/writing-about-code/building_this_blog.html#search",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "Search. I want search. This is pretty straightforward, we need a list of content and some javascript to do the searching. Jupyter notebooks mess this up as the converted markdown files have a bunch of js and other cruft.\nTo just search post titles is pretty easy, from direct js to algolias autocomplete library.\nIdeally I want to search across all the content is well, which takes some thinking as the output of jupyter notebooks can be huge, with all kinds of js embedded.\nSome things to look at:\n\nfusejs - blog post implementing this in hugo - used this first. It works and is pretty straightforward but has no python integration and I would like better examples as a js newbie.\nminisearch\nlunr.js as well as lunr.py to pre generate the index.\n\nSo step one is to build a search index - which my script does as a json file containing all the post attributes I want searched.\n\n\nGithub actions add superpowers to a repo - they can be set to be triggered at a time interval or on every code push to a branch. To make a github action: Save a github approved formatted yaml file to .github/workflows folder and it should run on every push. For this blog my actions:\n\ncopys the contents of the repo to the github runner\nsets up python - python versions available on github actions\ninstall dependencies as defined in requirements.txt"
  },
  {
    "objectID": "posts/writing-about-code/building_this_blog.html#misc-stuff",
    "href": "posts/writing-about-code/building_this_blog.html#misc-stuff",
    "title": "how the blog was built, python edition",
    "section": "",
    "text": "The oEmbed specifies how companies like youtube, twitter provide information about their content.\nCalling youtube like so: https://www.youtube.com/oembed?url=https://youtu.be/48A-7GBxZco returns a json like:\n{\"title\":\"Anthony Hyman Memorial Lecture 2022: The Refugee Crisis and Afghanistan\",\"author_name\":\"SOAS University of London\",\"author_url\":\"https://www.youtube.com/c/SoasAcUk\",\"type\":\"video\",\"height\":113,\"width\":200,\"version\":\"1.0\",\"provider_name\":\"YouTube\",\"provider_url\":\"https://www.youtube.com/\",\"thumbnail_height\":360,\"thumbnail_width\":480,\"thumbnail_url\":\"https://i.ytimg.com/vi/48A-7GBxZco/hqdefault.jpg\",\"html\":\"\\u003ciframe width=\\u0022200\\u0022 height=\\u0022113\\u0022 src=\\u0022https://www.youtube.com/embed/48A-7GBxZco?feature=oembed\\u0022 frameborder=\\u00220\\u0022 allow=\\u0022accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\\u0022 allowfullscreen\\u003e\\u003c/iframe\\u003e\"}\nThe html key is the html code to embed the video.\nTwitter also has an oembed api, called like so:\ncurl --request GET --url 'https://publish.twitter.com/oembed?url=https%3A%2F%2Ftwitter.com%2FInterior%2Fstatus%2F507185938620219395'\nand this too returns a json, with the html key being the embed code.\nSadly all the python wrappers are old and abandoned, so I need a tiny wrapper to do this myself.\n\n\n\nSome unicode fonts have emojis built in, so ways to enable emoji is:\n\nuse a unicode font and investigate python-markdown emoji parser to deal with them properly\nuse a script to replace :smile: with emoji incons - nope, don’t want images scattered all over the shop.\n\nEmoji test:\n\nsmiley face test copy pasted from some website: 😅\nsmiley face written using markdown code :smile:​ :smile:\n\ndang, this works in the markdown editor but not in the browser\n\n\n\n\n\nThis should show a twitter tweet embedded inside the page:\n\n\nSunsets don't get much better than this one over @GrandTetonNPS. #nature #sunset pic.twitter.com/YuKy2rcjyU\n\n— US Department of the Interior (@Interior) May 5, 2014\n\n\n\n\n\nfrom mako.template import Template\nfrom mako.lookup import TemplateLookup\nlookup = TemplateLookup(directories=[\"templates\"])\n\n# make the big picture templates\nfor tmpl in [\"index.html\"]:\n    template = lookup.get_template(tmpl)\n    html = template.render(posts=posts).strip()\n    path = path_publish / tmpl\n    path.write_text(html)\n    print(f\"wrote {tmpl} to {path}\")\n\n# write all the posts\ntemplate = lookup.get_template(\"post.html\")\nfor post in posts:\n    html = template.render(post=post).strip()\n    path = path_publish / f\"{post.slug}.html\"\n    path.write_text(html)\n    print(f\"wrote {post.slug} to {path}\")"
  },
  {
    "objectID": "posts/writing-about-code/esp32-eink.html",
    "href": "posts/writing-about-code/esp32-eink.html",
    "title": "Lilygo ttgo t5",
    "section": "",
    "text": "TTGO T5 V2.3 Wireless WiFi Basic Wireless Module ESP-32 esp32 2.13 Inch ePaper Display Development board.\n\nSKU: H239, github\nModel DEPG0213BN\nESP32-D0WDQ6 (revision 1)\nWiFi, BT, Dual Core, 240MHz, Crystal is 40MHz\n\nesptool.py --port /dev/ttyUSB1 chip_id tells us:\nChip is ESP32-D0WDQ6 (revision 1)\nFeatures: WiFi, BT, Dual Core, 240MHz\nI downloaded the latest stable generic micropython ESP-IDF v4.x firmware. Flash it by: (assuming board is on ttyUSB1)\nesptool.py --chip esp32 --port /dev/ttyUSB1 erase_flash\nesptool.py --chip esp32 --port /dev/ttyUSB1 --baud 460800 write_flash -z 0x1000 esp32-idf4-20200902-v1.13.bin\nSo now micropython is running on the board.\nAn easier way to do the above is to use Mu, which has a flasher built it.\n\n\n\nMu-editor has a built in file manager but it wasn’t working on my board, so I went with ampy.\nInstall that by: pip install adafruit-ampy then find the port the board is using.\nOn a mac this lists all the serial ports in use: ls -l /dev/tty.*\nOn my machine I got /dev/tty.usbserial-1420, so ampy basics is:\n\nlist files: ampy --port /dev/cu.usbserial-1420 ls\nput a file on the board: ampy --port /dev/cu.usbserial-1420 put config.py\nget a file: ampy --port /dev/cu.usbserial-1420 get config.py\ndelete: ampy --port /dev/cu.usbserial-1420 rm config.py\n\n\n\n\nThis board comes with a 2.13 inch eink display.\ntodo: make it work!\n\n\n\n\nhttps://www.re-innovation.co.uk/docs/ttgo-e-paper-display/\nhttps://github.com/lewisxhe/TTGO-EPaper-Series\nhttps://github.com/peterhinch/micropython-nano-gui\nhttps://github.com/Xinyuan-LilyGO/LilyGo-eink-v2.3-micropython\nhttps://github.com/vroland/epdiy - drivers for eink screens\n\nMicropython executes boot.py on startup, then runs main.py if found. So my code should go inside a main.py file. Mu should be able to see and write to the board, but its grayed out.\ntodo: find a easy solution to write files to the board."
  },
  {
    "objectID": "posts/writing-about-code/esp32-eink.html#the-board",
    "href": "posts/writing-about-code/esp32-eink.html#the-board",
    "title": "Lilygo ttgo t5",
    "section": "",
    "text": "TTGO T5 V2.3 Wireless WiFi Basic Wireless Module ESP-32 esp32 2.13 Inch ePaper Display Development board.\n\nSKU: H239, github\nModel DEPG0213BN\nESP32-D0WDQ6 (revision 1)\nWiFi, BT, Dual Core, 240MHz, Crystal is 40MHz\n\nesptool.py --port /dev/ttyUSB1 chip_id tells us:\nChip is ESP32-D0WDQ6 (revision 1)\nFeatures: WiFi, BT, Dual Core, 240MHz\nI downloaded the latest stable generic micropython ESP-IDF v4.x firmware. Flash it by: (assuming board is on ttyUSB1)\nesptool.py --chip esp32 --port /dev/ttyUSB1 erase_flash\nesptool.py --chip esp32 --port /dev/ttyUSB1 --baud 460800 write_flash -z 0x1000 esp32-idf4-20200902-v1.13.bin\nSo now micropython is running on the board.\nAn easier way to do the above is to use Mu, which has a flasher built it."
  },
  {
    "objectID": "posts/writing-about-code/esp32-eink.html#copy-files",
    "href": "posts/writing-about-code/esp32-eink.html#copy-files",
    "title": "Lilygo ttgo t5",
    "section": "",
    "text": "Mu-editor has a built in file manager but it wasn’t working on my board, so I went with ampy.\nInstall that by: pip install adafruit-ampy then find the port the board is using.\nOn a mac this lists all the serial ports in use: ls -l /dev/tty.*\nOn my machine I got /dev/tty.usbserial-1420, so ampy basics is:\n\nlist files: ampy --port /dev/cu.usbserial-1420 ls\nput a file on the board: ampy --port /dev/cu.usbserial-1420 put config.py\nget a file: ampy --port /dev/cu.usbserial-1420 get config.py\ndelete: ampy --port /dev/cu.usbserial-1420 rm config.py"
  },
  {
    "objectID": "posts/writing-about-code/esp32-eink.html#the-display",
    "href": "posts/writing-about-code/esp32-eink.html#the-display",
    "title": "Lilygo ttgo t5",
    "section": "",
    "text": "This board comes with a 2.13 inch eink display.\ntodo: make it work!"
  },
  {
    "objectID": "posts/writing-about-code/esp32-eink.html#resources",
    "href": "posts/writing-about-code/esp32-eink.html#resources",
    "title": "Lilygo ttgo t5",
    "section": "",
    "text": "https://www.re-innovation.co.uk/docs/ttgo-e-paper-display/\nhttps://github.com/lewisxhe/TTGO-EPaper-Series\nhttps://github.com/peterhinch/micropython-nano-gui\nhttps://github.com/Xinyuan-LilyGO/LilyGo-eink-v2.3-micropython\nhttps://github.com/vroland/epdiy - drivers for eink screens\n\nMicropython executes boot.py on startup, then runs main.py if found. So my code should go inside a main.py file. Mu should be able to see and write to the board, but its grayed out.\ntodo: find a easy solution to write files to the board."
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "I wanted to learn how to setup a simple interactive website, and react is so 2019 and verbose, so I looked around and svelte looked awesome, especially the tutorial which is a work of art.\nSvelte is a JS framework which puts html, css and javascript inside a .svelte file. It aims to stick mostly to basic html/css/js, so is easier to use than the more complicated frameworks like react which essentially are their own language. A good intro video.\nEach .svelte file can be thought of as a component, and you use them like lego to build an app. A very basic svelte file is just js, html and css - try it out in the svelte repl. A sample component (i.e a svelte file to be reused elsewhere) looks like:\n&lt;!-- saved as nested.svelte --&gt;\n&lt;script&gt;\n    export let a_variable = \"you can pass in your own value!\";\n&lt;/script&gt;\n\n&lt;h2&gt;nested h2&lt;/h2&gt;\n\n&lt;p&gt;This is a 1em paragraph, orange in nature, with this passed in beauty: &lt;strong&gt;{a_variable}&lt;/strong&gt;.&lt;/p&gt;\n\n&lt;style&gt;\n  p {color: orange; font-size: 1em;}\n    h2 {color: teal; font-size:1.5em;}\n&lt;/style&gt;\nThe main entry point would be called App.svelte and will have javascript, html and css like so:\n&lt;!-- svelte is mostly 100% javascript --&gt;\n&lt;script&gt;\n    import Nested from './nested.svelte';\n  let name = \"KO\";\n  let img = \"https://media.giphy.com/media/xT8qBsOjMOcdeGJIU8/giphy.gif?cid=ecf05e47oh5ia150hx4lkfrlccxgcucgqpxksgkroeru7x4p&rid=giphy.gif&ct=g\"\n&lt;/script&gt;\n\n&lt;!-- content goes here, curly brackets are javascript --&gt;\n&lt;h1&gt;Hello {name.toUpperCase()}!&lt;/h1&gt;\n&lt;p&gt;we be coding&lt;/p&gt;\n\n&lt;!-- import and use a component --&gt;\n&lt;Nested/&gt;\n\n&lt;!-- we can pass in values to the component, even computed ones --&gt;\n&lt;Nested a_variable={21*2}/&gt; \n\n&lt;img src={img} alt=\"alt text\"&gt;\n\n&lt;!-- css goes here, scoped only to this file --&gt;\n&lt;style&gt;\n  h1 {color: purple}\n&lt;/style&gt;\nWhile it’s possible to build an entire app inside a single .svelte file, we’re better off with making multiple components each doing one thing and import them as needed, as in the example above.\nThats the basics of svelte! Its actually pretty straightforward. For simple apps, this is all you need, but for more complex apps you need a router, which is where sveltekit comes in.\n\n\nSveltekit is a framework for svelte which transforms svelte files into a static or interactive web app.\n\n\nSveltekit uses a file based router, which is fancy speak for saying it looks at the folder src/routes and turns every svelte file into a page.  The entry point of the app is src/routes/index.svelte.\nSo, src/routes/about.svelte becomes the /about page, or alternatively, if the about page refers to other files and stuff, we can put all in a sub directory: src/routes/about/index.svelte to keep them contained.\n\n\n\ni.e shared components, like a header visible on each page. This file applies to ever page: src/routes/__layout.svelte. Layouts can be nested, so subfolders can contain a __layout.svelte file which only applies to the pages in that subfolder.\nPut __layout.reset.svelte in any subfolder where you don’t want the pages to inherit layouts.\n\n\n\nSveltekit sites can run on node server, or be pre-rendered to to run on any web server as static web pages. The following two adapters seem production ready:\n\nadaptor-static - static site, suitable for github pages or anywhere really\nadapter-vercel - dynamic server rendering, so you can have functions which run on the server returning fresh data or something.\n\nThese two are listed as experimental:\n\nadapter-netlify - dynamic server renering\nadapter-cloudflare-workers - suitable for sites needing dynamic server rendering\n\n\n\n\n\nCss has become so complex that using it directly is akin to programming in C. So there are a ton of css frameworks which do the heavy lifting. But all of them require separating the content (html) from design (css), and going back and forth can be a pain.\nTailwind provides a ton of helper styles to use directly inside html. This makes it easier to just modify things directly all in one file. This made sense to me with svelte as that puts content, code and style all in the one svelte file anyways.\nTo see more about tailwindcss, see this earlier blog post.\n\n\n\nThis is the complex part. So I’m laying it out here step by step for my future self.\nTo see this in action, here’s the github repo for this starter project and the final, super simple output site.\n\ninstall node using brew (mac) or nvm (cross platform, probably better) or fnm.\ninstall VS code and these extensions:\n\nsvelte vs code\ntailwindcss\n\n\nNote: tailwindcss wasn’t auto-completing so I had to update this vscode setting:\n\"editor.quickSuggestions\": {\n  \"strings\": true\n}\n\n\nSee sveltekit docs and svelte-add for more.\nnpm init svelte@next myapp\ncd myappp\nnpx svelte-add tailwindcss --jit\nnpm install\nThe install script asks for options, I choose:\n\nTemplate: Skeleton project\nUse Typescript: No\nUse ESlint: Yes\nAdd Prettier: Yes\n\nThis will have installed and configured sveltekit and tailwindcss. To run:\nnpm run dev -- --open\nAdditionally, add the --host flag to expose the site to the local network, handy for testing it on your phone and other computers. If on a laptop, since the page reloads automagically on change, you can prop up an ipad or something with the output open.\nOptionally, add the tailwind typography plugin:\nnpm install -D @tailwindcss/typography\nand update the plugins section in tailwind.config.js to include this:\nplugins: [\n        require('@tailwindcss/typography'),\n    ],\nThen start the site: npm run dev -- --open. Presto! A svelte site powered by sveltekit and tailwindcss should be up and running. Go to src/routes and open index.html and copy paste this below whats already there:\n&lt;article class=\"prose px-8 py-4 bg-gray-200\"&gt;\n    &lt;h1&gt;Welcome to SvelteKit&lt;/h1&gt;\n    &lt;p&gt;Visit &lt;a href=\"https://kit.svelte.dev\"&gt;kit.svelte.dev&lt;/a&gt; to read the documentation&lt;/p&gt;\n&lt;/article&gt;\nIf the above chunk is styled, and updates in the browser preview, then everything is setup fine and working.\n\n\n\n\nTime to setup an adaptor to publish this to the web. Initially I tried adaptor-static to deploy to github pages, but kept running into errors. Now the simplest thing is to build the static site locally, and use github pages to deploy the already built site, but this requires a working local dev env for any changes and thats so last year.\nSo I went with the vercel adaptor - this worked pretty painlessly out of the box, since essentially there is no local build step - you just run a local dev server: npm run dev -- --open to develop, and push to github once done.\n\n\nThe exciting part about using vercel to deploy is how easy it is:\n\nVercel detects the following frontend frameworks automatically and chooses the best default settings for you.\n\nInstall the adaptor:\nnpm install -D @sveltejs/adapter-vercel@next\nUpdate svelte.config.js so it has the vercel adapter line:\n    kit: {\n    adapter: vercel(),\n    }\nThen go to vercel, start a new project and point it to the repo for this starter project. The default settings should just work - it should build and publish the site, and auto-rebuild and publish on every git push.\nThe final output: https://sveltekit-starter-lemon.vercel.app.\n\n\n\n\n\nstorybookjs - create components\ndaisyui - components for tailwind\nSvelte summit 2021"
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html#sveltekit",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html#sveltekit",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "Sveltekit is a framework for svelte which transforms svelte files into a static or interactive web app.\n\n\nSveltekit uses a file based router, which is fancy speak for saying it looks at the folder src/routes and turns every svelte file into a page.  The entry point of the app is src/routes/index.svelte.\nSo, src/routes/about.svelte becomes the /about page, or alternatively, if the about page refers to other files and stuff, we can put all in a sub directory: src/routes/about/index.svelte to keep them contained.\n\n\n\ni.e shared components, like a header visible on each page. This file applies to ever page: src/routes/__layout.svelte. Layouts can be nested, so subfolders can contain a __layout.svelte file which only applies to the pages in that subfolder.\nPut __layout.reset.svelte in any subfolder where you don’t want the pages to inherit layouts.\n\n\n\nSveltekit sites can run on node server, or be pre-rendered to to run on any web server as static web pages. The following two adapters seem production ready:\n\nadaptor-static - static site, suitable for github pages or anywhere really\nadapter-vercel - dynamic server rendering, so you can have functions which run on the server returning fresh data or something.\n\nThese two are listed as experimental:\n\nadapter-netlify - dynamic server renering\nadapter-cloudflare-workers - suitable for sites needing dynamic server rendering"
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html#tailwindcss",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html#tailwindcss",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "Css has become so complex that using it directly is akin to programming in C. So there are a ton of css frameworks which do the heavy lifting. But all of them require separating the content (html) from design (css), and going back and forth can be a pain.\nTailwind provides a ton of helper styles to use directly inside html. This makes it easier to just modify things directly all in one file. This made sense to me with svelte as that puts content, code and style all in the one svelte file anyways.\nTo see more about tailwindcss, see this earlier blog post."
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html#setting-up-all-the-things",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html#setting-up-all-the-things",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "This is the complex part. So I’m laying it out here step by step for my future self.\nTo see this in action, here’s the github repo for this starter project and the final, super simple output site.\n\ninstall node using brew (mac) or nvm (cross platform, probably better) or fnm.\ninstall VS code and these extensions:\n\nsvelte vs code\ntailwindcss\n\n\nNote: tailwindcss wasn’t auto-completing so I had to update this vscode setting:\n\"editor.quickSuggestions\": {\n  \"strings\": true\n}\n\n\nSee sveltekit docs and svelte-add for more.\nnpm init svelte@next myapp\ncd myappp\nnpx svelte-add tailwindcss --jit\nnpm install\nThe install script asks for options, I choose:\n\nTemplate: Skeleton project\nUse Typescript: No\nUse ESlint: Yes\nAdd Prettier: Yes\n\nThis will have installed and configured sveltekit and tailwindcss. To run:\nnpm run dev -- --open\nAdditionally, add the --host flag to expose the site to the local network, handy for testing it on your phone and other computers. If on a laptop, since the page reloads automagically on change, you can prop up an ipad or something with the output open.\nOptionally, add the tailwind typography plugin:\nnpm install -D @tailwindcss/typography\nand update the plugins section in tailwind.config.js to include this:\nplugins: [\n        require('@tailwindcss/typography'),\n    ],\nThen start the site: npm run dev -- --open. Presto! A svelte site powered by sveltekit and tailwindcss should be up and running. Go to src/routes and open index.html and copy paste this below whats already there:\n&lt;article class=\"prose px-8 py-4 bg-gray-200\"&gt;\n    &lt;h1&gt;Welcome to SvelteKit&lt;/h1&gt;\n    &lt;p&gt;Visit &lt;a href=\"https://kit.svelte.dev\"&gt;kit.svelte.dev&lt;/a&gt; to read the documentation&lt;/p&gt;\n&lt;/article&gt;\nIf the above chunk is styled, and updates in the browser preview, then everything is setup fine and working."
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html#deploying",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html#deploying",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "Time to setup an adaptor to publish this to the web. Initially I tried adaptor-static to deploy to github pages, but kept running into errors. Now the simplest thing is to build the static site locally, and use github pages to deploy the already built site, but this requires a working local dev env for any changes and thats so last year.\nSo I went with the vercel adaptor - this worked pretty painlessly out of the box, since essentially there is no local build step - you just run a local dev server: npm run dev -- --open to develop, and push to github once done.\n\n\nThe exciting part about using vercel to deploy is how easy it is:\n\nVercel detects the following frontend frameworks automatically and chooses the best default settings for you.\n\nInstall the adaptor:\nnpm install -D @sveltejs/adapter-vercel@next\nUpdate svelte.config.js so it has the vercel adapter line:\n    kit: {\n    adapter: vercel(),\n    }\nThen go to vercel, start a new project and point it to the repo for this starter project. The default settings should just work - it should build and publish the site, and auto-rebuild and publish on every git push.\nThe final output: https://sveltekit-starter-lemon.vercel.app."
  },
  {
    "objectID": "posts/writing-about-code/sveltekit_tailwindcss.html#to-look-at-later",
    "href": "posts/writing-about-code/sveltekit_tailwindcss.html#to-look-at-later",
    "title": "Sveltekit with Tailwindcss",
    "section": "",
    "text": "storybookjs - create components\ndaisyui - components for tailwind\nSvelte summit 2021"
  },
  {
    "objectID": "posts/writing-about-code/anaconda.html",
    "href": "posts/writing-about-code/anaconda.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "posts/writing-about-code/anaconda.html#install-anaconda",
    "href": "posts/writing-about-code/anaconda.html#install-anaconda",
    "title": "",
    "section": "Install Anaconda",
    "text": "Install Anaconda\nThere seems to be two main ways to handle python packaging, pip with pipenv and conda. I’ve gone with conda, though it turns out I often have to use pip inside conda. Anyways, there is a cheatsheet, but here’s all I use:\n\ndownload and install anaconda from here.\nrestart terminal and check anaconda is in the path by ECHO $SHELL\n\nIf its not in the path then:\n\non mac, add export PATH=\"/anaconda3/bin:$PATH\" at the top of .zshrc if using zsh, on bash it should have automatically added that to .bash_profile, add if not present.\n\nAnd python + jupyter lab + a bunch of other packages should be up and running!"
  },
  {
    "objectID": "posts/writing-about-code/anaconda.html#using-environments",
    "href": "posts/writing-about-code/anaconda.html#using-environments",
    "title": "",
    "section": "Using environments",
    "text": "Using environments\nProtip: install pip inside a conda enviroment if planning to ever use pip install. Otherwise pip installs inside an environment use the main pip and that is NOT GOOD.\nSo here we create a new environment which will use Python 3.9x and pip.\nconda create -n py310 python=3.10 pip\nIf you need the latest version of python use conda-forge:\nconda create -n py310 -c conda-forge python=3.10 pip\nUse this environment:\nconda activate py310\nAnd a few basic commands:\n\nconda list - shows all packages installed in the active env\n\nShows all conda envs:\nconda env list\nDelete an environment:\nconda env remove -n py310 --all\nInstall packages from a file:\nconda install -c conda-forge --file requirements.txt\n\nWrite the packages in use to disk:\nThis will include both the conda and pip installed packeges in an environment, long as pip was installed inside the environment.\nconda list --explicit &gt; py310.txt\nNow if I git clone this repo somewhere else, I can recreate the environment by:\nconda env create --file py310.txt"
  },
  {
    "objectID": "posts/writing-about-code/anaconda.html#jupyter-lab",
    "href": "posts/writing-about-code/anaconda.html#jupyter-lab",
    "title": "",
    "section": "Jupyter lab",
    "text": "Jupyter lab\nJupyter lab is the new hotness and is ready to rock out of the box with anaconda. to make it easy to select from all the environments installed, in the main anaconda env (i.e not inside an env) run:\nconda install nb_conda\nThis should ideally let the conda env with jupyter lab see all the other kernels.\nAnaconda itself has an older version of jupyter lab, so lately I have take to installing it the updated version directly in a conda env:\nconda install -c conda-forge jupyterlab"
  },
  {
    "objectID": "posts/writing-about-code/tailwindcss.html",
    "href": "posts/writing-about-code/tailwindcss.html",
    "title": "TailwindCss",
    "section": "",
    "text": "CSS makes sense, I mean, who doesn’t want to seperate styling from code? Semantic html has been the web’s dream for a long time, but over time as web browsers became operating systems css has grown into a hugely complex kludge of a tool.\n🏇 This is where tailwindcss rode to the rescue, by doing the rocket science css behind the scenes and presenting a toolbox full of utilities to use directly in html. The beauty of tailwind, which took me a while to figure out, is that everything beautifully works together and scales with size in a way which is very hard to do directly in css without having to work through browser quirks and conflicting css styles.\nThis is a guide how to simply use tailwind in a simple html project from scratch.\n\n\nInstall tailwindcss and typography in the root project folder:\nnpm install -D tailwindcss@latest\nnpm install -D @tailwindcss/typography\nnpx tailwindcss init # creates a basic tw config file\nThe -D flag makes and adds these two packages to the package.json file in the root folder. Once this file exists, running npm install will install all the packages listed in package.json.\nThis install also makes a tailwind configuration file tailwinds.config.js. The tailwind typography plugin needs to be manually added to it in the plugins section, and all the html files using tailwindcss need to be pointed to in the purge section like so:\npurge: [\"./templates/*.html\"],\nplugins: [\n    require('@tailwindcss/typography'),\n  ],\nAdd all the extra node stuff to .gitignore:\n# node stuff\nnode_modules\nbuild\nnpm-debug.log\npackage-lock.json\nFinally, create a basic css file, initially just containing the tailwind basics - I made a ko.css file in my templates folder:\n/* basic tailwindcss file */\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n/* example component */\n@layer components {\n  .btn {\n    @apply px-4 py-2 bg-blue-600 text-white rounded;\n  }\n}\nThis file is needed for putting common patterns, i.e a bunch of tailwind styles used often like a button into a .btn component. This is useful to unclutter the html file and make it clearer you are looking at a .btn instead of having to figure it out from the various utility class names.\nFinally, we get to the actual css file, which is generated by:\nnpx tailwindcss -i ./templates/ko.css -o ./static/tailwind.css\nThe above command reads my custom css file and create a gigantic css file including all of tailwind and my custom components. This is way too big for actual usage, so once happy with how everything looks, create the final production css file by:\nNODE_ENV=production npx tailwindcss -i ./templates/ko.css -o ./static/tailwind.css\nAlso consider adding the flag --minify to further compress the production css if needed.\n\n\n\nFirst up, add the tailwindcss extension to vs code, otherwise its a pain to use.\nAdd the tailwindcss to every html page. I have a header template which gets added to every page, so I just need to update that:\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n  &lt;link href=\"/tailwind.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;\n\n\n\n\nRemove backticks from inline code - this is an annoying bug which renders inline code with backticks.\n\n\n\ntailwind has made svg icons which can be styled with css directly.\n&lt;svg xmlns=\"http://www.w3.org/2000/svg\" class=\"h-6 w-6\" fill=\"none\" viewBox=\"0 0 24 24\" stroke=\"currentColor\"&gt;\n  &lt;path stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M7 8h10M7 12h4m1 8l-4-4H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-3l-4 4z\" /&gt;\n&lt;/svg&gt;\n\n\n\n\n\ngenerate tailwind colors"
  },
  {
    "objectID": "posts/writing-about-code/tailwindcss.html#install-and-setup-tailwind",
    "href": "posts/writing-about-code/tailwindcss.html#install-and-setup-tailwind",
    "title": "TailwindCss",
    "section": "",
    "text": "Install tailwindcss and typography in the root project folder:\nnpm install -D tailwindcss@latest\nnpm install -D @tailwindcss/typography\nnpx tailwindcss init # creates a basic tw config file\nThe -D flag makes and adds these two packages to the package.json file in the root folder. Once this file exists, running npm install will install all the packages listed in package.json.\nThis install also makes a tailwind configuration file tailwinds.config.js. The tailwind typography plugin needs to be manually added to it in the plugins section, and all the html files using tailwindcss need to be pointed to in the purge section like so:\npurge: [\"./templates/*.html\"],\nplugins: [\n    require('@tailwindcss/typography'),\n  ],\nAdd all the extra node stuff to .gitignore:\n# node stuff\nnode_modules\nbuild\nnpm-debug.log\npackage-lock.json\nFinally, create a basic css file, initially just containing the tailwind basics - I made a ko.css file in my templates folder:\n/* basic tailwindcss file */\n@tailwind base;\n@tailwind components;\n@tailwind utilities;\n\n/* example component */\n@layer components {\n  .btn {\n    @apply px-4 py-2 bg-blue-600 text-white rounded;\n  }\n}\nThis file is needed for putting common patterns, i.e a bunch of tailwind styles used often like a button into a .btn component. This is useful to unclutter the html file and make it clearer you are looking at a .btn instead of having to figure it out from the various utility class names.\nFinally, we get to the actual css file, which is generated by:\nnpx tailwindcss -i ./templates/ko.css -o ./static/tailwind.css\nThe above command reads my custom css file and create a gigantic css file including all of tailwind and my custom components. This is way too big for actual usage, so once happy with how everything looks, create the final production css file by:\nNODE_ENV=production npx tailwindcss -i ./templates/ko.css -o ./static/tailwind.css\nAlso consider adding the flag --minify to further compress the production css if needed."
  },
  {
    "objectID": "posts/writing-about-code/tailwindcss.html#usage",
    "href": "posts/writing-about-code/tailwindcss.html#usage",
    "title": "TailwindCss",
    "section": "",
    "text": "First up, add the tailwindcss extension to vs code, otherwise its a pain to use.\nAdd the tailwindcss to every html page. I have a header template which gets added to every page, so I just need to update that:\n&lt;head&gt;\n  &lt;meta charset=\"UTF-8\" /&gt;\n  &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\" /&gt;\n  &lt;link href=\"/tailwind.css\" rel=\"stylesheet\"&gt;\n&lt;/head&gt;"
  },
  {
    "objectID": "posts/writing-about-code/tailwindcss.html#misc",
    "href": "posts/writing-about-code/tailwindcss.html#misc",
    "title": "TailwindCss",
    "section": "",
    "text": "Remove backticks from inline code - this is an annoying bug which renders inline code with backticks.\n\n\n\ntailwind has made svg icons which can be styled with css directly.\n&lt;svg xmlns=\"http://www.w3.org/2000/svg\" class=\"h-6 w-6\" fill=\"none\" viewBox=\"0 0 24 24\" stroke=\"currentColor\"&gt;\n  &lt;path stroke-linecap=\"round\" stroke-linejoin=\"round\" stroke-width=\"2\" d=\"M7 8h10M7 12h4m1 8l-4-4H5a2 2 0 01-2-2V6a2 2 0 012-2h14a2 2 0 012 2v8a2 2 0 01-2 2h-3l-4 4z\" /&gt;\n&lt;/svg&gt;"
  },
  {
    "objectID": "posts/writing-about-code/tailwindcss.html#tools",
    "href": "posts/writing-about-code/tailwindcss.html#tools",
    "title": "TailwindCss",
    "section": "",
    "text": "generate tailwind colors"
  },
  {
    "objectID": "posts/writing-about-code/udacity-cs101.html",
    "href": "posts/writing-about-code/udacity-cs101.html",
    "title": "Udacity CS101: Intro to CS",
    "section": "",
    "text": "Udacity CS101: Intro to CS\nThis was one of Udacity’s first computer science MOOC’s, and I really liked it. Its a great fast intro to computer science concepts and python.\nIn particular, I liked that the class both had a cameo by Google’s founder and the final project was to build a simple version of Google’s original search engine using the pagerank algorithim."
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html",
    "href": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html",
    "title": "deeplearning.ai: Neural Networks and Deep Learning",
    "section": "",
    "text": "It had been a while since I finished Udacity’s Deep Learning course, so as a refresher I signed up for Andrew NG’s deeplearning.ai course.\nThese are my notes for the first course in the series: Neural Networks and Deep Learning.\nCourse Resources\n\nDiscussion forum\nYouTube playlist for Course 1\ncourse reviews: 1\n\n\n\nA basic NN looks at housing price prediction, can use simple regression\n\nthere are many input variables, like size, bedrooms, zip code, etc so you end up with a more complex regression\nWhat Neural networks are great at, giving training data of inputs and matching outputs, is figuring out functions which predict the price for us.\n\nSupervised learning maps Input(x) → Output(y)\n\n\nStructured data essentially means when you have a defined input → output model, like with housing data, recommendation engines, etc\nUnstructured data is things like text, audio, video\nDeep learning is taking of because of:\n\nvast increase in data\nincrease in computing power - enabled experimentation\nwhich led to better algorithms for NN \n\n\nGeoffrey Hinton Interview:\n\ndeveloped back-propagation algorithm (as did others)\n\nusing the chain rule to get derivatives, see wikipedia\n\ntoday, most excited about Boltzmann machines\n\nneural nets which can learn internal representations\nrestricted Boltzmann machines can do efficient inference\n\n’93 - variation bayes paper\ncurrently working on a paper on backprop and gradient\n\nis backprop a really good algorithim for learning?\nif cells can turn into eyeballs or teeth, than they can implement backprop as well. so, does the brain use it? the brain might have something close to it\n\nmultiple time skills in DL - TK\nreally believes in capsules:\n\nrepresent multi dimensional activities by a little vector of activities - in each region of a image, (assuming there is only one of each type of feature) a bunch of neurons can represent different aspects of a feature. normally in NN u have a great big layer, this partitions the representation into capsules.\n\nsupervised learning has worked really well, but unsupervised learning is going to be crucial in the future\ntoday, varitional altering code and GAN’s are huge\nadvice for breaking into deep learning:\n\nread the literature, but don’t read too much\nnotice something which doesn’t feel right, then figure out how to do it right\nnever stop programming\nread enough so you start developing intuitions then trust them\nresearch ideas for grad students: find a adviser whose beliefs are similar to yours\n\ndeep learning is not quite a second industrial revolution, buts its close to it. there is a huge change from programming computers to showing them, and they figure it out\nearly days of AI ppl were convinced you need symbolic representations of things and logic and that the essence of intelligence was reasoning, but that was a huge mistake\nNow, the view is that a thought is just a great big vector of neural activity. say, words go in, words go out, but whats in b/w doesn’t have to be anything like a string of words. Hinton says its silly to think thoughts have to be in some kind of language.\n\nHinton is fascinating and explains things really clearly.\n\n\n\n\nFirst up, we do a binary classification via logistic regression, where we classify an image into 1 cat or 0 not cat. Key concepts for this:\n\na derivative is just the slope of a line, and on a curve its different at different points on the curve.\nloss function: this gives us a score for prediction vs actual (on a single sample)\ncost function: gives us the average of errors across all predictions\ngradient descent: move slowly in the direction of the correct result\ncomputation graph: functions or calculations can be represented as a graph of the actual calcs to be done and added together\n…\nvectorization: running code is fast, and using vectors speeds up code tremendously. Wherever possible avoid for loops.\n\nPieter Abbeel Interview:\n\nhis twitter\ndeep reinforcement learning is taking off because regular reinforcement learning needs a lot of domain expertise, while the deep learning version needs a lot less engineering\nsupervised learning is about input output mapping, while deep reinforcement learning is much more about learning the incoming data - the exploration problem, then how do you credit actions which got rewards/failures?\nthe big challenge is how to get systems to reason over long time horizons\nsafety is a big challenge - how do you learn safety, for example in a self driving car, you need both success and failure to learn.\nmake sure you can make the connection from the code/math to what change it can make\ngood time to get into AI, Andrej Karpathy’s DL course, UC Berkely has a Deep Reinforcement course online.\ndeep reinforcement learning is really powerful and general, the same learning can take in different inputs and learn a task, like teaching a two legged robot to move, then the same algo can take a four legged robot to move even though the inputs (sensors) and outputs (motors) are now different.\n\ndrl algos still start from scratch for tasks\n\n\n\n\n\n\ngoes over the vector math to compute NN output\nactivation functions take in weighted data and apply a non-linear transformation\n\nsigmoid squeezes output to 0 to 1, useful for predicting probablity, not used much anymore\ntanh is like sigmoid but from -1 to 1 - superior to sigmoid in almost all applications\nrelu function is very popular, anything below zero becomes zero, otherwise the value stats as it is\nleaky relu multiplies negative numbers by a small number, like 0.01 - this helps to increase the range of the relu output\nmore, try different activation functions\n\nnon-linear activation functions are a critical part of a NN since they create new relationships b/w data points, increasing complexity with each layer.\nlinear hidden layers are more or less useless since any number of linear hidden layers can be reduced to a single linear layer\nits important to randomly initialise weights as starting with weights set to zero the hidden units will initialise the same way and thus all the hidden units are symmetric and compute the same function.\nFix this by initialising weights and b to small random numbers like 0.01. If weights are large, the slopes of an activation function like tanh or sigmoid are flat for large numbers, so the gradient is small and training is slow.\n\nIan Goodfellow interview:\n\nhad been studying generative models for a long time, so GANs rose out of that. At a bar, coded it up at midnight after a party, and it worked on the first try.\nworking on making GANs more reliable\nwrote the first modern textbook on deep learning, very popular\nlinear algebra and probability are very important to master deep learning\nDL only really got going five years ago\nthere’s an amazing amount of AI work to be done\nwrite good code and put it on github, solve a problem which ppl have\n\n\n\n\n\nA deep neural network is a neural network with many layers which can learn functions which shallower networks are unable to. Its hard to predict the number of layers for a given problem, so experiment.\nwrite down all the matrix dimensions of all the layers in your NN - this helps with debugging and understanding how data is being transformed as it passes through the NN\nwhy do we need deep networks?\n\ndifferent layers learn different features\nearly layers learn simpler features, the deeper layers learn more complex objects and representations\nDNN allow more complex functions to be computed - circuit theory postulates that a small L layer deep NN can compute functions which a shallower network would require exponentially more hidden units to compute\ndeep learning makes for great PR\n\ngradient descent needs forward and backward functions:\n\nforwards: we compute a prediction, moving through all the layers, and caching the results at each layer\nbackwards: now we compute the gradients at each layer, moving backwards\nfinally we use these gradients to tweak each layer slightly towards the right direction\n\na lot of the complexity in ml comes from the data, not the code\nparameters are the weights and beta\nhyper parameters:\n\nlearning rate\n\n\n\nchoice of activation function\nthere are lots of other hyperparameters like momentum, minibatch size, regularization, etc\n\nthere is a lot of experimentation around parameters, try to do this empirically and build intuitions around what works for different kinds of data and NN as parameters work differently for different problems\ndon’t compare DL networks to the human brain - while there are simplistic analogies b/w a single neuron and a logistic unit with sigmoid activation - but we don’t know how neurons in a human brain work or how the brain learns"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-1-introduction",
    "href": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-1-introduction",
    "title": "deeplearning.ai: Neural Networks and Deep Learning",
    "section": "",
    "text": "A basic NN looks at housing price prediction, can use simple regression\n\nthere are many input variables, like size, bedrooms, zip code, etc so you end up with a more complex regression\nWhat Neural networks are great at, giving training data of inputs and matching outputs, is figuring out functions which predict the price for us.\n\nSupervised learning maps Input(x) → Output(y)\n\n\nStructured data essentially means when you have a defined input → output model, like with housing data, recommendation engines, etc\nUnstructured data is things like text, audio, video\nDeep learning is taking of because of:\n\nvast increase in data\nincrease in computing power - enabled experimentation\nwhich led to better algorithms for NN \n\n\nGeoffrey Hinton Interview:\n\ndeveloped back-propagation algorithm (as did others)\n\nusing the chain rule to get derivatives, see wikipedia\n\ntoday, most excited about Boltzmann machines\n\nneural nets which can learn internal representations\nrestricted Boltzmann machines can do efficient inference\n\n’93 - variation bayes paper\ncurrently working on a paper on backprop and gradient\n\nis backprop a really good algorithim for learning?\nif cells can turn into eyeballs or teeth, than they can implement backprop as well. so, does the brain use it? the brain might have something close to it\n\nmultiple time skills in DL - TK\nreally believes in capsules:\n\nrepresent multi dimensional activities by a little vector of activities - in each region of a image, (assuming there is only one of each type of feature) a bunch of neurons can represent different aspects of a feature. normally in NN u have a great big layer, this partitions the representation into capsules.\n\nsupervised learning has worked really well, but unsupervised learning is going to be crucial in the future\ntoday, varitional altering code and GAN’s are huge\nadvice for breaking into deep learning:\n\nread the literature, but don’t read too much\nnotice something which doesn’t feel right, then figure out how to do it right\nnever stop programming\nread enough so you start developing intuitions then trust them\nresearch ideas for grad students: find a adviser whose beliefs are similar to yours\n\ndeep learning is not quite a second industrial revolution, buts its close to it. there is a huge change from programming computers to showing them, and they figure it out\nearly days of AI ppl were convinced you need symbolic representations of things and logic and that the essence of intelligence was reasoning, but that was a huge mistake\nNow, the view is that a thought is just a great big vector of neural activity. say, words go in, words go out, but whats in b/w doesn’t have to be anything like a string of words. Hinton says its silly to think thoughts have to be in some kind of language.\n\nHinton is fascinating and explains things really clearly."
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-2-basics-of-nn-programming",
    "href": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-2-basics-of-nn-programming",
    "title": "deeplearning.ai: Neural Networks and Deep Learning",
    "section": "",
    "text": "First up, we do a binary classification via logistic regression, where we classify an image into 1 cat or 0 not cat. Key concepts for this:\n\na derivative is just the slope of a line, and on a curve its different at different points on the curve.\nloss function: this gives us a score for prediction vs actual (on a single sample)\ncost function: gives us the average of errors across all predictions\ngradient descent: move slowly in the direction of the correct result\ncomputation graph: functions or calculations can be represented as a graph of the actual calcs to be done and added together\n…\nvectorization: running code is fast, and using vectors speeds up code tremendously. Wherever possible avoid for loops.\n\nPieter Abbeel Interview:\n\nhis twitter\ndeep reinforcement learning is taking off because regular reinforcement learning needs a lot of domain expertise, while the deep learning version needs a lot less engineering\nsupervised learning is about input output mapping, while deep reinforcement learning is much more about learning the incoming data - the exploration problem, then how do you credit actions which got rewards/failures?\nthe big challenge is how to get systems to reason over long time horizons\nsafety is a big challenge - how do you learn safety, for example in a self driving car, you need both success and failure to learn.\nmake sure you can make the connection from the code/math to what change it can make\ngood time to get into AI, Andrej Karpathy’s DL course, UC Berkely has a Deep Reinforcement course online.\ndeep reinforcement learning is really powerful and general, the same learning can take in different inputs and learn a task, like teaching a two legged robot to move, then the same algo can take a four legged robot to move even though the inputs (sensors) and outputs (motors) are now different.\n\ndrl algos still start from scratch for tasks"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-3-code-a-simple-nn",
    "href": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-3-code-a-simple-nn",
    "title": "deeplearning.ai: Neural Networks and Deep Learning",
    "section": "",
    "text": "goes over the vector math to compute NN output\nactivation functions take in weighted data and apply a non-linear transformation\n\nsigmoid squeezes output to 0 to 1, useful for predicting probablity, not used much anymore\ntanh is like sigmoid but from -1 to 1 - superior to sigmoid in almost all applications\nrelu function is very popular, anything below zero becomes zero, otherwise the value stats as it is\nleaky relu multiplies negative numbers by a small number, like 0.01 - this helps to increase the range of the relu output\nmore, try different activation functions\n\nnon-linear activation functions are a critical part of a NN since they create new relationships b/w data points, increasing complexity with each layer.\nlinear hidden layers are more or less useless since any number of linear hidden layers can be reduced to a single linear layer\nits important to randomly initialise weights as starting with weights set to zero the hidden units will initialise the same way and thus all the hidden units are symmetric and compute the same function.\nFix this by initialising weights and b to small random numbers like 0.01. If weights are large, the slopes of an activation function like tanh or sigmoid are flat for large numbers, so the gradient is small and training is slow.\n\nIan Goodfellow interview:\n\nhad been studying generative models for a long time, so GANs rose out of that. At a bar, coded it up at midnight after a party, and it worked on the first try.\nworking on making GANs more reliable\nwrote the first modern textbook on deep learning, very popular\nlinear algebra and probability are very important to master deep learning\nDL only really got going five years ago\nthere’s an amazing amount of AI work to be done\nwrite good code and put it on github, solve a problem which ppl have"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-4-code-a-deep-nn",
    "href": "posts/writing-about-code/deeplearning-ai-part-1-of-5.html#week-4-code-a-deep-nn",
    "title": "deeplearning.ai: Neural Networks and Deep Learning",
    "section": "",
    "text": "A deep neural network is a neural network with many layers which can learn functions which shallower networks are unable to. Its hard to predict the number of layers for a given problem, so experiment.\nwrite down all the matrix dimensions of all the layers in your NN - this helps with debugging and understanding how data is being transformed as it passes through the NN\nwhy do we need deep networks?\n\ndifferent layers learn different features\nearly layers learn simpler features, the deeper layers learn more complex objects and representations\nDNN allow more complex functions to be computed - circuit theory postulates that a small L layer deep NN can compute functions which a shallower network would require exponentially more hidden units to compute\ndeep learning makes for great PR\n\ngradient descent needs forward and backward functions:\n\nforwards: we compute a prediction, moving through all the layers, and caching the results at each layer\nbackwards: now we compute the gradients at each layer, moving backwards\nfinally we use these gradients to tweak each layer slightly towards the right direction\n\na lot of the complexity in ml comes from the data, not the code\nparameters are the weights and beta\nhyper parameters:\n\nlearning rate\n\n\n\nchoice of activation function\nthere are lots of other hyperparameters like momentum, minibatch size, regularization, etc\n\nthere is a lot of experimentation around parameters, try to do this empirically and build intuitions around what works for different kinds of data and NN as parameters work differently for different problems\ndon’t compare DL networks to the human brain - while there are simplistic analogies b/w a single neuron and a logistic unit with sigmoid activation - but we don’t know how neurons in a human brain work or how the brain learns"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html",
    "href": "posts/writing-about-code/harvard-cs109-notes.html",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "",
    "text": "Notes for Harvard’s Cs109 data science class\nThese are my notes for Harvard’s 2015 CS109 class, which i went through with Sydney Machine Learning’s study group from Ausgust to October 2017 at Amazon Web Service’s sydney office.\nWhy CS109? This class was recommended at Quora and a few other places as being a good resource for practical data science, so here goes. These notes are updated as I work through the course syllabus and the labs and homeworks.\nThe stuff to watch and work through: - Lecture videos & notes\nNote: download the videos using this script, and merge pull request 11 to get the 2015 videos.\nStudy Suggestions before starting:"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-1-what-is-data-science",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-1-what-is-data-science",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 1: What is Data Science",
    "text": "Week 1: What is Data Science\nLecture 1 introduces data science. The basic stuff covered in every blog post."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-2-intro-data-analysis-and-viz",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-2-intro-data-analysis-and-viz",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 2: Intro Data Analysis and Viz",
    "text": "Week 2: Intro Data Analysis and Viz\nThe lecture 2 notebook goes through getting data and putting it into a pandas dataframe.\nLab 1 has three very introductory notebooks: pythonpandas, followed by babypython, and finally git. However, since the course dates back to 2015, some of the python is a bit dated and uses 2.x code.\nAfter doing the three intro notebooks, hw0 runs you through installing anaconda, git, and setting up a github and aws account.\nHw0 has one interesting section, where you solve the montyhall problem one step at a time. I didn’t really like their steps, so made a simpler monty hall implementation.\nMoving on to the Lecture 2 & its quiz notebook, this goes through some more pandas and data scraping web pages and parsing them.\nI made a couple of notebooks expand on some of the stuff covered:\n\nmovielens notebook for basic pandas workflow of downloading a zip file, extracting it and putting into pandas dataframes and doing some q&a\ntwitter notebook - basic usage of twitter api and doing something with tweets\n\nLecture 3 (slides, video):\n\nask a q, get data to answer it, explore & check data, then model it and finally communicate and visualize the results.\nkeep viz simple and think of the kind of chart needed."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-3-databases-sql-and-more-pandas",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-3-databases-sql-and-more-pandas",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 3 : Databases, SQL and more Pandas",
    "text": "Week 3 : Databases, SQL and more Pandas\nLab 2 introduces web scraping with requests and then parsing html with beautiful soup 4.\nLecture 4 (video, slides) (covers some more Pandas and SQL.\nLecture 5 (slides, video) on stats is a bit sparse. Some supplementary material:\n\nStanford Statistics Course - check on this one vs the MIT one.\nThink Stats is a good basic book covering stats using Python.\nThink Bayes follows on from Think Stats and covers Bayesian stats in Python."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-4-probablity-regression-and-some-stats",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-4-probablity-regression-and-some-stats",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 4: Probablity, regression and some stats",
    "text": "Week 4: Probablity, regression and some stats\nLab 3 has three notebooks: - Lab3-Probability covers basic probability. Uses a lot of numpy methods, so its a good idea to brush up on numpy. - scipy.stats - very handy, has most stats stuff needed for DS built in. - Lab3-Frequentism, Samples and the Bootstrap - use seaborn for plotting, very handy. a good guide to sns factorplot and facetgrids - PDF tells us the probability of where a continuus random variable will be in set of possible values that random variable can be (the sample space). - PMF tells us the probability that a discrete random variable will be ecactly equal to some value - CDF function tells us the probability that a random discrete or continous variable X will take a value less than or equal to X. Video\n\nLecture 6: Story Telling and Effective Communication (slides, video)\nGood insights on how to tell a story with data. Infer, model, use an algorithim and draw conclusions (and check!).\n\nStart with two fundamental questions:\n\nWhats the goal? think first of that rather than going first to all the many ways you can slice and dice data.\nWho cares? Know your audience and tell them a story. Have a clear sense of direction and logic.\n\nRead some howto’s on scientific writing\nhave some memorable examples or small stories\n\nTell a story:\n\nknow your audience and why/what they care about this data - what do they want?\nDon’t make them think - clearly tell them what you want them to know in a way they can follow. highlight key facts and insights.\nunexpectedness - show something the audience didn’t expect. I liked the story which highlighted that bigfoot sightings are dropping sharply\nWhat tools can we give the audience? For example, a web app for them to further explore the data, or a takeaway presentation with key points.\nbe careful of your point of view and don’t distort the data, but depending on the audience you can frame your story - for example presenting war deaths in red rather than a regular plot color.\nimportant to put the message up front - what does my graph show? Show it in stages if a lot of data, highlighting what to look at. Design matters.\n\nMore resources:\n\nThe Functional Art\n\n\n\nLecture 7: Bias and Regression (slides, video)\n\nthink about bias, missing data, etc\ncombine independent, unbiased estimators for a parameter into one:\n\nfisher weighting\nnate silver weighting method\n\nBonferroni\ngood segment on weighting variables\nregression towards the mean\nthink of regression in terms of population or a projection of the column space of x - i.e what combination of the variables of x gets us closest to the value of y?\nlinear regression means we’re taking linear combination of predictors, the actual regression equation can be nonlinear\nwhat function of x gives the best predictor of y?\nGauss-Markov Theorem\nthe residuals are the diff b/w the actual value of y and the predicted value - plot residuals vs fitted values and vs each predictor variable - good way to eyeball quality of linear regression model\nvariance R^2 measures goodness of fit, but doesn’t mean model is good.\nBest way to check a model is prediction."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-5-scikit-learn-regression",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-5-scikit-learn-regression",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 5: Scikit learn & regression",
    "text": "Week 5: Scikit learn & regression\n\nLab 4 - Regression in Python (video, notebook)\n\nWikipeda article\nWe have data X, which is related to Y in some way.\nLinear regression uses X to predict Y, and also tells us the predictive power of each variable of X\nLinear regression assumes the distribution of each Y is normal (which isn’t always so)\nthere are many ways to fit a linear regression model, most common is the least squares method\nbe careful that the features (variables in X) aren’t too similar\nexplore your data, plot variables, scatterplots etc. Use seaborn to plot regression.\nuse sklearn to split dataset into a train/test\nuse cross-validation\noverfitting happens when the model ‘learns’ the train data so performs better on that than the test dataset\nthere are many types of regressions so think about which one to use\nfor high d data, closed form vs gradient decent:\n\nclosed form - use math to solve. this becomes computationally intensive very quickly, is ordered n cubed\ngradient descent is O(n), so for large high d data it’s gradient descent all the way\n\nLogistic regression - used where outcome is binary, for example a chance of success/failure. read:Adit’s explanation.\n\n\n\nLecture 8: More Regression (video, slides)\n\ncollinerarity - when some variables are highly correlated with each other - this is bad\nLogistic Regression\nOdds Ratio: ratio of two different people’s odds of an outcome.\nCrude Odds Ratio Estimate - quick estimate but flawed as doesn’t control for anything.\nConfounding Factors - i.e is one group pre-disposed to our outcome for some reason?\nCurse of dimensionality - in high d settings, vast majority of data is near boundaries, not center. But, high d can also be a blessing.\ndealing with high dimensionality: ridge regression, shrinkage estimation\nStein’s Paradox wikipedia, good article\nLASSO and Ridge help with high D data by reducing features\n\nLasso does L1 regularization, reduces number of features\nRidge does L2 regularization, doesn’t necessarily reduce features but reduces the impace of features on the model by reducing coefficient value\n\nElasticnet does both L1 and L2 regularization\n\n\n\nLecture 9: Classification (video, slides)\n\nwe take data and assign labels\n1 nearest neighbour - simple classification method for low d data\n\nslow, has to check all points to find nearest neighbour\n\nk nearest neighbours - use k nearest points to find decision boundary\n\nfind ideal k\nwhat distance function to use?\nmy own very simple kNN algo implementation\n\ncross validation - for 5 fold cross validation, the data is split into 6 folds - 4 for training, one for validation and the sixth for testing, which is only used at the end.\nCIFAR-10 for 60K images - is split into 50K training and 10K test\n\npics are 32x32x3\n\nL1 distance is the absolute diff b/w two vectors\nL2 is the Euclidean distance i.e “ordinary” straight-line distance\nfor images, l1 and l2 are pretty bad, so there are a lot more methods\nmore features are good for classification, but too many features means the data gets sparse - the curse of dimensionality strikes\nso often we want to reduce dimensionality\nPrincipal Component Analysis - project a dataset from many variables into fewer less correlated ones, called the principal components.\nSingular Value Decomposition (SVD) - computational method to calculate pricipal components of a dataset. It transforms a large matrix of data into three smallter matrixes: A (m*n) = U(m*r) x E(r*r) x V(r*n). The values in the middle matrix r*r are the singular values and we can discard bits of them to reduce the amount of data to a more manageable number.\ngood pca and svd explanation\nWatch Statistics for Hackers\n\n\n\nHW2 Q1 (notebook)\n\nUses svd and pca to analyze gene data\na pandas excercise in downloading csv files into a data frame, usijng pd.datetime and visualising samples vs time"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-6-svm-trees-and-forests",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-6-svm-trees-and-forests",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 6: SVM, trees and forests",
    "text": "Week 6: SVM, trees and forests\nNow the course finally gets interesting. Before starting this weeks work, think about project ideas and see Hans Rosling videos to see how to present data. Pitch this project idea (to study group or the internet at large).\nThere are quite a few companies automating the entire datascience chain, so the key is being able to present your findings well.\n\nHW 2 Questions 2,3 & 4 (notebook)\nH2 depends wholly on week 5, so good idea to get it done first. Used seaborn for all the viz questions makes some of them trivial.\n\nq2 looks at polling data and bias\nq3 is more of the same but with seaborn\nq4 nothing much to see here besides using list comprehensions to make a list of all the .csv files (I’m trying to use python to do all the work instead of my stone age past life of copying and pasting links)\n\nurl_str = \"http://elections.huffingtonpost.com/pollster/api/charts/?topic=2014-senate\"\nelection_urls = [election['url'] + '.csv' for election in requests.get(url_str).json()]\n\n\nLab 5: Machine Learning\nLearning Models (notebook, video)\n\nwe often have a small sample of a much dataset, and we want to predict the larger data from our sample.\nthis isn’t just statistical analysis, as we make models which involve domain knowledge and choices.\nneed to think about whether our sample is in some way representative of the population\nStochastic noise, i.e randomness\nsystematic error, i.e where the sampling isn’t representative, like polling ppl using landlines\noverfitting: models can ’memrize the the data points in the training set, becoming useless or inaccurate at predicting real world data. With many data sets a more and more complex dataset will keep getting better while getting worse on test/validation data. The best model minimizes test set error, and not training set error.\ngreat illustration of variance at 24:30 and 35min in the video\nuse from sklearn.cross_validation import train_test_split for splitting datasets into a train test split. See sklearn\nsklearn has 3 main features:\n\nbuild and fit models\npredict\ntransform data.\n\nsklearn expects days in a 2d array or a matrix of size [n_samples, n_features], so reshape 1d data using np.reshape(-1,1)\nValidation - keep a chunk of data seperate to check the model after training on the test/train data.\nCross Validation: randomly split data into K diff train/test splits - so you traion on K-1 partitions and test on 1, so there are a total of K combinations, leading to K risks. This leads to better results then just doing one test/train split.\nregularization helps with overfitting\n\nClassification (notebook, video)\n\nsort data into classes, i.e what kind of fruit\nmost datasets can be projected onto a lower dimensial space, for e.g using PCA\nread sklearn’s PCA docs\nkNN:\nLogistic Regression - use sklearn, main paramater is C which defines how much to regularize the data. Read this explanation\nUse sklearns GridSearchCV to find hyperparameters\none way to classify: use PCA to reduce the feature space, then use logistic regression to classify\nmany datatypes, like images, have tons of features, so important to reduce dimensionality.\nsklearn PCA returns the principal components in order of how much they explain the data:\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=60) # PCA with no. of components to return\nX = pca.fit_transform(data)\nprint(pca.explained_variance_ratio_) # how much of variance is explained\n\nsklearn uses the same interface for all its classifiers, so makes it easy to put a wrapper around gridsearchCV and pass in diff classifiers to compare.\ndiscriminative classifier - finds the decision boundary b/w classes\nmaximum-margin classifier - for many classifincation problems, multiplie diff lines can seperate classes. Choose that line where the margin b/w classes is the largest, which makes this model senstive to boundaries b/w classes, rather than to point samples deep inside a class.\nSVM is a discrimant classier which finds the widest possible margin b/w classes, including some points touching the boundary which are called the support vectors. (since they support or establish the margins.)\n\n\n\nLecture 10: SVM, Evaluation (video, slides)\n\nKNN - training is fast, prediction slow since we need to check all the data points to find the nearest neighbours\nbut if we know the decision boundary (the seperating hyperplane) we don’t need all the data points\n\nw: weight vector defines the orientation of the hyperplane, and bias b.\nso a new point x is classified by w(transpose)*x + b\nthis is the mathematical model of a neuron, invented 1957 by Rosenblatt\n\nstep function vs sigmoid activation\nSupport Vector Machines (SVM) are widely used, some consider it best of the shelf classifier. They add a new dimension to help seperate classes and also use maximum margin classification. SVM is called svm becuase of the support vectors defining the max margin lines for the classification boundary.\nlarge data is good for training svm as the points on the boundary are rare and svm cares about establishing the boundary\nsince outliers can change the svm boundaries, there is a concept of slack variables - it allows the SVM to missclassify outliers to make a neat decision boundary. sklearn uses the parameter C to define the slack. the lower the number the more the slack.\nkernel tricks for svm - go to aribitarily mary dimensions with little computational cost. need to think about what kernel to use. Read What are kernels in machine learning and SVM and why do we need them?.\nread Andrew Ng’s cs229 svm notes\ntodo: tale sklearns ‘faces’ dataset and use svm to predict\nsvm tips:\n\nnormalize data to 0,1 or -1,1 interval. (check whether the library already normalizes)\nRBF kernel is a good default\nRead Chich-Wei Hsu practical guide to SVM\ntune paramaters - which kernel, what parameters for it and what C?\n\nROC curve: plot true positive rate vs false positive rate\n\ntrue +ve is tp/(tp+fn)\nfalse +ve is fp/(fp+tn)\none useful summary stat is area under the curve\n\nPrecision Recall Curve sklearn, quora\n\nPrecision: tp/(tp+fp) - how much are we getting right, or the probability a a random +ve sample is actually +ve (since some +ve samples are false positives).\nRecall tp/(tp+fn) - same as in the ROC, how much of the data are we finding. for a random +ve sameple, the probability that it’s making a correct prediction. (consider the false negatives.)\nideally we want both to be one.\n\ngood way to compare classifiers\nHow do you classify multple classes with a SVM, e.g 10 types of fruit\n\nOne vs all - pick one class, and train it against all the other classes one by one. So you train n classifers for n classes.\nOne vs One - Train n(n-1)/2 classifiers, take majority vote\nuse a confusion matrix of predicted label vs true labels to see classification results\n\n\n\n\nLecture 11: Decision Trees and Random Forests (video, slides)\n\nBooks: Read Elements of Statiscal Learning and Pattern Recognition and Machine Learning\nDecision Tree - fast training and prediction, easy to understand and interpret. DT basically paritions a feature space into cells using a series of decision rules on one feature at a time\nwhich feature to query and what thresholds to use?\nnode purity: do splits so cells are pure, i.e have only one class in them\n\nGini Impurity gives us the expected error of predicted a class from a random sample\nGini Index\nNode purity gain: compare gini impurity of parent vs child nodes. Lets us see whether a split has improved classification better than a simple missclassification number.\n\nOptimal trees - diff ways to get there\nTree pruning - easy to overfit, so first we make the tree, then go backwards and remove ‘bad’ decisions, or merge cells etc.\nDT disadvatages: sensitive to small changes in data, overfit, only axis aligned splits\nDT vs SVM:\nNetflix prize winners used an ensemble of over 800 models. somewhat disapointing as they didn’t come up with a new method\nDT doesn’t perform well, but what if we use many of them?\nBootstrap is one way to do this. It’s a resampling method from statistics, useful to get error bags on estimates.\n\nBootstrap lets us generate more sample sets from one dataset, each one slightly different.\nTake N data points and draw N times with replacement, then get an estimate from each bootstrapped samples.\nbagging: bootstrap aggregrating, where you learn a classifer from each bootstrap sample and average the . (normally uses just one type of classifier)\n\nSee bootstrap example notebook\nbagged DT’s perform better than one a single tree\nnot useful for linear models\n\nBias-Variance trade off: train models with a high variance, then the average might get close\n\nRandom Forest builds on bagging, builds a tree from each bootstrap sample with node splits selected from random feature subsets. See 1, or rather 2"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-7-machine-learning-best-practices",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-7-machine-learning-best-practices",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 7: Machine Learning best practices",
    "text": "Week 7: Machine Learning best practices\nHW 3 Q1 due: a lot of pandas manipulation on baseball data\nStart the project\n\nTowards the end of the course you will work on a month-long data science project. The goal of the project is to go through the complete data science process to answer questions you have about some topic of your own choosing. You will acquire the data, design your visualizations, run statistical analysis, and communicate the results. You will work closely with other classmates in a 3-4 person project team.\n\n\nLab 6: Machine Learning 2\nClassification, Probabilities, ROC curves, and Cost (video,notebook)\n\nNot a very useful lab, essentially better of from sciki-learn understand how to use:\n\nLogistic Regression\nSVM, or swm with kernels - good tutorial\nConfusion Matrix\n\n\nComparing Models (video, notebook)\n\nlearn Bayes classifiers in sklean\nuse ROC curves - often accuracy is the the relevant, the true +ve and false -ve rate is more important. Since False negatives can be costly, you often want to change the threshold probability from the default 0.5. So write your own prediction function as the sklearn one uses 0.5, or with bayes classifiers adjust the prior probability.\nsklearn has a roc function, tutorial\n\n\n\nLecture 12: Ensemble Methods ((video, slides))\n\nphilisophical point: who do ensenble methods work so well? in real life wisdom of the crowds is overrated, but it does a lot better in computerland. Some averaging methods pick up useful stuff in the data, while others cancel out each others errors.\nDecision trees are easy but have poor predictive accuracy, tend to ovefit\nEnsemble learning combines many learners using methods like weighted avg, boosting, etc. See sklearn’s ensemble page\nrandom forests is an extension of bagging of decision trees\n\nrandom forests using sqrt(features) of random predicters to make trees give the best results\n\nBoosting is another ensemble method like bagging, but better for most applications.\n\ntune by num of trees, splits in each tree, weights\noften using very simple trees, which you adjust the weights as u make more trees to classify better the things u got wrong. At the end u make a weighted avg.\nmost popular & succesful boosting algorithim is AdaBoost\n\n\nNote: read this series on machine learning\n\n\nLecture 13: Best Practices (video, slides)\n\nstory telling is important for data scientists - explain whats going on, even in your own notebooks. good presentation is very important.\nDiff b/w bagging and random forest: rf has bagging idea + random feature subsets\ndidn’t really find this video that useful, for example:\n\nknn and trees can be used for regression too, but why would we?\nSVM’s can be used for regression\n\ntake care to normalize your data in a sane manner"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-8-ec2-and-spark",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-8-ec2-and-spark",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 8: EC2 and Spark",
    "text": "Week 8: EC2 and Spark\nHW3 q2 uses the iris data set & q3 uses sklearn’s digits dataset and gridsearchCV to find best parameters for a KNN classifier for two simple datasets.\n\nLab 7: Decision Trees, Random Forests, Ensemble Methods (video, notebook)\n\ndecision trees, but they use their own function on top of sklearn so its a bit annoying\nrandom forests:\n\ntake a random subsample of the data\nselect a subset of all possible variables (at each node) and build the tree on the best split\nrepeat, then finally take a majority vote\n\nEnsemble learning - put together a lot of classifiers to build a better one\nAdaBoost Classifier uses weights on the training data.\nGradient Boost Classifier is similar to AdaBoost but uses tree as its base classifier, can do regression or classification.\n\n\n\nLecture 14: Best Practices, Recommendations and MapReduce (video, slides)\n\nshould have started final project by now, if not, start now!\nNesting: use 5 fold cross validation, take each fold and further apply 5 fold cv to find the right hyperparameters, then use those params in the original fold to train the classifier.\nthink about how and why to normalize data, e.g data already in a range from 0-1 might not need to be normalized, think about normazlization as hyperparameters to your model.\n\nget the mean estimates from your training data and use that to normalize training, validation and testing data. these values need to be stored to normalize future data.\n\nknow your application domain\nmany problems are to do with imbalanced data. Fixes to get balanced data for training:\n\noversample: take bigger samples of the sparse clasess\nsubsampling: take smaller samples of the more frequently occuring classes\nor weight classes so classifier pays more attention to the sparese classes, see sklearn\n\nMissing data, where some of the features are missing for certain samples:\n\ndelete, but can reduce data too much\nuse mean\nuse regression to estimate missing values\n\nCollobrative filtering is a common technique for recommendations or filling in missing values. Can be user based or item based.\n\nKNN is widely used for collobrative filtering.\nSVD is widely used, famously in the netflix contest. watch this svd video for a refresher if needed.\n\n\nMoving on from Machine Learning…\nMap reduce is a way to deal with very large data sets by distributing work over many machines. Developed by Google, and Apache Hadoop is a open source implementation.\n\ndata is in key value pairs and is distributed to however many servers\nwe write a map function which does something to a key value pair.\nthe mapreduce implementation reaggregrates the key value pairs incoming from all the servers (sorted by keys)\nwe write a reduce which does something else\nsimple mapreduce example:\n\nmap takes in sentences, sends them off to diff machines to process,\nthose machines send back key:value pairs, say we are just counting words so we get back from machine 1: \"the\":1, \"the\":1, \"a\":4 and machine 2 sends back “the”:4, “a”10,\nthere can be a combine step here, which takes the output from one mapper and combines it, so the two the words from machine 1 become one \"the\":2. this reduces network traffic and makes the work upstream easier. the combine output has to be of the same type as the mapper, since combine is just an optional optimizatizion step not a DO WORK thing.\nthe mapreduce thingamajig aggregrates all the key:value pairs and sends them to the\nreduce function, which counts the words and finally we end up with “the”:6 and “a”:14 \nUdacity has a course on mapreduce with hadoop\n\n\n\n\nLecture 15: MapReduce Combiners and Spark (video, slides)\n\nrefresher on mapreduce: it takes away the headache of dealing with machines, clusters, etc\n\nbe careful on how you use reducers, they shouldn’t effect the algorithims effectiveness\nthe reducer should work whether or not the combiner runs\nyou could have a in-mapper combining, but here we are gettting into complex territory\n\nWhy did we switch to map reduce from good old write a function and run it computing to the horror that is map reduce?\n\nlarge large data which looms like a t-rex, scaring our current teeny tiny computers cowering away in fear. luckily the computers gathered in data centers where mapreduce and other such algos can harness them in parallel.\nmany problems are embarrisingly parallel\n\nApache Hadoop is an open source implementation of map reduce, built way back in 2006. It provides us a way to store data on clusters of commodity machines and process it using mapreduce, without having to worry about the rocketscience of managing the clusters and splitting data and processes across all the machines.\nThough these days you probably want to use Hadoop with Apache Spark running on Amazon’s EMR or Google Cloud.\npython has tools like dask and ipyparallel for parallel computing. for many projects python is enough\nFunctional programming: where your functions take an input, process it and return an output, without messing around with other functions or data stored elsewhere. this makes so much sense that it shouldn’t be a big deal, it sounds like a toyota corolla of the computing world, but its a big deal because ppl manage to write functions which fuck around with your programming world enough that after a few rounds of their unfunctional running around no one has a good idea of whats the actual state of things.\n\nFunctional programming is clean! Watch Joel Grus Learning Data Science Using Functional Python or this one\n\nGetting back to mapreduce, when you are programming functionally, then it makes it possible to distribute the data and computing across different machines as the functions aren’t trying to mess with each other. When a machine dies, its no big deal as we know what data and function we sent it to process, we just resend it to some other machine.\n\naside: this is why pandas always outputs a new dataframe whenever we do something, trying to emulate this ideal of not fucking up existing things. Spark also does similar stuff.\n\nApache Spark makes all this easy for us. Runs on top of Hadoop and provides nice interface and methods for all kind of stuff. So nicer, faster, shineir than Hadoop.\n\nSpark stores data on a Resilient distributed dataset (RDD) - a fault tolerant collection of stuff which can be operated on in parallel.\nthe basics of using sparl: write a function, some kind of mapreduce job, spark runs it on the RDD and makes a new processed RDD.\na lot of the pandas style commands work on spark\nspark is lazy - it makes a graph of the stuff we want to do and then runs it from start to end every time we execute. so caching is important so we don’t keep computing same stuff over and over again\nexplore spark…. todo"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-9-bayes",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-9-bayes",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 9: Bayes!",
    "text": "Week 9: Bayes!\n\nLab 8: Vagrant and VirtualBox, AWS, and Spark (video, notebook)\n\nMoving on from sklearn…\n\n\n\n\nLecture 16: Bayes Theorem and Bayesian Methods (video, slides)\n\nbook recommendations from the lecture: (the one I liked best in bold)\n\neasy reading: How Bayes’ Rule Cracked the Enigma Code also see her talk at google.\nsimple stuff, in python: Think Bayes, though maybe too basic\nProbabilistic Programming & Bayesian Methods for Hackers - text and python code is in jupyter notebooks which u can clone to your own pc and go through.\nProper textbooks: Bayesian Data Analysis and Statistical Rethinking - this comes with python examples.\n\nBayes rule: P(A|B) = P(B|A)P(A) / P(B)\nbayes rules tells us how to update our beliefs (the prior) as we get new data, which gives us a new posterior (which is just a fancy word for our new, updated belief). A more wordy description:\n\nThe theorem itself can be stated simply. Beginning with a provisional hypothesis about the world (there are, of course, no other kinds), we assign to it an initial probability called the prior probability or simply the prior. After actively collecting or happening upon some potentially relevant evidence, we use Bayes’s theorem to recalculate the probability of the hypothesis in light of the new evidence. This revised probability is called the posterior probability or simply the posterior.\n\n\nbayes is controversial becuase traditional stats doesn’t like giving numbers to unknown thinks, for example bayes essentially makes up the prior. the prior is often our subject belief about something.\n\nhowever, even when starting out with different priors, given that they aren’t ridiculously dogmatic, with a large sample size the different priors will converge\n\ndiscriminative model: focus on predicting y given x, generative model: we simulate the entire model, i.e we can generate x and y\nnaive bayes: assumes probablities are conditionally independent of each other, greatly simplifies the calculations. sometimes unrealistic but works well for many scenarios. sklearn has bayes, of course.\nConjugate prior says that if u start with a family of distributions, like beta, you stay in the same distribution. simplifies computations\none way to think about bayesian\n\n\nMore Reading:\n\nCount Baye’s intro to bayesian statistics\n\n\n\nLecture 17: Bayesian Methods Continued (video, slides)\n\nyou can estimate your prior from the data, though some bayesians would say you’re tainting your priors and the data by doing that, but this is an accepted way to get an acceptable prior.\nthe lecture works through examples from a blog, which has collected its bayes posts into this book: Introduction to Empirical Bayes: Examples from Baseball Statistics. the explanations in the book look great.\n\nNote: Bayes is simple to do yet hard to understand. So read a number of guides/blogs/posts/youtubes till it makes sense. Some talks to see:\n\nEric J Ma Bayesian Statistical Analysis with Python PyCon 2017 - 30 min talk, uses PyMC3\nChristopher Fonnesbeck Probabilistic Programming with PyMC3 PyCon 2017 - 30min, more PyMC3"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-10-text",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-10-text",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 10: Text",
    "text": "Week 10: Text\nhw5 - did this with my group, so need to redo the hw and commit to my own github."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-11-clustering",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-11-clustering",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 11: Clustering!",
    "text": "Week 11: Clustering!"
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-12-deep-learning",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-12-deep-learning",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 12: Deep Learning",
    "text": "Week 12: Deep Learning\nUsed tensorflow and keras. update repo."
  },
  {
    "objectID": "posts/writing-about-code/harvard-cs109-notes.html#week-13-final-project-wrapup",
    "href": "posts/writing-about-code/harvard-cs109-notes.html#week-13-final-project-wrapup",
    "title": "Notes for Harvard’s Cs109 data science class",
    "section": "Week 13: Final Project & Wrapup",
    "text": "Week 13: Final Project & Wrapup\nMy final project was a proof of concept bot which ingested your bank transactions and answered questions about your money. It used wit.ai to parse user queries, machine learning to categorize transactions and straight forward scipy to crunch numbers and make graphs using matplotlib and seaborn. It was a fun learning excercise, to make something which lived briefly live on facebook messenger and could answer questions. using wit.ai made the NLP easy, though with more time writing my own NLP parser or using one of the many offline libraries would be a good learning excercise."
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html",
    "href": "posts/writing-about-code/esp8266.html",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "I’ve been wanting to try out a IOT board to do something for a while, and somebody lent me a mystery ESP8266 based board.\nSo, this is a log of my journey from never using an IOT device to making it blink lights or something.\nNote: most of the below is unneeded with Mu 1.1, it autofinds the board, flashes it and makes it easy to run code on the board itself.\n\n\nFirst up, I plugged in the board to the pc… and nothing happened, besides the a blue led coming on.\nlsusb listed a bunch of devices, out of which my IOT board seems to be: Bus 001 Device 012: ID 1a86:7523 QinHeng Electronics CH340 serial converter. I figured this out by running lsusb with it plugged and unplugged. All this tells me is that my board is using the CH340 chip to provide usb connectivity.\nFirst up, I need to know what port the device is, so running sudo dmesg has this at the end: usb 1-1: ch341-uart converter now attached to ttyUSB0.\nAnother useful tool is using usb-devices | less to list all the usb devices, and using vim commands to search.\nOk so far I can see that there is a board connected.\nSo I need esptool:\nconda create -n iot # using a new environment\nconda install -c conda-forge esptool # esptool for flashing esp boards\nMy user couldn’t access the usb port to actually use esptool, so I had to give my user these permissions:\nsudo usermod -a -G tty ko\nsudo usermod -a -G dialout ko\n, then logout and log back in.\nRunning esptool.py --port /dev/ttyUSB0 chip_id tells me:\nesptool.py v3.0\nSerial port /dev/ttyUSB0\nConnecting....\nDetecting chip type... ESP8266\nChip is ESP8266EX\nFeatures: WiFi\nCrystal is 26MHz\n...\nMicropython or Circuitpython?\nMicropython came first, and at some point adafruit forked it to make circuitpython. The three main chipsets micropython seems to be using is esp8266 and its succesor esp32, and whatever pyboard itself uses. Circuitpython seems to support a lot more boards, but they all seem to be a lot more expensive than the esp ones.\nOne big different is that circuitpython supports fstrings! I like fstrings! But overall the choice comes down to what board - the esp ones are cheap and support micropython, the adafruit and competition are expensive and support both.\n\n\nSo this has a ESP8266EX chip. So I headed over to the micropython esp8266 page and grabed the latest stable firmware, 2M of more of flash build, which seems to be the only one. As of 1.13, micropython only supports boards with 2M of flash.\nFirst up, I erased the flash: esptool.py --port /dev/ttyUSB0 erase_flash, cause why not.\nthen installed micropython by:\nesptool.py --port /dev/ttyUSB0 --baud 460800 write_flash --flash_size=detect 0 esp8266-20200911-v1.13.bin\nEureka! It detected 4MB of flash and succesfully installed. Now to test if it works by logging into the board: picocom /dev/ttyUSB0 -b115200. This gives a simple REPL interface. I tested if the board works by:\nimport machine as m\npin = m.Pin(2, m.Pin.OUT) # I read somewhere that pin 2 is an output led\npin.on()\npin.off()\nOn and off are reversed, but this works to turn on and off the led. When I changed the pin number to 3 to see what happens, the repl froze and I had to power cycle the board and reconnect.\n\n\n\n\n\n\nNow using a cli REPL is a but too old school, so of course there is a plugin for VS Code. This needs nodejs (why!) so first install node using nvm, then the extension:\nCtrl-P and ext install pycom.Pymakr to install pymakr.\nNote: Investigate https://github.com/BradenM/micropy-cli\nSo pymakr refuses to connect to the board even though it should, since the board itself is connecting and working just fine. Instead of wasting time with the pymakr extension which seems to have a lot of pending issues I switched to mu-editor.\n\n\n\nOnly mu 1.1 which is still in alpha release supports esp boards so installed that in a new env by:\nconda create -n mu python=3.7 pip # mu refuses to install on 3.8 or above\nconda activate mu\nconda install -c conda-forge esptool\n# installing mu\ngit clone https://github.com/mu-editor/mu.git\ncd mu\npip install -e \".[dev]\"\nNote: I installed mu in its own env as it uses a bunch of old packages.\nRunning mu-editor launches, it found the esp board right away with no config needed and it just works! with autocompleting ide and the repl at the bottom. So I dropped VS Code as its way overkill for the simple code needed for these devices and went with mu.\nTesting it blinks:\nimport machine as m\nimport time\n\n# set up output led\npin = m.Pin(2, m.Pin.OUT)\n\nfor _ in range(5):\n    time.sleep(0.1)\n    pin.on()\n    time.sleep(0.2)\n    pin.off()\n    \nprint(\"The light should have blinked a few times!\")\nMu soft-reboots the board, then executes the file on it. The repl has autocomplete, which is handy, but no syntax highlighting.\n\n\n\nMicropython executes boot.py on startup, then runs main.py if found. So my code should go inside a main.py file.\nMu-editor has a built in file manager but it wasn’t working on my board, so I went with ampy.\nInstall that by: pip install adafruit-ampy then find the port the board is using.\nOn a mac this lists all the serial ports in use: ls -l /dev/tty.*\nOn my machine I got /dev/tty.usbserial-1420, so ampy basics is:\n\nlist files: ampy --port /dev/cu.usbserial-1420 ls\nput a file on the board: ampy --port /dev/cu.usbserial-1420 put config.py\nget a file: ampy --port /dev/cu.usbserial-1420 get config.py\ndelete: ampy --port /dev/cu.usbserial-1420 rm config.py\n\n\n\n\n\n\n\nTo connect to wifi:\ndef do_connect(ssid=\"****\", password=\"***\", wait=5):\n    \"connects to the network and returns sta_if\"\n    import network\n    sta_if = network.WLAN(network.STA_IF)\n    \n    if not sta_if.isconnected():\n        print('connecting to network...')\n        sta_if.active(True)\n        sta_if.connect(ssid, password)\n        \n        # while loop to wait until it connects with a timeout\n        timeout = time.time() + wait   # wait seconds from now\n        while not sta_if.isconnected():\n            if time.time() &lt; timeout:\n                pass\n            else:\n                print(\"Timed out...\")\n                break\n                \n    print('network config:', sta_if.ifconfig())\n    return sta_if\n\nwifi = do_connect() # use wifi.isconnected() as needed\nRunning this is hit and miss, sometimes it connects, other times it doesn’t. I’m returning the sta_if object so I can check later on if the board is connected when doing internet things.\n\n\n\n\nThe board by itself is pretty useless. I can make it blink a light, but it has no built in sensors or outputs (besides one led). So first up I need to get some things to connect it to.\nProject ideas:\n\nfunky clock - needs display and some time transitions\ndoor bell - needs camera and a button.\n\nshould be cheaper than buying a ring, and as a bonus can use wifi to send a pic on button press.\ncould also parse video feed and save pics of movements\n\nweather display - with wind speed, water temp, tide etc\nrobot - a few sensors, motors\n\n(is an esp board powerful enough to parse simple video or do i need a pi)\n\npixel art, needs some kind of funky led lights to put somewhere\nmusic player\n\n\n\n\n\nPyCon 2019 keynote: Using MP to control LEDs\nAwesome Micropython"
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html#initial-steps",
    "href": "posts/writing-about-code/esp8266.html#initial-steps",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "First up, I plugged in the board to the pc… and nothing happened, besides the a blue led coming on.\nlsusb listed a bunch of devices, out of which my IOT board seems to be: Bus 001 Device 012: ID 1a86:7523 QinHeng Electronics CH340 serial converter. I figured this out by running lsusb with it plugged and unplugged. All this tells me is that my board is using the CH340 chip to provide usb connectivity.\nFirst up, I need to know what port the device is, so running sudo dmesg has this at the end: usb 1-1: ch341-uart converter now attached to ttyUSB0.\nAnother useful tool is using usb-devices | less to list all the usb devices, and using vim commands to search.\nOk so far I can see that there is a board connected.\nSo I need esptool:\nconda create -n iot # using a new environment\nconda install -c conda-forge esptool # esptool for flashing esp boards\nMy user couldn’t access the usb port to actually use esptool, so I had to give my user these permissions:\nsudo usermod -a -G tty ko\nsudo usermod -a -G dialout ko\n, then logout and log back in.\nRunning esptool.py --port /dev/ttyUSB0 chip_id tells me:\nesptool.py v3.0\nSerial port /dev/ttyUSB0\nConnecting....\nDetecting chip type... ESP8266\nChip is ESP8266EX\nFeatures: WiFi\nCrystal is 26MHz\n...\nMicropython or Circuitpython?\nMicropython came first, and at some point adafruit forked it to make circuitpython. The three main chipsets micropython seems to be using is esp8266 and its succesor esp32, and whatever pyboard itself uses. Circuitpython seems to support a lot more boards, but they all seem to be a lot more expensive than the esp ones.\nOne big different is that circuitpython supports fstrings! I like fstrings! But overall the choice comes down to what board - the esp ones are cheap and support micropython, the adafruit and competition are expensive and support both.\n\n\nSo this has a ESP8266EX chip. So I headed over to the micropython esp8266 page and grabed the latest stable firmware, 2M of more of flash build, which seems to be the only one. As of 1.13, micropython only supports boards with 2M of flash.\nFirst up, I erased the flash: esptool.py --port /dev/ttyUSB0 erase_flash, cause why not.\nthen installed micropython by:\nesptool.py --port /dev/ttyUSB0 --baud 460800 write_flash --flash_size=detect 0 esp8266-20200911-v1.13.bin\nEureka! It detected 4MB of flash and succesfully installed. Now to test if it works by logging into the board: picocom /dev/ttyUSB0 -b115200. This gives a simple REPL interface. I tested if the board works by:\nimport machine as m\npin = m.Pin(2, m.Pin.OUT) # I read somewhere that pin 2 is an output led\npin.on()\npin.off()\nOn and off are reversed, but this works to turn on and off the led. When I changed the pin number to 3 to see what happens, the repl froze and I had to power cycle the board and reconnect."
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html#setting-up-a-dev-env",
    "href": "posts/writing-about-code/esp8266.html#setting-up-a-dev-env",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "Now using a cli REPL is a but too old school, so of course there is a plugin for VS Code. This needs nodejs (why!) so first install node using nvm, then the extension:\nCtrl-P and ext install pycom.Pymakr to install pymakr.\nNote: Investigate https://github.com/BradenM/micropy-cli\nSo pymakr refuses to connect to the board even though it should, since the board itself is connecting and working just fine. Instead of wasting time with the pymakr extension which seems to have a lot of pending issues I switched to mu-editor.\n\n\n\nOnly mu 1.1 which is still in alpha release supports esp boards so installed that in a new env by:\nconda create -n mu python=3.7 pip # mu refuses to install on 3.8 or above\nconda activate mu\nconda install -c conda-forge esptool\n# installing mu\ngit clone https://github.com/mu-editor/mu.git\ncd mu\npip install -e \".[dev]\"\nNote: I installed mu in its own env as it uses a bunch of old packages.\nRunning mu-editor launches, it found the esp board right away with no config needed and it just works! with autocompleting ide and the repl at the bottom. So I dropped VS Code as its way overkill for the simple code needed for these devices and went with mu.\nTesting it blinks:\nimport machine as m\nimport time\n\n# set up output led\npin = m.Pin(2, m.Pin.OUT)\n\nfor _ in range(5):\n    time.sleep(0.1)\n    pin.on()\n    time.sleep(0.2)\n    pin.off()\n    \nprint(\"The light should have blinked a few times!\")\nMu soft-reboots the board, then executes the file on it. The repl has autocomplete, which is handy, but no syntax highlighting.\n\n\n\nMicropython executes boot.py on startup, then runs main.py if found. So my code should go inside a main.py file.\nMu-editor has a built in file manager but it wasn’t working on my board, so I went with ampy.\nInstall that by: pip install adafruit-ampy then find the port the board is using.\nOn a mac this lists all the serial ports in use: ls -l /dev/tty.*\nOn my machine I got /dev/tty.usbserial-1420, so ampy basics is:\n\nlist files: ampy --port /dev/cu.usbserial-1420 ls\nput a file on the board: ampy --port /dev/cu.usbserial-1420 put config.py\nget a file: ampy --port /dev/cu.usbserial-1420 get config.py\ndelete: ampy --port /dev/cu.usbserial-1420 rm config.py"
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html#board-setup",
    "href": "posts/writing-about-code/esp8266.html#board-setup",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "To connect to wifi:\ndef do_connect(ssid=\"****\", password=\"***\", wait=5):\n    \"connects to the network and returns sta_if\"\n    import network\n    sta_if = network.WLAN(network.STA_IF)\n    \n    if not sta_if.isconnected():\n        print('connecting to network...')\n        sta_if.active(True)\n        sta_if.connect(ssid, password)\n        \n        # while loop to wait until it connects with a timeout\n        timeout = time.time() + wait   # wait seconds from now\n        while not sta_if.isconnected():\n            if time.time() &lt; timeout:\n                pass\n            else:\n                print(\"Timed out...\")\n                break\n                \n    print('network config:', sta_if.ifconfig())\n    return sta_if\n\nwifi = do_connect() # use wifi.isconnected() as needed\nRunning this is hit and miss, sometimes it connects, other times it doesn’t. I’m returning the sta_if object so I can check later on if the board is connected when doing internet things."
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html#do-something",
    "href": "posts/writing-about-code/esp8266.html#do-something",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "The board by itself is pretty useless. I can make it blink a light, but it has no built in sensors or outputs (besides one led). So first up I need to get some things to connect it to.\nProject ideas:\n\nfunky clock - needs display and some time transitions\ndoor bell - needs camera and a button.\n\nshould be cheaper than buying a ring, and as a bonus can use wifi to send a pic on button press.\ncould also parse video feed and save pics of movements\n\nweather display - with wind speed, water temp, tide etc\nrobot - a few sensors, motors\n\n(is an esp board powerful enough to parse simple video or do i need a pi)\n\npixel art, needs some kind of funky led lights to put somewhere\nmusic player"
  },
  {
    "objectID": "posts/writing-about-code/esp8266.html#resources",
    "href": "posts/writing-about-code/esp8266.html#resources",
    "title": "starting out with micropython on a esp8266",
    "section": "",
    "text": "PyCon 2019 keynote: Using MP to control LEDs\nAwesome Micropython"
  },
  {
    "objectID": "posts/writing-about-code/apache-spark.html",
    "href": "posts/writing-about-code/apache-spark.html",
    "title": "Apache Spark",
    "section": "",
    "text": "Way back in the good old days Google came up with mapreduce to store and crunch data scattered on disks across multiple machines. The world looked at mapreduce and thought it good, and made an open source implementation called Hadoop.\nPpl wanted more, and faster, and luckily memory prices went down. And thus Apache Spark was born at UC Berkely. Spark stores data across a bunch of machines in memory. This makes it easy and fast to data scientist all the data.\n\n\nThe key buzzword here is RDD which stands for resilient distributed data set. Spark has a SparkContext object which manages the connection to clusters and how processes are run on those clusters. So we call it using code like data = sc.textFile(``'``name_of_text_file.txt``'``). With text data, this just becomes a list of strings - one per each line in the text file which is accessed using sc.take(num_of_lines).\nNow, even though this looks very pythonic, don’t think of spark data objects which look like lists as lists. Python lists are meant to be run on a single machine, so they can be sliced and diced. Spark data is designed to be used across machines, so slices don’t make sense in that context.\nRDD’s are immutable - so to modify data stored inside it, we get back a new RDD. Python has both mutable objects - like lists and dictionaries - and immutable ones like tuples, so this isn’t anything different from mainstream python.\n\n\n\nmap(func) applies the function func to all the things in the RDD and returns a new RDD. so something like data.map(lambda line: line.split('\\t')) applies the func passed into map to each object stored in the RDD and returns back a new RDD.\nmap is often accompanies by reduceByKey e.g to count stuff:\nlines = sc.textFile(\"data.txt\")\npairs = lines.map(lambda s: (s, 1))\ncounts = pairs.reduceByKey(lambda a, b: a + b)\nNow, spark deals with transformations only when it has to - so it builds up a pipelines of tasks and runs them when the output of those tasks is called.\n\n\n\nUse Spark on a local machine is useful for learning purposes - for actual production use I would use Amazon EMR or Google Cloud Dataproc.\nSpark needs Java, so to install it properly it needs a lot of installation. So to avoid all that I’m using a docker image with spark preinstalled: jupyter/pyspark-notebook\ndocker run -d -P --name notebook jupyter/pyspark-notebook\nNote: use -v to mount the current directory in the docker container."
  },
  {
    "objectID": "posts/writing-about-code/apache-spark.html#rdd",
    "href": "posts/writing-about-code/apache-spark.html#rdd",
    "title": "Apache Spark",
    "section": "",
    "text": "The key buzzword here is RDD which stands for resilient distributed data set. Spark has a SparkContext object which manages the connection to clusters and how processes are run on those clusters. So we call it using code like data = sc.textFile(``'``name_of_text_file.txt``'``). With text data, this just becomes a list of strings - one per each line in the text file which is accessed using sc.take(num_of_lines).\nNow, even though this looks very pythonic, don’t think of spark data objects which look like lists as lists. Python lists are meant to be run on a single machine, so they can be sliced and diced. Spark data is designed to be used across machines, so slices don’t make sense in that context.\nRDD’s are immutable - so to modify data stored inside it, we get back a new RDD. Python has both mutable objects - like lists and dictionaries - and immutable ones like tuples, so this isn’t anything different from mainstream python."
  },
  {
    "objectID": "posts/writing-about-code/apache-spark.html#map-all-the-things-to-something-else",
    "href": "posts/writing-about-code/apache-spark.html#map-all-the-things-to-something-else",
    "title": "Apache Spark",
    "section": "",
    "text": "map(func) applies the function func to all the things in the RDD and returns a new RDD. so something like data.map(lambda line: line.split('\\t')) applies the func passed into map to each object stored in the RDD and returns back a new RDD.\nmap is often accompanies by reduceByKey e.g to count stuff:\nlines = sc.textFile(\"data.txt\")\npairs = lines.map(lambda s: (s, 1))\ncounts = pairs.reduceByKey(lambda a, b: a + b)\nNow, spark deals with transformations only when it has to - so it builds up a pipelines of tasks and runs them when the output of those tasks is called."
  },
  {
    "objectID": "posts/writing-about-code/apache-spark.html#install-spark-on-your-local-machine",
    "href": "posts/writing-about-code/apache-spark.html#install-spark-on-your-local-machine",
    "title": "Apache Spark",
    "section": "",
    "text": "Use Spark on a local machine is useful for learning purposes - for actual production use I would use Amazon EMR or Google Cloud Dataproc.\nSpark needs Java, so to install it properly it needs a lot of installation. So to avoid all that I’m using a docker image with spark preinstalled: jupyter/pyspark-notebook\ndocker run -d -P --name notebook jupyter/pyspark-notebook\nNote: use -v to mount the current directory in the docker container."
  },
  {
    "objectID": "posts/kid-friendly-hikes-in-sydney.html",
    "href": "posts/kid-friendly-hikes-in-sydney.html",
    "title": "Short hikes in Sydney",
    "section": "",
    "text": "Hermitage Foreshore track\nhttps://www.nationalparks.nsw.gov.au/things-to-do/walking-tracks/hermitage-foreshore-track\nA lovely 5k hike. Good chunks of it feel like you are really out in the bush, and as a bonus there are 3-4 small beaches to swim at.\nPark at Nielsen Park, walk to Bottle and Glass Point and start the hike from there."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KO",
    "section": "",
    "text": "Quarto Jupyter notebook test\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe Naked Don’t Fear the Water\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\n\n\n\n\n\n\n\n\n\nApr 21, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nPaneer Beetroot Biryani\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nZucchini pasta\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nMar 1, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nGhachar Ghochar\n\n\n\n\n\n\n\nbooks\n\n\nindia\n\n\n\n\n\n\n\n\n\n\n\nFeb 28, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nA Passage North\n\n\n\n\n\n\n\nbooks\n\n\nsri lanka\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2022\n\n\nAnuk Arudpragasam\n\n\n\n\n\n\n  \n\n\n\n\nTermination Shock\n\n\n\n\n\n\n\nbooks\n\n\nclimate\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAloo Keema\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nJan 13, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAloo Anda ka salan\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nBhaigan ka burta\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nKhata Bhaigan\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nIndian Summer: The Secret History of the End of an Empire\n\n\n\n\n\n\n\nbooks\n\n\nhistory\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe Key Man\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSveltekit with Tailwindcss\n\n\n\n\n\n\n\nwebdev\n\n\n\n\nbuilding an interactive website\n\n\n\n\n\n\nAug 24, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nTailwindCss\n\n\n\n\n\n\n\nwebdev\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nShort hikes in Sydney\n\n\n\n\n\n\n\nhikes\n\n\nsydney\n\n\n\n\n\n\n\n\n\n\n\nAug 6, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAloo Sabzi\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nMung Beans Salad\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nMacOS\n\n\n\n\n\n\n\nmac\n\n\n\n\nsetup notes for osx\n\n\n\n\n\n\nJul 26, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe Uninhabitable Earth\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\nclimate change\n\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nToo Migrant, Too Muslim, Too Loud\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\naustralia\n\n\n\n\n\n\n\n\n\n\n\nJul 8, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nQuarterly Essay 81: Getting to Zero - Australia’s Energy Transition\n\n\n\n\n\n\n\nbooks\n\n\nenergy\n\n\n\n\n\n\n\n\n\n\n\nJun 2, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nLilygo ttgo t5\n\n\n\n\n\n\n\niot\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nstarting out with micropython on a esp8266\n\n\n\n\n\n\n\niot\n\n\n\n\n\n\n\n\n\n\n\nJan 8, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nUsing Anaconda\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nJan 2, 2021\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nExercise\n\n\n\n\n\n\n\nfitness\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nhow the blog was built, python edition\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 11, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nWindows 10\n\n\n\n\n\n\n\nwindows\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe Three Body Problem\n\n\n\n\n\n\n\nbooks\n\n\nscience fiction\n\n\n\n\n\n\n\n\n\n\n\nOct 19, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nKheera ka raita\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nDhal Fry\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nChana Masala aka Spicy Chickpeas\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2015\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\na verbose solve of AOC 2015\n\n\n\n\n\n\nAug 10, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSprouted Daal (lentils)\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nVegan Shami kebab\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nVisual Studio Code\n\n\n\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nOct 10, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nPyCon Australia 2019\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes for PyconAU 2019.\n\n\n\n\n\n\nSep 20, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nJupyter Lab tips and tricks\n\n\n\n\n\n\n\njupyter\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nDesi Omelette\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nPaneer Karhai\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nDaal (lentils)\n\n\n\n\n\n\n\nrecipes\n\n\n\n\n\n\n\n\n\n\n\nAug 4, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nStatistics Done Wrong: The Woefully Complete Guide\n\n\n\n\n\n\n\nbooks\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nMay 5, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nData Science blogs\n\n\n\n\n\n\n\ndata science\n\n\n\n\nuseful data science related blogs\n\n\n\n\n\n\nMar 4, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nGoogles Best Practices for ML Engineering\n\n\n\n\n\n\n\nml\n\n\n\n\nmy read through notes on googles ml guide\n\n\n\n\n\n\nMar 2, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2016\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes on some of the AOC16 puzzles\n\n\n\n\n\n\nFeb 2, 2019\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nMITx 6.00.2X: Intro to Computational Thinking and Data Science\n\n\n\n\n\n\n\ncourses\n\n\npython\n\n\n\n\na great intro course to python\n\n\n\n\n\n\nSep 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nPyCon Australia 2018\n\n\n\n\n\n\n\npython\n\n\n\n\nNotes for PyconAU 2018.\n\n\n\n\n\n\nSep 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSetup Linux\n\n\n\n\n\n\n\nlinux\n\n\n\n\nsetup notes for debian/ubuntu\n\n\n\n\n\n\nSep 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSML 2018-09: Deep Reinforcement Learning\n\n\n\n\n\n\n\nmeetup\n\n\ndata science\n\n\n\n\nnotes from the meetup\n\n\n\n\n\n\nSep 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSetting up Linux on a Chromebook with Crostini\n\n\n\n\n\n\n\nchromebooks\n\n\nlinux\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSML 2018-08: Deep Reinforcement Learning\n\n\n\n\n\n\n\nmeetup\n\n\ndata science\n\n\n\n\nnotes from the meetup\n\n\n\n\n\n\nAug 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nFloodfill algorithim\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nGrokking Deep Learning by Andrew Trask\n\n\n\n\n\n\n\nbooks\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nTechfugees Sydney Hackathon 2018\n\n\n\n\n\n\n\nmeetup\n\n\n\n\nhackathon notes\n\n\n\n\n\n\nJul 20, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nDSS-2018-07: Michael Allwright and Inna Kolyshkina\n\n\n\n\n\n\n\nmeetup\n\n\n\n\nnotes taken during/after this meetup\n\n\n\n\n\n\nJul 12, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nMicrosoft OpenHack 2018 Sydney\n\n\n\n\n\n\n\ndata science\n\n\nmeetup\n\n\n\n\nincomplete notes from openhack\n\n\n\n\n\n\nJun 14, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe Fifth Risk\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nfastai Practical Deep Learning For Coders, Part 1, 2018\n\n\n\n\n\n\n\ncourses\n\n\ndeep learning\n\n\n\n\nnotes taken during/after this meetup\n\n\n\n\n\n\nMay 5, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\ndeeplearning.ai: Structuring Machine Learning Projects\n\n\n\n\n\n\n\ncourses\n\n\n\n\nnotes for part 3 of the deeplearning.ai course\n\n\n\n\n\n\nJan 1, 2018\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nUdacity ND101: Deep Learning Nanodegree\n\n\n\n\n\n\n\ncourses\n\n\ndeep learning\n\n\n\n\nnotes from the course\n\n\n\n\n\n\nAug 15, 2017\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nNotes for Harvard’s Cs109 data science class\n\n\n\n\n\n\n\npython\n\n\ndata science\n\n\n\n\nNotes as I worked through CS109.\n\n\n\n\n\n\nApr 4, 2017\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nGooogle Chrome\n\n\n\n\n\n\n\napps\n\n\n\n\n\n\n\n\n\n\n\nMar 9, 2017\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\ndeeplearning.ai: Improving Deep Neural Networks\n\n\n\n\n\n\n\ncourses\n\n\n\n\nmy notes on part 2 of the deeplearning.ai course\n\n\n\n\n\n\nJan 1, 2017\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nApache Spark\n\n\n\n\n\n\n\ndata science\n\n\n\n\nstarted some notes on Spark, but didn’t get very far…\n\n\n\n\n\n\nJan 1, 2016\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\ndeeplearning.ai: Neural Networks and Deep Learning\n\n\n\n\n\n\n\ncourses\n\n\n\n\n\n\n\n\n\n\n\nJan 1, 2016\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nUdacity CS101: Intro to CS\n\n\n\n\n\n\n\ncourses\n\n\npython\n\n\n\n\ncourse review\n\n\n\n\n\n\nJul 10, 2013\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nFreedom at Midnight\n\n\n\n\n\n\n\nbooks\n\n\nnonfiction\n\n\n\n\n\n\n\n\n\n\n\nMay 13, 2013\n\n\nKO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/chrome.html",
    "href": "posts/chrome.html",
    "title": "Gooogle Chrome",
    "section": "",
    "text": "Notes on making Google chrome work for me. Needs updating for 2020.\nThe two top things:"
  },
  {
    "objectID": "posts/chrome.html#flags",
    "href": "posts/chrome.html#flags",
    "title": "Gooogle Chrome",
    "section": "Flags",
    "text": "Flags\nSome of these flags get set by\n\nQuiet notifications: chrome://flags/#quiet-notification-prompts kills the annoying notifications popups, just displays a bell in the notification bar instead. You can also choose to just block them altogether, but I like this compromise.\nDark Mode: chrome://flags/#enable-force-dark forces dark mode\n\nMake tab titles prominent in dark mode: chrome://flags/#prominent-dark-mode-active-tab-title"
  },
  {
    "objectID": "posts/chrome.html#extensions",
    "href": "posts/chrome.html#extensions",
    "title": "Gooogle Chrome",
    "section": "Extensions",
    "text": "Extensions\nBlock adds: uBlock Origin is essential, can’t use computers without it anymore. Click here on the chrome web store link to install.\nVim style keyboard shortcuts: Vimium"
  },
  {
    "objectID": "posts/exercise.html",
    "href": "posts/exercise.html",
    "title": "Exercise",
    "section": "",
    "text": "Exercise\nMany ways to get fit. This is mine."
  },
  {
    "objectID": "posts/exercise.html#at-home-routine",
    "href": "posts/exercise.html#at-home-routine",
    "title": "Exercise",
    "section": "At home routine",
    "text": "At home routine\n\n\n\nWarmup\n\n\n\n\n\n\nYuri’s Shoulder Band Warmup\n5-10\n\n\n\nSquat Sky Reaches\n5-10\n\n\n\nGMB Wrist Prep\n10\n\n\n\nDeadbugs\n30s\n\n\n\nArch Hangs\n10s\n\n\n\nSupport Hold\n30s\n\n\n\nBeginner Shrimp Squat\n10\n\n\n\nSingle Legged Deadlift\n10\n\n\n\nMain Exercises\n\nAlternate b/w two main exercises at a time\n\n\nChin upsSquat: kb Squat\n3x5-83x5-8\npullup  if no kb a single leg squat like pistol or shrimp\n\n\nDips - rings or barHinge: kb swing\n3x5-8\ngoblet squatSingle leg deadlift if no kb\n\n\nRowPushup\n3x5-83x6-10\nrow with gymnastic rings\n\n\nTurkish getup (extra)\n10\nEric\n\n\nCore triplet\n\n\n\n\nRing rolloutsPallof press (banded or ring)reverse hyperextension\n3x8\nor a 30second planksuperman on floorAntraniks video of all 3\n\n\nWarm down\n\n\n\n\nFoam roll\n\n\n\n\na few random yoga stretches.\n\nto be figured out…\n\n\n\nEquipment needed:\n\nRings, height adjustable to transition for pullups, dips and rows.\n12 and 18kg kettlebell (any cast iron one.)\n\nNotes:\n\n3x5 means 3 sets of 5 reps\nuse a foam roller before and after\n\nSources:\n\nreddit bodyweight routine\nEnter the kettlebell"
  },
  {
    "objectID": "posts/exercise.html#gym-routine",
    "href": "posts/exercise.html#gym-routine",
    "title": "Exercise",
    "section": "Gym routine",
    "text": "Gym routine\nI started with starting strengths 5x5 program, but on reading more have picked up Ivysaurs program:\n\nAfter this, consider:\n\nGZCLP"
  },
  {
    "objectID": "posts/exercise.html#travel-routine",
    "href": "posts/exercise.html#travel-routine",
    "title": "Exercise",
    "section": "Travel Routine",
    "text": "Travel Routine\nSometimes you got no stuff. Then:\n\ndo the warm-up as above.\nrun!\na few pushups, planks, whatever.\n\nDone."
  },
  {
    "objectID": "posts/writing-about-code/mitx6002x.html",
    "href": "posts/writing-about-code/mitx6002x.html",
    "title": "MITx 6.00.2X: Intro to Computational Thinking and Data Science",
    "section": "",
    "text": "MITx 6.00.2X: Intro to Computational Thinking and Data Science\nI really liked this class - it was a engaging course to do after having done some intro level stuff.\n\n6.00.2x will teach you how to use computation to accomplish a variety of goals and provides you with a brief introduction to a variety of topics in computational problem solving . This course is aimed at students with some prior programming experience in Python and a rudimentary knowledge of computational complexity. You will spend a considerable amount of time writing programs to implement the concepts covered in the course. For example, you will write a program that will simulate a robot vacuum cleaning a room or will model the population dynamics of viruses replicating and drug treatments in a patient’s body.\n\nSome of the topics covered:\n\nAdvanced programming in Python 3\nKnapsack problem, Graphs and graph optimization\nDynamic programming\nPlotting with the pylab package\nRandom walks\nProbability, Distributions\nMonte Carlo simulations\nCurve fitting\nStatistical fallacies"
  },
  {
    "objectID": "posts/writing-about-code/fastai-part1-2018.html",
    "href": "posts/writing-about-code/fastai-part1-2018.html",
    "title": "fastai Practical Deep Learning For Coders, Part 1, 2018",
    "section": "",
    "text": "fastai Practical Deep Learning For Coders, Part 1, 2018\nThis is a great top down introduction to deep learning. Jeremey Howard is a great teacher. This is probably the best AI course on the interwebs. The fast.ai course websites blurb:\n\nThis 7-week course is designed for anyone with at least a year of coding experience, and some memory of high-school math. You will start with step one—learning how to get a GPU server online suitable for deep learning—and go all the way through to creating state of the art, highly practical, models for computer vision, natural language processing, and recommendation systems.\n\n2020: The course has been greatly updated since I did it, and the libray has evolved a lot. So its even more awesome now!"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html",
    "href": "posts/writing-about-code/udacity-nd101.html",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "",
    "text": "My notes & files for Udacity’s Deep Learning intro course.\n\n\n\nRead Machine learning is fun\nWatch Andrew NG: Deep Learning in Practice (34 minutes)\nGo though UD730 deep learning course on Udacity\nWatch Learn tensorflow 3 hour video\nhttp://rodneybrooks.com/patrick-winston-explains-deep-learning/\n\n\n\nThe course recommneds knowing basic python from here, but I found the following two resources better:\n\nA whirlwind tour of Python\nPython Data Science Handbook\n\n\n\n\nNeed to know multivariable calculus & linear algebra.\n\nKhan Academy Multivariable calculus\nLinear algebra youtube playlist or over at lemma\nKhan Academy’s Linear Algebra Intro\n\n\n\n\n\nGrokking Deep Learning by Andrew Trask. This provides a very gentle introduction to Deep Learning and covers the intuition more than the theory.\nNeural Networks And Deep Learning by Michael Nielsen. This book is more rigorous than Grokking Deep Learning and includes a lot of fun, interactive visualizations to play with.\nThe Deep Learning Textbook from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This online book has lot of material and is the most rigorous of the three books suggested."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#before-the-course-starts",
    "href": "posts/writing-about-code/udacity-nd101.html#before-the-course-starts",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "",
    "text": "Read Machine learning is fun\nWatch Andrew NG: Deep Learning in Practice (34 minutes)\nGo though UD730 deep learning course on Udacity\nWatch Learn tensorflow 3 hour video\nhttp://rodneybrooks.com/patrick-winston-explains-deep-learning/\n\n\n\nThe course recommneds knowing basic python from here, but I found the following two resources better:\n\nA whirlwind tour of Python\nPython Data Science Handbook\n\n\n\n\nNeed to know multivariable calculus & linear algebra.\n\nKhan Academy Multivariable calculus\nLinear algebra youtube playlist or over at lemma\nKhan Academy’s Linear Algebra Intro\n\n\n\n\n\nGrokking Deep Learning by Andrew Trask. This provides a very gentle introduction to Deep Learning and covers the intuition more than the theory.\nNeural Networks And Deep Learning by Michael Nielsen. This book is more rigorous than Grokking Deep Learning and includes a lot of fun, interactive visualizations to play with.\nThe Deep Learning Textbook from Ian Goodfellow, Yoshua Bengio, and Aaron Courville. This online book has lot of material and is the most rigorous of the three books suggested."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#backpropagation",
    "href": "posts/writing-about-code/udacity-nd101.html#backpropagation",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Backpropagation",
    "text": "Backpropagation\nThis is the key to understanding neural nets, so it’s important to understand how Backpropagation works.\n\nCS231n Winter 2016 Lecture 4: Backpropagation"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#project-1-predictin-bike-sharing-demand-from-historical-data",
    "href": "posts/writing-about-code/udacity-nd101.html#project-1-predictin-bike-sharing-demand-from-historical-data",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Project 1: Predictin Bike Sharing demand from historical data",
    "text": "Project 1: Predictin Bike Sharing demand from historical data\nFinal Notebook"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#model-evaluation-and-validation",
    "href": "posts/writing-about-code/udacity-nd101.html#model-evaluation-and-validation",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Model Evaluation and Validation",
    "text": "Model Evaluation and Validation\nGeneralization is better than overfitting.\nR2 score - simplest possible model is to take the avg of all values and draw a straight line, then calculate the mean square error - the R2 score is 1 minus the error of our regression model divided by the error of the simplest possible model - if we have a good model, the error will be small compared to the simple model, thus R2 will be close to 1 - for a bad model, the ratio of errors will be closer to 1, giving a small R2 values\nfrom sklearn.metrics import r2_score\n\nTwo types of error\n\noverfitting\nunderfitting"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#sentiment-analysis-with-andrew-trask",
    "href": "posts/writing-about-code/udacity-nd101.html#sentiment-analysis-with-andrew-trask",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Sentiment Analysis with Andrew Trask",
    "text": "Sentiment Analysis with Andrew Trask\nThis was a good project - built a simple neural network to classify reviews as negative or positive."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#intro-to-tflearn",
    "href": "posts/writing-about-code/udacity-nd101.html#intro-to-tflearn",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Intro to TFLearn",
    "text": "Intro to TFLearn\nSigmoid activation functions have a max derivate of .25, so errors shrink by 75% or more during backprogation. This means the neural network takes a long time to train. Instead of sigmoid, most DL networks use RLU - which is a supersimple function which outputs max(input, 0). For a +ve inut the output equals the input, and for a -ve input the output is 0. Relu nodes can die if there is a large graident through them, so they are best used with a small learning rate.\nFor a simple binary classification, the sigmoid function works, but for mulitple outputs, for example reconigizing digits, use the softmax function. A softmax function squashes outputs to be b/w 0 and 1 and divides them such that the total sum of the output equals 1.\none hot encoding means using a simple vector, like y=[0,0,0,0,1,0,0,0,0,0] to represent 4.\nhttp://tflearn.org/"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#rnn",
    "href": "posts/writing-about-code/udacity-nd101.html#rnn",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "RNN",
    "text": "RNN\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/ http://karpathy.github.io/2015/05/21/rnn-effectiveness/"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#using-tflearn-to-do-sentiment-analysis",
    "href": "posts/writing-about-code/udacity-nd101.html#using-tflearn-to-do-sentiment-analysis",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "using tflearn to do sentiment Analysis",
    "text": "using tflearn to do sentiment Analysis"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#sirajs-math-for-deep-learning",
    "href": "posts/writing-about-code/udacity-nd101.html#sirajs-math-for-deep-learning",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Siraj’s Math for Deep learning",
    "text": "Siraj’s Math for Deep learning\nNeed to know statistics, linear algebra and calculus. C"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#miniflow",
    "href": "posts/writing-about-code/udacity-nd101.html#miniflow",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Miniflow",
    "text": "Miniflow\nbuilt a simple graph based nn\nan alternative to Amazon’s EC2 gpu machines: floyd"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#intro-to-tensorflow",
    "href": "posts/writing-about-code/udacity-nd101.html#intro-to-tensorflow",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Intro to Tensorflow",
    "text": "Intro to Tensorflow\nDeep learning is a family of techniques which adapts to all sorts of data and problems. the basic techiniques of DL apply to a bunch of diff fields. Neural Networks have been around for decades but had pretty much disappread from the CS science. They came back in a bigway in the 2010’s with advances in speech reconizition, computer vision and machine translation. This was enabled by lots of data and cheap gpus.\nAll the hotness is in my intro to tensorflow notebook."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#convolution-networks-or-cnn",
    "href": "posts/writing-about-code/udacity-nd101.html#convolution-networks-or-cnn",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Convolution Networks, or CNN",
    "text": "Convolution Networks, or CNN\nA CNN breaks up an image into many pieces and learns to first reconigzie basic shapes, lines, curves, then the more complex objects as combinations of the simpler shapes, then classifies the image by combining the complex objects together. A CNN can have many layers, with each layer capturing a different level of complexity.\nResources:\n\nIt seems the time is now to read this book on neural networks and go through cs231.\nThis article is a nice simple overview of neural networks and builds a simple covnet network using keras.\nA beginers guide to understanding n=cnovlutional networks"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#sirajs-image-classification",
    "href": "posts/writing-about-code/udacity-nd101.html#sirajs-image-classification",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Siraj’s Image Classification",
    "text": "Siraj’s Image Classification"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#project-2",
    "href": "posts/writing-about-code/udacity-nd101.html#project-2",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Project 2",
    "text": "Project 2\nGoal is to Classify images from the CIFAR dataset.\nA good time to watch this video intro to Tensorflow.\nMy final project notebook.\nNote: I only got 65% accuracy, but that was at 20 epochs. Running it at a hundred or so shuold bump up the accuracy over 70%, but I got tired of waiting for the model to train."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#intro-to-recurrent-neural-networks",
    "href": "posts/writing-about-code/udacity-nd101.html#intro-to-recurrent-neural-networks",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Intro to Recurrent Neural Networks",
    "text": "Intro to Recurrent Neural Networks\nhttp://colah.github.io/posts/2015-08-Understanding-LSTMs/"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#aside",
    "href": "posts/writing-about-code/udacity-nd101.html#aside",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "aside",
    "text": "aside\nDeep Learning chat https://deeplearning4j.org/word2vec"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#embeddings-and-word2vec",
    "href": "posts/writing-about-code/udacity-nd101.html#embeddings-and-word2vec",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Embeddings and Word2vec",
    "text": "Embeddings and Word2vec\nWord2Vec\nNotebook: https://github.com/udacity/deep-learning/tree/master/embeddings"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#tensorboard",
    "href": "posts/writing-about-code/udacity-nd101.html#tensorboard",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "TensorBoard",
    "text": "TensorBoard\nThis is really useful to see the initial model, and then to see what is happening while it’s training.\n\ntensorboard intro\nbasic tensorboard intro\n\nCan also be used for hyperparameter search by selecting diff combinations of parameters, writing them to a logstring and viewing the diff runs in tensorboard all nicely charted out.\ntodo: make a simple mnist NN with tensorboard summaries of different parameters. something like:\nfor lstm_size in [128,256,512]:\n    for num_layers in [1, 2]:\n        for learning_rate in [0.002, 0.001]:\n            log_string = 'logs/4/lr={},rl={},ru={}'.format(learning_rate, num_layers, lstm_size)\n            writer = tf.summary.FileWriter(log_string)"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#how-to-make-a-text-summarizer",
    "href": "posts/writing-about-code/udacity-nd101.html#how-to-make-a-text-summarizer",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "how to make a text summarizer",
    "text": "how to make a text summarizer\n….."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#weight-initialization",
    "href": "posts/writing-about-code/udacity-nd101.html#weight-initialization",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Weight Initialization",
    "text": "Weight Initialization\nThe initial value of weights is very important to how well a NN trains. If all the weights start of the same, it makes it hard to update the weights since they end up all giving similar outputs, making it hard for the NN to learn.\nRandom weights work much better. Tensorflow’s tf.random_uniform() is a good weight initilization function. Be default, tf.random.uniform picks float values uniformly spread b/w 0 and 1, which is decent, but we can do better.\nSmart people have tested out different initial weights, and it seems using weights distributed around zero with a std dev around 0.1 works well, with tails cut off. Tensorflow has a built in function:\ntf.truncated_normal which generates a random normal distribution around zero with values &gt; 2 std devs from the mean repicked. So making 3 weights would look someething like:\nweights = [\n    tf.Variable(tf.truncated_normal(layer_1_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_2_weight_shape, stddev=0.1)),\n    tf.Variable(tf.truncated_normal(layer_3_weight_shape, stddev=0.1))\nA normal distribution means that the weights will tend to be closer to the mean rather than uniformly distributed.\ntodo: run a simple network on MNIST with different weights and see in tensorboard todo: find some approchable resources on weights\n\nweight initilizaiton resources\n\ncs231n weight initilization - supports the above method of small random weights centered on zero, but warns that smaller numbers aren’t always better."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#sentiment-prediction-rnn",
    "href": "posts/writing-about-code/udacity-nd101.html#sentiment-prediction-rnn",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Sentiment Prediction RNN",
    "text": "Sentiment Prediction RNN"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#project-3---generate-a-tv-script",
    "href": "posts/writing-about-code/udacity-nd101.html#project-3---generate-a-tv-script",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Project 3 - Generate a TV Script",
    "text": "Project 3 - Generate a TV Script\nThis was an interesting project - to generate new text after training a RNN network on a subset of Simpsons scripts, set in Moe’s Cavern.\nCompleted project 3 notebook"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#transfer-learning",
    "href": "posts/writing-about-code/udacity-nd101.html#transfer-learning",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Transfer Learning",
    "text": "Transfer Learning\nTraining nerual networks can take a long time so we can take an exisiting pretrained network and use that to extract features or as the initial network to further build upon. Here, we use VGGNet.\nI’m using a pretrained VGG network from here."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#language-translation",
    "href": "posts/writing-about-code/udacity-nd101.html#language-translation",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Language Translation",
    "text": "Language Translation"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#sequence-to-sequence",
    "href": "posts/writing-about-code/udacity-nd101.html#sequence-to-sequence",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Sequence to Sequence",
    "text": "Sequence to Sequence\nis a kind of RNN network.\ntf.contrib.seq2seq\nUseful posts - Text Summarization with Amazon Reviews - Seq2Seq talk - Stanford lectures on NLP using deep learning"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#how-to-make-a-chatbot",
    "href": "posts/writing-about-code/udacity-nd101.html#how-to-make-a-chatbot",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "How to make a chatbot",
    "text": "How to make a chatbot\nTrain a DNN to learn text and answers. Facebook has some datasets to learn on.\nUsing Keras it’s relative straightforward. See this implementation."
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#reinforcement-learning",
    "href": "posts/writing-about-code/udacity-nd101.html#reinforcement-learning",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Reinforcement learning",
    "text": "Reinforcement learning\nUse rewards when things be done right. Here we look at Q-Learning. Using negative rewards for bad stuff, and postiive rewards for good stuff, you train an agent to seek out the max reward path, using a q-table which stores rewards for each state and action possible from that state.\nOpenAI Gym has lots of implementations.\nbasic RL"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#project-4---translation",
    "href": "posts/writing-about-code/udacity-nd101.html#project-4---translation",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "Project 4 - Translation",
    "text": "Project 4 - Translation\nuse a rnn to translate english to french.\nProject is built on TF 1.0, so see release notes for later versions as TF changed some RNN bits.\nI had a problem with training times. Even on a GPU it took forever. This is with relatively limited data, with a decently sized data it will take a looong time!\nQuestions to ponder:\n\nshould the encoding & decoding embed size be exactly the vocab size?\n\nParameters which should give &gt; 90% accuracy:\n\nepochs = 3\nbatch_size = 256\nrnn_size = 128\nnum_layers = 2\nencoding_embedding_size = 256\ndecoding_embedding_size = 256\nlearning_rate = 0.005\nkeep_probability = 0.9"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#gans",
    "href": "posts/writing-about-code/udacity-nd101.html#gans",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "GANS",
    "text": "GANS\nGans use a differentiable function represented by a NN to transform an output from an input (generally random noise?).\nA second network, called the discriminator, which is just a regular NN classifier which has been trained on real images, gives the generated image a probablity of being real or fake. The discriminator receives a mix of real images and generated images.\nOver time, the generator gets good at generating images which passes the discrimator’s test.\nGame theory tells us that the two NN’s should come to a equilibrium where neither NN can improve their situation.\nWe train both networks alternately - look up best practices.\n\nGAN by Example using Keras\nanother blog post\nOriginal GAN paper\nOpenAI GAN explainer\nQuora GANs topic\n\n\nBatch normalization\nThis greatly aids deep networks to learn faster, as well so stops them flaking out and killing too many neurons.\n\n\nFace generation project\nA GAN which generates mnist images and faces. Relatively straigtforward, but adding in batch normalization involves some tensorflow kungfu. Doing this project in TF made me yearn for Keras.\nSpecifically, the batch_norm layer in TF isn’t magical enough, so we have to wrap the train operations inside tf.control_dependencies block so the batch normalization layers get updated. The below is the code from Udacty but since I added batch norm to both the generator and the discriminator this didn’t work for me:\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\nAfter much googling and help from the forums, this worked:\n# need this to make batch normalization work properly during inference\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    g_updates  = [opt for opt in update_ops if opt.name.startswith('generator')]\n    \n    with tf.control_dependencies(g_updates):\n        d_train_opt = tf.train.AdamOptimizer(learning_rate, beta1).minimize(d_loss, var_list=d_vars)\n        g_train_opt = tf.train.AdamOptimizer(learning_rate, beta1).minimize(g_loss, var_list=g_vars)\nPutting this here becuase there must be better/simpler easier way to do batch_norm without getting into the graph bowels of tensorflow.\ntodo: reimplement in Keras\nProject notebok"
  },
  {
    "objectID": "posts/writing-about-code/udacity-nd101.html#all-done",
    "href": "posts/writing-about-code/udacity-nd101.html#all-done",
    "title": "Udacity ND101: Deep Learning Nanodegree",
    "section": "All done!",
    "text": "All done!\nLink to the official course certificate."
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html",
    "href": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html",
    "title": "deeplearning.ai: Structuring Machine Learning Projects",
    "section": "",
    "text": "This course covers how to think about and improve machine learning systems.\n\nYou will learn how to build a successful machine learning project. If you aspire to be a technical leader in AI, and know how to set direction for your team’s work, this course will show you how.\n\nCourse Resources\n\nCourse home\nDiscussion forum\nWeek 1: ML Strategy\n\nOrthogonalization\nSetting up your goal\n\nSingle number evaluation metric\nSatisficing and Optimizing metric\nTrain/dev/test distributions\nSize of the dev and test sets\nWhen to change dev/test sets and metrics\n\nComparing to human-level performance\n\nWhy human-level performance?\nAvoidable bias\nUnderstanding human-level performance\nSurpassing human-level performance\nImproving your model performance\n\nAndrej Karpathy interview\n\nWeek 2: More ML Strategy\n\nError Analysis\n\nCarrying out error analysis\nCleaning up incorrectly labeled data\nBuild your first system quickly, then iterate\n\nMismatched training and dev/test data\n\nTraining and testing on different distributions\nBias and Variance with mismatched data distributions\nAddressing data mismatch\n\nLearning from multiple tasks\n\nTransfer learning\nMulti-task learning\n\nEnd-to-end deep learning\n\nWhat is end-to-end deep learning\nWhether to use end-to-end deep learning\n\nRuslan Salakhutdinov interview\n\n\n\n\nThere are lots of ways to improve a deep learning system, so its important to sse quick and effective ways to figure out the most promising things to try and and improve.\n\n\n\nold school tv’s had a number of knobs to tune the picture - all the settings made it very hard to get the picture perfect\ncars have orthogonal controls - a steering and speed which effect two different things making it easier to control. If we have controls which effect the steering and speed at the same time it would make it much harder to get the speed and steering angle we wanted.\nwe have a chain of assumptions in ML: fit training set on cost func, fit dev set, fit test set, then perform well in the real world and we want to have different set of knobs to tune each part.\nideally we have a number of controls which do one task well without effecting other things too much.\nof course, some controls apply across many layers and are still useful, like early stopping\n\n\n\n\n\n\n\nset up a single real number to evaluate performance before starting out on a project\na well defined dev set and a single metric speeds up the iterative process of developing a good model\nlooking at a simple cat classifier:\n\n\n\n\nClassifier\nPrecision\nRecall\n\n\n\n\nA\n95%\n90%\n\n\nB\n98%\n85%\n\n\n\n\nPrecision: of the examples our classifier says are cats, what percentage is actually right\nRecall: what % of actual cats are correctly classified\nThe problem with using precision and recall is it can make it hard to figure out which classifier is better\nF1 Score is the harmonic average of precision and recall using \\(F1 = 2 / ((1/P) + (1/R))\\) or \\(2 * (precision * recall) / (precision + recall)\\)\n\nF1 score is usefeul as it balances precision and recall, rather than just taking the simple average, which would favour outliers\n\nto summarize, have a single number metric (this could be an average of several metrics) and a good dev set\n\n\n\n\n\nits not easy to setup a single number - we might narrow things down to a single number, accuracy, but also want to evaluate running time\nsolve this by picking one metric to optimize, and satisfy the other metric by picking a threshold, like all running times below 1000ms are good enough.\n\nfor voice recognition, like on Amazon Alexa, we have metrics for how long humans are ok to wait, so we can just use that number as a threshold\n\nso, we have a make metric to optimize, and any number of other metrics which we satisfy with thresholds.\n\n#### Train/dev/test distributions\n\ndev (cross validation set) and test (the final holdout data) sets have to come from the same distribution.\nfor example, if we have data from different regions, don’t say us/uk/europe is the dev set and asia is the test set - regions will have differences.\nan ML team wasted time optimizing loan approvals on medium income zip codes, but was testing on low income zip codes.\nchoose a dev and test set to reflect data you expect to get in the future and consider important to do well on.\n\n\n\n\n\nold rule of thumb for train/dev/test: 60/20/20 - worked for smaller data.\nbut now we have much larger data sets, so for 1M data set, 1% or 10K might be enough for a test set.\ndeep learning is data hungry so we want to feed it as much data as possible for training\nthe test set helps us evaluate the overall system, so it has to be big enough to give us high confidence.\n\nthis will vary depending on the data set.\nsometimes we might not need a test set at all.\n\n\n\n\n\n\nsay we’ve built two cat classifiers, A has 3% error, B has 5% error.\nA is better, but lets through porn images. B stops all the porn images, so even though it has higher error, B is better algorithm.\nso we want to change our metrics here, say adding a weight to penalize porn images\nthink of machine learning as having two separate steps:\n\n\nfigure out the metric\n\n\nworry about how to actually do well on the metric\n\n\nother things happen, like our image classifier performs well on our data set, but users upload bad quality pictures which lower performance, so change metric and/or the dev set to better capture what we need our algorithm to actually do.\n\n\n\n\n\n\n\n\ntwo main reasons we compare to human level performance:\n\nML is getting better so its become feasible to compare to human level performance in many applications.\nthe workflow of designing and building ML systems is much more efficient when comparing to what humans also do.\n\nfor many problems, progress is rapid approaching human level performance, and slows down upon reaching it. the hope is that its reaching a theoretical limit, called the Bayes optimal error.\nBayes optimal error is the best possible error\n\nfor tasks humans are good at, there isn’t much range b/w human error and the bayes optimal error.\n\nwhen our ML algo is worse than humans, get humans to:\n\ngive us more data, like label images, translate text, etc.\nmanual analysis errors - why did a human get it right and algo get it wrong?\nbetter analysis of bias/variance\n\n\n\n\n\n\nhumans are great at image classification, so looking at two image classification algos:\n\n\n\n\n\nBias Example\nVariance Example\n\n\n\n\nHumans\n1%\n7.5%\n\n\nTraining error\n8%\n8%\n\n\nDev Error\n10%\n10%\n\n\n\n\nthe left example illustrates bias - since humans are doing much better than the algo, we focus on improving training error.\nthe right example shows that we are close to human error - so we focus on reducing the variance\nif we are doing better than Bayes error we’re overfitting.\nhaving an estimate of the bayes optimal error helps us to focus on whether to reduce bias or variance.\n\navoidable bias: training error - human level error\nvariance: dev error - training error\n\n\n\n\n\n\nthere are multiple levels of human performance - choose what matters and what we want to achieve for our system\n\ndo we use a typical doctors error level, an experienced doctors, or the lower error level from a team of experienced doctors.\nsurpassing an average radiologist might mean our system is good enough to deploy\n\nuse human level error as a proxy for Bayes error.\nAn error analysis example from medical classification, where say human error ranges from 0.5-1%:\n\n\n\n\nError\nBias Example\nVariance Example\n\n\n\n\nHuman/Bayes\n1%\n1%\n\n\nTraining\n5%\n1%\n\n\nDev\n6%\n5%\n\n\n\n\nthe left column we need to concentrate on reducing the training error - the variance, and the human error we pick doesn’t matter.\nthe right col we need to concentrate on the dev error - i.e the variance, and picking the lower range of human error is more important since we are close to human level performance.\nso we have a more nuanced view of error, and using bayes error instead of zero error leads to better and faster decisions on reducing bias or variance.\n\n\n\n\n\nml gets harder as we approach/surpass human level performance\nwhen we’ve surpassed human level performance, are we overfitting, or is the bayes error lower than human error.\nsome applications have surpassed human level: online advertising, product recommendation, loan approval, logistics\n\nall these examples are learning from structured data, not natural perception problems which humans are very good at\nteams have access to heaps of data\n\ncomputers haven gotten to single human level at certain perception tasks, like image recognition, speech.\n\n\n\n\n\ntwo fundamental assumptions of supervised learning:\n\nwe can fit the training set pretty well, or avoid bias\nthe training set performance generalizes well to the dev/test set - achieve low variance\n\nto improve a deep learning supervised system\nfor improving bias:\n\ntrain bigger model\ntrain longer or use better optimization algo (RMSprop, Adam, momentum, etc)\ntry other model architectures, hyperparameter search\n\nimprove variance:\n\nmore data\nregularization (L2, dropout, data augmentation)\nNN architecture, hyperparameter search\n\n\n\n\n\n\n\n\nbefore NN’s ml/ai was a lot of graphs, tree pruning, not very satisfying - NN’s actually felt like AI vs the old school AI techniques\nbefore, humans wrote code to do something, with neural networks / machine learning humans write the optimization algo and specify inputs/outputs and the computers write code\nwhile working on cifar10, had predicted an error rate of 10%, but we’re now down to 2-3%.\nbuilt a javascript interface to show himself imagenet images and classify them\nfamous for putting his deep learning stanford class online\nunderstand of deep learning has changed:\n\nsurprised by how general deep learning is - no one saw how well it would work\n\namazed at how you can take pre trained networks and apply them to other tasks\npeople are crushing tasks one after the other with transfer learning\nsurprised that unsupervised learning hasn’t worked so well\nhas been thinking about the future of AI, thinks will split into two directions:\n\napplied AI, where ppl train NN’s to do something, generally supervised learning\nartificial general intelligence, working on dynamic systems to do everything a human can\n\nthinks the approach to do one thing at a time, like with current neural nets which recoginze images, or do speech etc, won’t lead to AGI, we need a single kind of NN which learns all the tasks a human can do - see his short story on how this might look.\npeople liked CS213N becuase it didn’t abstract away things, it works through the low-level details. Implementing stuff is the best way to learn it.\n\n\n\n\n\n\n\n\n\n\nmanually examining what your algo is doing gives insight\n\ne.g looking at a 100 misclassified dev set examples, see whats the biggest type of error and work on fixing that. i.e we want to work on the errors which make up the bulk of our problem, rather than the lower percentage errors.\n\nerror analysis helps us pick useful actions which will have meaningful improvements.\nwe can evaluate multiple ideas in parallel - use a spreadsheet to help decide:\n\n\n\nso summarize errors, find categories, prioritize important ones\n\n\n\n\n\nsometimes its worthwhile to check training sets for mislabelled images\n\ndeep neural networks are quite robust to random errors in the training set, so few mistakes are ok\nbut systematic errors aren’t ok, like labelling all white dogs as cats\n\nmodify our error spreadsheet above to have a mis-labelled category, we should always go after the biggest cause of error first\ncorrecting dev/test set examples:\n\napply same process to dev/test so they continue to come from the same distribution\nconsider examining examples the algo got right as well as the ones it got wrong\nwe might decide to only correct labels in the dev/test sets and not go through the bigger training set, so dev/test data may now come from slightly different distrubutions. This is ok, long as both dev/test have the same distribution, and the train set is a slightly different distribution\n\n\n\n\n\n\nthere are many ways to make a speech recognition system more robust. More generally, for any ML project there are 50 different directions to improve a ml system\nquickly set up a dev/test set and metric, build a initial system, then use Bias/Variance analysis and error analysis to prioritize next steps\na lot of value in the initial ML system is to help us prioritize future efforts.\n\n\n\n\n\n\n\n\ndeep learning needs big data, so often teams end up shoving whatever they can find into a training set, so you end up with different distributions b/w train and test/dev sets.\ne.g we can get 200k cat pictures from web pages, which are good quality and in large numbers, and 10k pictures from a mobile app which are crappy.\nso what can we do:\n\ncombine the datasets and randomly shuffle into a train/dev/test split (205k, 2,500, 2,500), so everything is from the same distribution\n\nthe problem here is that 2,381 out of 2,500 images in the dev/test set will come from the mobile app\n\nmake the training set use all 200k from the web plus 5K from the app, and test/dev set will be 2,500 images each from the app. This sets the target what we want it to be, even though the distributions are no different.\n\na real example of a speech recognizer for directions - we are only listening for sentences like “find the nearest gas station” but we can train on all kinds of speech data, while the dev/test sets are from our specific area. If our specific application has enough data we can add some of it to the training set as well.\n\n\n\n\n\nthe way we analyze bias and variance changes when train dev/test sets are from different distributions\nso we need a training-dev set which has the same distribution as the training set, and isn’t used for training. so we would have error analysis like:\n\n\n\n\n\nVariance Problem\ndata mismatch Problem\n\n\n\n\nTraining Error\n1%\n1%\n\n\nTraining-dev error\n9%\n1.5%\n\n\nDev Error\n10%\n10%\n\n\n\n\n\n\n\ncarry out manual error analysis to understand difference b/w training and dev/test sets\nwe might find errors like a lot of our dev/test data is nosier than training set\nso then we can collect more training data, or make it similar to dev/test data\nuse artificial data synthesis to make the training data more similar to dev set:\n\ncombine some of the training data with something to make it similar to the test/dev distribution, like recorded background noise, or generate new examples\n\nbe careful about simulating data from a small subset of space, like background noise which isn’t representative of actual background noise\nwe can synthesize data which passes the human test but it could lead to over fitting as our synthetic data might just be making a subset of the dataset.\n\n\n\n\n\n\n\n\napply knowledge from one task to another\ntake a image recognizer NN, delete the last layer, create a new layer with random weights and retrain the NN on the new dataset. (called fine tuning)\n\nwe can train only the last layer, or given enough data we can train the entire network.\nthis works as a lot of the low level layers are recognizing structures and features which transfers across to different tasks\n\ntransfer learning makes sense when:\n\ntask A and B have the same input X (image, speech)\nwe have a lot of data for task A and less data for task B\nthe low level features are similar for both tasks\n\nobviously, having a large dataset for the actual problem is ideal, but often we end up having better/more data for a well known problem, like ImageNet for images, and a lot less for what we want to do (say radiology photos).\n\n\n\n\n\ntry to have one NN learn multiple tasks - like a simple self driving car has to detect several things: stop signs, pedestrians, cars, traffic lights, etc\nso the NN predicts a output vector where each position indicates whether something is there or not. (one image has multiple labels)\nwe could have a NN for each type of object, so when does multi-task learning make sense?\n\nlow level features are shared across each object.\nthe amount of data for each task is similar\ncan train a big enough NN to do well on all tasks (can perform better than single NNs)\n\nin practice, multi-task is used a lot less than transfer learning - though is used a lot in computer vision\n\n\n\n\n\n\n\n\nthere are some learning systems which need multiple stages of processing, like speech recognition and vision - engineers had spent years engineering feature pipelines\nand end to end deep learning neural network replaces all these multiple stages with a single neural network\nbut sometimes you don’t want to just feed raw data, like with face recognition - it works better if the image is zoomed and cropped to the face - so you have two deep learning stages:\n\nfigure out where is the face\nnow figure out whose face it is\n\nwhen we have heaps of data, like with translation, end to end works better\nestimating a childs age from a hand xray: there isn’t much data so multistep approach works better.\n\n\n\n\n\nbenefits of end-to-end deep learning:\n\nlets the data speak - if we have enough data a large enough NN will figure it out\nless hand designing of components needed\n\ncons\n\nmay need large amount of data\nmay exclude potentially useful hand designed components\n\nlearning algos have two main sources of knowledge: the data and human knowledge\nkey q: do we have enough data to learn a function of the complexity to map x to y?\nin a self driving car, we have image/radar/lidar -&gt; cars/pedestrians -&gt; route -&gt; steering and currently some of these things are done with non deep learning approaches\nit would be great to input data and get a steering/accel output, but this isn’t very promising yet for self driving cars\n\n\n\n\n\n\nDirector of AI research at Apple twitter, linkedin\nsee his slides on his talk on the deep learning revolution\nboltzmann machines and deep boltzmann machines are very powerful but we haven’t figured out how to use them yet\ngenerative modelling is exciting, expects a lots of progress in the near future\ncompanies have lots of unlabelled data but we haven’t figured out how to use it\nadvice for new researchers:\n\ntry different and new things. with neural networks too many ppl wrote them off as non-convex problems impossible to optimize.\ncode backprop yourself once to understand it\nphd vsd industry: more freedom in academia for long term problems, but industry research is very exciting and well funded, impacts millions of users\n\nexciting areas: unsupervised learning, deep reinforcement learning, NLP\n\n\ncourse done and dusted:"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html#week-1-ml-strategy",
    "href": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html#week-1-ml-strategy",
    "title": "deeplearning.ai: Structuring Machine Learning Projects",
    "section": "",
    "text": "There are lots of ways to improve a deep learning system, so its important to sse quick and effective ways to figure out the most promising things to try and and improve.\n\n\n\nold school tv’s had a number of knobs to tune the picture - all the settings made it very hard to get the picture perfect\ncars have orthogonal controls - a steering and speed which effect two different things making it easier to control. If we have controls which effect the steering and speed at the same time it would make it much harder to get the speed and steering angle we wanted.\nwe have a chain of assumptions in ML: fit training set on cost func, fit dev set, fit test set, then perform well in the real world and we want to have different set of knobs to tune each part.\nideally we have a number of controls which do one task well without effecting other things too much.\nof course, some controls apply across many layers and are still useful, like early stopping\n\n\n\n\n\n\n\nset up a single real number to evaluate performance before starting out on a project\na well defined dev set and a single metric speeds up the iterative process of developing a good model\nlooking at a simple cat classifier:\n\n\n\n\nClassifier\nPrecision\nRecall\n\n\n\n\nA\n95%\n90%\n\n\nB\n98%\n85%\n\n\n\n\nPrecision: of the examples our classifier says are cats, what percentage is actually right\nRecall: what % of actual cats are correctly classified\nThe problem with using precision and recall is it can make it hard to figure out which classifier is better\nF1 Score is the harmonic average of precision and recall using \\(F1 = 2 / ((1/P) + (1/R))\\) or \\(2 * (precision * recall) / (precision + recall)\\)\n\nF1 score is usefeul as it balances precision and recall, rather than just taking the simple average, which would favour outliers\n\nto summarize, have a single number metric (this could be an average of several metrics) and a good dev set\n\n\n\n\n\nits not easy to setup a single number - we might narrow things down to a single number, accuracy, but also want to evaluate running time\nsolve this by picking one metric to optimize, and satisfy the other metric by picking a threshold, like all running times below 1000ms are good enough.\n\nfor voice recognition, like on Amazon Alexa, we have metrics for how long humans are ok to wait, so we can just use that number as a threshold\n\nso, we have a make metric to optimize, and any number of other metrics which we satisfy with thresholds.\n\n#### Train/dev/test distributions\n\ndev (cross validation set) and test (the final holdout data) sets have to come from the same distribution.\nfor example, if we have data from different regions, don’t say us/uk/europe is the dev set and asia is the test set - regions will have differences.\nan ML team wasted time optimizing loan approvals on medium income zip codes, but was testing on low income zip codes.\nchoose a dev and test set to reflect data you expect to get in the future and consider important to do well on.\n\n\n\n\n\nold rule of thumb for train/dev/test: 60/20/20 - worked for smaller data.\nbut now we have much larger data sets, so for 1M data set, 1% or 10K might be enough for a test set.\ndeep learning is data hungry so we want to feed it as much data as possible for training\nthe test set helps us evaluate the overall system, so it has to be big enough to give us high confidence.\n\nthis will vary depending on the data set.\nsometimes we might not need a test set at all.\n\n\n\n\n\n\nsay we’ve built two cat classifiers, A has 3% error, B has 5% error.\nA is better, but lets through porn images. B stops all the porn images, so even though it has higher error, B is better algorithm.\nso we want to change our metrics here, say adding a weight to penalize porn images\nthink of machine learning as having two separate steps:\n\n\nfigure out the metric\n\n\nworry about how to actually do well on the metric\n\n\nother things happen, like our image classifier performs well on our data set, but users upload bad quality pictures which lower performance, so change metric and/or the dev set to better capture what we need our algorithm to actually do.\n\n\n\n\n\n\n\n\ntwo main reasons we compare to human level performance:\n\nML is getting better so its become feasible to compare to human level performance in many applications.\nthe workflow of designing and building ML systems is much more efficient when comparing to what humans also do.\n\nfor many problems, progress is rapid approaching human level performance, and slows down upon reaching it. the hope is that its reaching a theoretical limit, called the Bayes optimal error.\nBayes optimal error is the best possible error\n\nfor tasks humans are good at, there isn’t much range b/w human error and the bayes optimal error.\n\nwhen our ML algo is worse than humans, get humans to:\n\ngive us more data, like label images, translate text, etc.\nmanual analysis errors - why did a human get it right and algo get it wrong?\nbetter analysis of bias/variance\n\n\n\n\n\n\nhumans are great at image classification, so looking at two image classification algos:\n\n\n\n\n\nBias Example\nVariance Example\n\n\n\n\nHumans\n1%\n7.5%\n\n\nTraining error\n8%\n8%\n\n\nDev Error\n10%\n10%\n\n\n\n\nthe left example illustrates bias - since humans are doing much better than the algo, we focus on improving training error.\nthe right example shows that we are close to human error - so we focus on reducing the variance\nif we are doing better than Bayes error we’re overfitting.\nhaving an estimate of the bayes optimal error helps us to focus on whether to reduce bias or variance.\n\navoidable bias: training error - human level error\nvariance: dev error - training error\n\n\n\n\n\n\nthere are multiple levels of human performance - choose what matters and what we want to achieve for our system\n\ndo we use a typical doctors error level, an experienced doctors, or the lower error level from a team of experienced doctors.\nsurpassing an average radiologist might mean our system is good enough to deploy\n\nuse human level error as a proxy for Bayes error.\nAn error analysis example from medical classification, where say human error ranges from 0.5-1%:\n\n\n\n\nError\nBias Example\nVariance Example\n\n\n\n\nHuman/Bayes\n1%\n1%\n\n\nTraining\n5%\n1%\n\n\nDev\n6%\n5%\n\n\n\n\nthe left column we need to concentrate on reducing the training error - the variance, and the human error we pick doesn’t matter.\nthe right col we need to concentrate on the dev error - i.e the variance, and picking the lower range of human error is more important since we are close to human level performance.\nso we have a more nuanced view of error, and using bayes error instead of zero error leads to better and faster decisions on reducing bias or variance.\n\n\n\n\n\nml gets harder as we approach/surpass human level performance\nwhen we’ve surpassed human level performance, are we overfitting, or is the bayes error lower than human error.\nsome applications have surpassed human level: online advertising, product recommendation, loan approval, logistics\n\nall these examples are learning from structured data, not natural perception problems which humans are very good at\nteams have access to heaps of data\n\ncomputers haven gotten to single human level at certain perception tasks, like image recognition, speech.\n\n\n\n\n\ntwo fundamental assumptions of supervised learning:\n\nwe can fit the training set pretty well, or avoid bias\nthe training set performance generalizes well to the dev/test set - achieve low variance\n\nto improve a deep learning supervised system\nfor improving bias:\n\ntrain bigger model\ntrain longer or use better optimization algo (RMSprop, Adam, momentum, etc)\ntry other model architectures, hyperparameter search\n\nimprove variance:\n\nmore data\nregularization (L2, dropout, data augmentation)\nNN architecture, hyperparameter search\n\n\n\n\n\n\n\n\nbefore NN’s ml/ai was a lot of graphs, tree pruning, not very satisfying - NN’s actually felt like AI vs the old school AI techniques\nbefore, humans wrote code to do something, with neural networks / machine learning humans write the optimization algo and specify inputs/outputs and the computers write code\nwhile working on cifar10, had predicted an error rate of 10%, but we’re now down to 2-3%.\nbuilt a javascript interface to show himself imagenet images and classify them\nfamous for putting his deep learning stanford class online\nunderstand of deep learning has changed:\n\nsurprised by how general deep learning is - no one saw how well it would work\n\namazed at how you can take pre trained networks and apply them to other tasks\npeople are crushing tasks one after the other with transfer learning\nsurprised that unsupervised learning hasn’t worked so well\nhas been thinking about the future of AI, thinks will split into two directions:\n\napplied AI, where ppl train NN’s to do something, generally supervised learning\nartificial general intelligence, working on dynamic systems to do everything a human can\n\nthinks the approach to do one thing at a time, like with current neural nets which recoginze images, or do speech etc, won’t lead to AGI, we need a single kind of NN which learns all the tasks a human can do - see his short story on how this might look.\npeople liked CS213N becuase it didn’t abstract away things, it works through the low-level details. Implementing stuff is the best way to learn it."
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html#week-2-more-ml-strategy",
    "href": "posts/writing-about-code/deeplearning-ai-part-3-of-5.html#week-2-more-ml-strategy",
    "title": "deeplearning.ai: Structuring Machine Learning Projects",
    "section": "",
    "text": "manually examining what your algo is doing gives insight\n\ne.g looking at a 100 misclassified dev set examples, see whats the biggest type of error and work on fixing that. i.e we want to work on the errors which make up the bulk of our problem, rather than the lower percentage errors.\n\nerror analysis helps us pick useful actions which will have meaningful improvements.\nwe can evaluate multiple ideas in parallel - use a spreadsheet to help decide:\n\n\n\nso summarize errors, find categories, prioritize important ones\n\n\n\n\n\nsometimes its worthwhile to check training sets for mislabelled images\n\ndeep neural networks are quite robust to random errors in the training set, so few mistakes are ok\nbut systematic errors aren’t ok, like labelling all white dogs as cats\n\nmodify our error spreadsheet above to have a mis-labelled category, we should always go after the biggest cause of error first\ncorrecting dev/test set examples:\n\napply same process to dev/test so they continue to come from the same distribution\nconsider examining examples the algo got right as well as the ones it got wrong\nwe might decide to only correct labels in the dev/test sets and not go through the bigger training set, so dev/test data may now come from slightly different distrubutions. This is ok, long as both dev/test have the same distribution, and the train set is a slightly different distribution\n\n\n\n\n\n\nthere are many ways to make a speech recognition system more robust. More generally, for any ML project there are 50 different directions to improve a ml system\nquickly set up a dev/test set and metric, build a initial system, then use Bias/Variance analysis and error analysis to prioritize next steps\na lot of value in the initial ML system is to help us prioritize future efforts.\n\n\n\n\n\n\n\n\ndeep learning needs big data, so often teams end up shoving whatever they can find into a training set, so you end up with different distributions b/w train and test/dev sets.\ne.g we can get 200k cat pictures from web pages, which are good quality and in large numbers, and 10k pictures from a mobile app which are crappy.\nso what can we do:\n\ncombine the datasets and randomly shuffle into a train/dev/test split (205k, 2,500, 2,500), so everything is from the same distribution\n\nthe problem here is that 2,381 out of 2,500 images in the dev/test set will come from the mobile app\n\nmake the training set use all 200k from the web plus 5K from the app, and test/dev set will be 2,500 images each from the app. This sets the target what we want it to be, even though the distributions are no different.\n\na real example of a speech recognizer for directions - we are only listening for sentences like “find the nearest gas station” but we can train on all kinds of speech data, while the dev/test sets are from our specific area. If our specific application has enough data we can add some of it to the training set as well.\n\n\n\n\n\nthe way we analyze bias and variance changes when train dev/test sets are from different distributions\nso we need a training-dev set which has the same distribution as the training set, and isn’t used for training. so we would have error analysis like:\n\n\n\n\n\nVariance Problem\ndata mismatch Problem\n\n\n\n\nTraining Error\n1%\n1%\n\n\nTraining-dev error\n9%\n1.5%\n\n\nDev Error\n10%\n10%\n\n\n\n\n\n\n\ncarry out manual error analysis to understand difference b/w training and dev/test sets\nwe might find errors like a lot of our dev/test data is nosier than training set\nso then we can collect more training data, or make it similar to dev/test data\nuse artificial data synthesis to make the training data more similar to dev set:\n\ncombine some of the training data with something to make it similar to the test/dev distribution, like recorded background noise, or generate new examples\n\nbe careful about simulating data from a small subset of space, like background noise which isn’t representative of actual background noise\nwe can synthesize data which passes the human test but it could lead to over fitting as our synthetic data might just be making a subset of the dataset.\n\n\n\n\n\n\n\n\napply knowledge from one task to another\ntake a image recognizer NN, delete the last layer, create a new layer with random weights and retrain the NN on the new dataset. (called fine tuning)\n\nwe can train only the last layer, or given enough data we can train the entire network.\nthis works as a lot of the low level layers are recognizing structures and features which transfers across to different tasks\n\ntransfer learning makes sense when:\n\ntask A and B have the same input X (image, speech)\nwe have a lot of data for task A and less data for task B\nthe low level features are similar for both tasks\n\nobviously, having a large dataset for the actual problem is ideal, but often we end up having better/more data for a well known problem, like ImageNet for images, and a lot less for what we want to do (say radiology photos).\n\n\n\n\n\ntry to have one NN learn multiple tasks - like a simple self driving car has to detect several things: stop signs, pedestrians, cars, traffic lights, etc\nso the NN predicts a output vector where each position indicates whether something is there or not. (one image has multiple labels)\nwe could have a NN for each type of object, so when does multi-task learning make sense?\n\nlow level features are shared across each object.\nthe amount of data for each task is similar\ncan train a big enough NN to do well on all tasks (can perform better than single NNs)\n\nin practice, multi-task is used a lot less than transfer learning - though is used a lot in computer vision\n\n\n\n\n\n\n\n\nthere are some learning systems which need multiple stages of processing, like speech recognition and vision - engineers had spent years engineering feature pipelines\nand end to end deep learning neural network replaces all these multiple stages with a single neural network\nbut sometimes you don’t want to just feed raw data, like with face recognition - it works better if the image is zoomed and cropped to the face - so you have two deep learning stages:\n\nfigure out where is the face\nnow figure out whose face it is\n\nwhen we have heaps of data, like with translation, end to end works better\nestimating a childs age from a hand xray: there isn’t much data so multistep approach works better.\n\n\n\n\n\nbenefits of end-to-end deep learning:\n\nlets the data speak - if we have enough data a large enough NN will figure it out\nless hand designing of components needed\n\ncons\n\nmay need large amount of data\nmay exclude potentially useful hand designed components\n\nlearning algos have two main sources of knowledge: the data and human knowledge\nkey q: do we have enough data to learn a function of the complexity to map x to y?\nin a self driving car, we have image/radar/lidar -&gt; cars/pedestrians -&gt; route -&gt; steering and currently some of these things are done with non deep learning approaches\nit would be great to input data and get a steering/accel output, but this isn’t very promising yet for self driving cars\n\n\n\n\n\n\nDirector of AI research at Apple twitter, linkedin\nsee his slides on his talk on the deep learning revolution\nboltzmann machines and deep boltzmann machines are very powerful but we haven’t figured out how to use them yet\ngenerative modelling is exciting, expects a lots of progress in the near future\ncompanies have lots of unlabelled data but we haven’t figured out how to use it\nadvice for new researchers:\n\ntry different and new things. with neural networks too many ppl wrote them off as non-convex problems impossible to optimize.\ncode backprop yourself once to understand it\nphd vsd industry: more freedom in academia for long term problems, but industry research is very exciting and well funded, impacts millions of users\n\nexciting areas: unsupervised learning, deep reinforcement learning, NLP\n\n\ncourse done and dusted:"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html",
    "href": "posts/writing-about-code/PyConAu2018.html",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "My notes for the PyConAU 2018 talks I went to.\n\n\n\nThis talk will tell the story of a foster fail, how Python helped to save the life of a rescue dog and how the initial medication feeder grew from a single IoT device into a full Internet of Dog (IoD) madness. I will show the design and implementation of the initial Python-based medication feeder, what we have learned from running it over the summer and how it has continued to grow into a multi IoT device, Python-powered, full dog carer solution. There will be microservice architecture drawings, Python code, and, of course, pictures of the dog! #\n\n\n\nAndreas Moll - twtr - Works at ANSTO on scientific software in Python\nwere fostering a dog, Willow - fostering: look after for a limited time and prepare for adoption\nWillow was badly treated by previous owner and needed anti-anxiety medication every 8 hrs\n4 weeks of meds, improved Willow a lot. But then they both went back to work, and nobody had adopted Willow. So they had two issues: monitor Willow and afternoon medication\nmonitoring was easy - used a webcam. Considered off the shelf dispensers, but didn’t like\nOnion Omega2+ attached to a servo which dropped the treat+drugs on time\nFirst drug feeder:\n\nVM in the cloud - wrote a flask webapp, backed by a Redis database. The webapp sent actions Feed|Home to the VM. The Onion has an endless loop which polls the VM every second asking for an action, and then does it.\npolling is a very simple scheme, and if anything breaks down, nothing bad happens, but there is a lag b/w button and action, as well as the dependency on the IOT and VM in cloud\nusb audio using alsa libraries which would call the dog\n\nthe IOT was only part of the puzzle - they had to train the dog as well. Ran this setup for a couple of months, worked suprisingly well. Sometimes failed becuase of power outages, so they added a UPS.\nWillow didn’t get adopted - so they adopted her. Now that they were going to have her, they decided to get serious about the IOT\nSplit into infrastructure and service\n\nDevice: Raspberry Pi Zero connected to I/O, sensors, a Pi3 running gateway services, docker registery, consul server\nService: Web service exposing a RESTful API running on flask, service discovery using Consul, packaged in a docker container and depolyed using docker-compose\n\npi zero is nice and simple, is using one per physical device/sensor\nthe gateway service polls the VM for actions, then does a REST call to the relevant PiZero to do the action\nFuture: websockets instead of polling, wants things event driven and real time readings for sensors. web interface with react\n\nQ&A\n\nWhy not docker swarm?\n\nwanted to use consul\n\n\ntakeaway: genius AND way overkill. but infrastructure is hard. consider using orchestration tools.\n\n\n\n\nLED lighting rigs are expensive. Worse, they have little to no controls aside from on/off. Most are not dimmable and changing colors requires the use of gels. In this talk I will discuss how CircuitPython was used in conjunction with LEDs and microcontrollers to make a custom LED photo lighting rig. #\n\n\n\nStacey Morse - geekgirlbeta, web is an artist who also does macro photography, web dev in python\nart is awesome, but led lights are very expensive and crap. the intersection b/w art and tech: both are very process drivenl\nhad a conversation with a python programmer who suggested python controlled lights could solve her lighting issues - and thus she started to learn how to program in micropython\nmacro photography is very technical - there is a lot of math involved, understanding light, lens dynamics, lighting\nlights: on/off, select number of leds, control brightness, colour\nhardware: microcontroller, leds, rotary encoder, buttons, ir remote\nsoftware: CircuitPython (very beginner friendly), MicroPython\nwent with neopixels for the lights - gives true white light as its RGBW\nused Feather M4 Express for the microcontroller\nthe code is very straightforward\n\nQ&A:\n\nhot shoe\n\nfigures she can use the hot shoe in a camera to communicate with the light - so she can write light settings in the exif data\n\nwhat can she do with this which she can’t do with a traditional flash kit?\n\nled’s blink, so u need them in blink in coordination with your camera settings. this allows her to focus on a specific part of an object. traditional flashes are bigger, whole panels of lights\n\n\ntakeaway: building a real tangible project is fun. Curcuit Python looks super easy to use.\n\n\n\n\nMicroPython is a reimplementation of Python which is specifically designed to run on computing devices that have very few resources, such as CPU power, RAM and storage. Often when you write scripts in MicroPython you want to make the most of your available resources, and have code run as fast as possible (faster code usually saves power, which is important when running from a battery!) and there are certain ways of writing MicroPython code that are more efficient than others. In this talk I will go over the tricks and techniques for writing fast and efficient Python code in MicroPython. As part of this I will delve into some technical details of how MicroPython works, in order to better understand what it’s doing behind the scenes and how to make the most of it. I will discuss general techniques for making things run faster (some of which would be applicable to normal Python), as well as ways to completely avoid memory allocation, which is important for both efficiency and making code execution deterministic. The talk will include some hardware demos to show off the techniques, including five different ways to blink an LED fast. #\n\n\n\nDamien George - creator of MicroPython\nloading local variables is fast, global variables is slow. So try to use local variables.\nits slow to call functions from within a function\nsome things allocate memory on the heap, others don’t - try not to allocate memory.\ndon’t allocate memory: All the basic statements like if/else, local variables, int arthimetic, some builtin funcs like any, len, etc\ndoes allocate memory: importing, defining functions and classes, global variables, data structures\ntips to reduce cpu time:\n\nuse funcs - for example put a loop within a func and call the func\ndon’t use global, try to use local variables as much as possible\ncache funcs\ncache variables as local\nprefer longer expressions, not split up ones\nuse consts from micropython import const\n\ntips to reduce ram usage:\n\ndon’t use heap when possible\nuse short variable names\ntemp buffers\nuse XXX_into methods\ndon’t use 8 or ** args\nuse const, minify scripts\nuse mpy-cross to produce .mpy - this saves a lof ot time in compiling the script on the device\nultimate solution is to freeze scripts into the firmware (bytecode gets stored into flash storage, advanced)\n\nwithin a func, preload methods on = led.on and then use on(). So when calling a method multiple times this is much faster.\nfor small loops, unroll the loop\nviper mode - undocumented, being written, writes registers directly to say turn on/off leds\ncan also write inline assembler in python syntax\npre allocate a buffer and write to it\n\ntakeaway: micropython can be fast. don’t use globals, but everyone knows that already, so instead apply some of the tricks from above, like caching methods, and for micropython don’t have too many funcs.\n\n\n\n\nAsyncio provides a way to achieve concurrency in a relatively simplistic fashion. However, first-time users still struggle with the concepts so let’s sort them out! Then we’ll see why it’s especially useful in an embedded environment.#\n\n\n\nMatt Trentini - a software engineer, has worked with lots of projects with embedded firmware with c compilers, has been looking for better alternatives\nmicropython is the way forward for embedded development\nCoroutines - the ability to run things concurrently\n\nuse yield points to relingquish control and an event loop to scedule tasks\nprovides cooperative multitasking\nconcurrency isn’t parallelism\n\na lot of async funcs is similar - just needs async def func and await where are are awaiting stuff.\ncoroutines (tasks) --&gt;&lt;-- event loop --&gt;&lt;--- blocking operations (network access, sleep, database queries)\nwhat about threads? wasn’t this a solved problem? threads provid pre-emptive multi-tasking, but they get complex, are heavy (each thread needs its own stack) and hesienbergs\nCoroutines are better cause:\n\nminimal overhead, high performance\nlocking is less problematic\nmore closely reemble synchrous code\n\nCoroutines in python are biult on generators and use async/await keywords\nPEP 492 in Python 3.5 brought async\nPython needed async to stay relevant and competitive with other async languages\nasync needs all the funcs to support async code, so hard to use with older synchronus code\nasync is in active development\ncoroutines in micropython is a subset but for practical use the same\nasync in embedded devices is exciting as threading support is limited and many embedded operations are cncurrent by nature\nclassic problem in embedded space: Debouce\n\nneed to wait for a button to stop bouncing - since they’re typically not digital\nasync lets us write a loop which pools the button, and when a button is pressed, it can await the sleep so the controller can do other things instead of just waiting on the button\n\nthere is micropython-async - look up tutorial from Peter Hinch\nuPTV - counts down minutes to next train\n\nNTP - time support import ntptime\nHTTP request - uses a requests library - python has aiohttp, micropython has a very crude version\nLED control\nasyncio to create loop and run it forver - sync time, poll for train, update LED\n\nLED strip controller\n\nrotary encoder to set brightness and push on/off\nuses a async fade routine to smoothly fade to brightness\nasync means can change the brightness while still fading, so no waiting\n\n\nQ&A\n\nppl using coroutines for real time?\n\nin experimental stage - if u request 5ms sleep, it can be less or more depending on what the other coroutines are doing\n\nhow is it implemented?\n\nin micropython uses queues implemnted in C\n\ntiming and checking length of executable paths\n\nup to you to define - for example a fade routine might have a 5ms delay b/w levels so await sleep(5ms)\n\nhow do you deal with a blocking library?\n\nput timeouts on things - call a func, if it takes too long to run, interrupt it, or use threads - async can push things into threads\n\nwith asyncio, do you apprach problems differently from threads?\n\nasync feels more natural - you can state what you want to do. With threading you have to think a lot about synchronization and how long things will take.\n\n\ntakeaway: get micropython hardware and start coding. Understand async better, it seems pretty straightforward, and on slow hardware a godsend - i.e async sending logs + async doing other stuff makes for easy real time monitering even if the machine is stuck on something..\n\n\n\n\nConnecting IoT devices using low power over wide area wireless (LoRaWAN) makes sense. But how LoRaWAN works, duty cycles, frequency plans, receive windows, etc. doesn’t. #\n\n\nThis talk will demystify how LoRaWAN works using PyCom devices.\n\n\n\ntalk by Brian Danilko of Likeable Software\nLoPy\nPycom has lots of examples for LoRa and LoRaWAN\nLoRa is the transport layer - patented, developed by Cycleo, Semtech owns it\n\nuses a Chirp Spread Spectrum (CSS) to transmit info reliably using low power\nuses different power levels, frequencies, bandwidths, spread factors to optimize power use vs time on air vs distance\n\nLoRoWAN adds the wide area network on top of LoRa\n\nconnects end nodes to the internet through gateways\nencrypts for secure transmission, controls what devices can join the network, confirms and retries (but this doubles power and clogs airways so don’t use if not needed)\n\nyou have multiple end nodes talking to one gateway, which connects to the internet via a wired link (can be wireless too)\ncan have multiple gateways for backup - so nodes can connect to multiple gateways\njoin a LoRaWAN network by OTAA (over the air, same keys embedded in all devices) or ABP (unique keys)\nfrequencies for gateways and nodes have to be the same - something to be aware of when setting up devices\n\ntakeaway: look at LoRo/PyCom if using wireless IOT away from wifi.\n\n\n\n\nHave you often wondered where the quietest spot in the office is right now? In this talk, we explain how we built a real-time system that does just that using CircuitPython. #\n\n\n\nAiden Ray, works at BVN\nbuilding awards are handed out before ppl use them - very little feedback from real users\nthe web is totally instrumented, you know exactly whats going on\nthe physical world is a lot less instrumented\nAmaon Go is sensored everywhere - this is where the world is moving, you get tons of data, can see where to improve\nCircuit Playground Express by Adafruit\nC/C++ vs Python boils down to runtime speed vs development speed\nCircuit Python is Adafruits fork of MicroPython, more beginner focused, doesn’t have access to async, so you have a basic event loop\nif sys.implementation == 'circuitpython' to check implementation\n\ntakeaway: pervasive monitoring is creepy but its going to happen. Circuit Python is amazingly easy. Make something with it.\n\n\n\n\nHome Automation is a fun new field for the modern Pythonista. In this talk I will be walking through how a developer can leverage a python library to use the HomeKit service and automate the devices in their home. I will be covering topics like hardware selection for local and remote access, HomeKit service registration and management and potential security concerns around IoT. #\n\n\n\nSean Johnson twtr ln\nhomekit is a software framework for controlling smart hardware, built on Apples HomeKit Accessory Protocol\nthere are a few opensource implementations in node and python\napple predefines hardware configurations, you can’t define your own\nHome App allows manual and automated control of devices\nthere are many existing hardware, from ikea to phillips\nRaspberry Pi - supports python\nuses HAP-Python, demo of a controlling an aircon via infrared\npretty straightforward, except tied to Apple and Siri\n\nq & a\n\nIR is unidirectional so what happens if u miss a packet?\n\nevery packet is single state, so contains all the info in one go. So if a packet fails, the next one doesn’t rely on it. Also, leep transmitter close to the receiver so the signal is clear.\n\n\ntakeaway: No Apple, hence no homekit for me.\n\n\n\nFriday August 24 2018, Education Track, C3.4 & C3.5, 16:00 AEST\n\nEight short (10 min) talks from high school students across Australia. They’ll be talking about projects they’ve built with Python using machine learning, robotics, natural language processing, and more #\n\n\nMENACE - building a learning matchbox machine in Python\n\nmachine learning is the use of programming to make a computer learn from data\ninspired by Matt Parker from Standup Math channel on youtube\nproblems with indexing errors leading to endless loop\nwants to use pygame or kivy to make a actual game\n\ntakeaway: awesome.\nOptimising Memory Retention via a Machine Learning based Flashcard System built in Python\n\nThis project aims to leverage Python’s machine learning capabilities, combined with psychological theories of learning and forgetting, to construct predictive models of human memory in order to improve upon traditional flashcard systems.\n\n\nIn this talk, I will share my experience of: (1) utilising Python’s sci-kit-learn package, alongside the Latent Skill Embedding package, to train, evaluate and visualise the performance of various models; (2) implementing the model into a web-based flashcard application built in Flask - a popular Python micro framework; and (3) testing the effectiveness of the system through a classroom experiment on 36 Japanese language students.\n\n\nhttp://josephtey.com/\nresearched various techniques for the optimal time to review\nexperimented on 40 students\nstarted with R, but how do u deploy with R? so moved on to Python as it can both train a model and an app to serve it with\n\ntakeaway: build my own flash cards to learn stuff with spaced repetition.\nText Summariser\n\nThe Text Summariser is a program I built for when one is unable or unwilling to summarise information from a large block of text themselves. In my talk, I will discuss how it works, what inspired the project, and how I overcame the (many) challenges of building my program. I will also talk about computational linguistics and Natural Language Processing (NLP), two big components of how the text summariser works. After listening to this talk, you will have learnt some basic Natural Language Processing, and how you can apply it in Python Programs.\n\n\nsorts words by type - verbs, nouns, etc\nuses markov chains to summarize\n\ntakeaway: too much cleverness going on to summarize.\nNOR: creating generated worlds on iPad\n\nNOR is a 2d puzzle exploration game for iPad that I made over the course of year 10. It features procedurally generated landscapes that collide with and can be edited by the player. The landscapes are host to procedurally generated bushes, trees and puzzles. My talk will discuss how I reached the point where I could set off on a large scale python coding project, how I built up the game and made the systems work, and how anyone can pick up an iPad and start developing.\n\n\nlearned calculus for the game movements etc\nthings are based on each other\nuses monte carlo to generate gaphics, of a random seed, which are dependend on the ground and level number\nused pythonista\nuses momemtum + inertia for players movement\n\ntakeaway: procedural generation is awesome.\nRule-Based Machine Translation\n\nThe Text Summariser is a program I built for when one is unable or unwilling to summarise information from a large block of text themselves. In my talk, I will discuss how it works, what inspired the project, and how I overcame the (many) challenges of building my program. I will also talk about computational linguistics and Natural Language Processing (NLP), two big components of how the text summariser works. After listening to this talk, you will have learnt some basic Natural Language Processing, and how you can apply it in Python Programs.\n\n\nuses machine translation to switch from english to latin\nfour main ways to translate: rule based, example based, statistical, neural\nGoogle Translate sucks at Latin translation\nvery impressive work\n\ntakeaway: NLP for the win.\nSVG Graph Calculator\n\nThe name of my project is somewhat self-explanatory, it is an SVG Graph calculator. I know right? My talk is going to be about how I decided to do this project, and my struggles and innovations in making this project happen.\n\n\nturns an equation into a SVG graph\nWhy? had to draw graphs for yr9 math class. There are online tools which do this already, but decided it would be fun to do this\nthought of writing it in python turtle, but too slow\nwanted something which worked on web\nhad to reverse engineer SVG file\nif looking for inspiration, just copy something\n\ntakeaway: good basis for building a equation solver.\nEmojifer in @ school\n\nEmojifer is an implementation of a sequence model in Machine Learning. It will analyse the meaning of a sentence and give it the appropriate emoji. Emojifier plays an important role in @ school which is a cloud-based learning management system written in React with Flask served as the server. Come along to this talk, if you want to know what’s under the hood of Emojfier and how I make it happen. Additionally, I’ll talk about some of the problems I’ve encountered so far and how I overcame it. This talk will give you an idea of how to get started in Machine Learning as well as full-stack web development if you’re new to the area.\n\n\nhttp://emojifier.surge.sh/#/\nhttps://github.com/anhphamduy/emojifier_model\nuses RNN and LSTM\nkeras and flask\nmade his own training data\nlessons: best way to learn is to teach, jump in head first, pair program, stack overflow is awesome\n\ntakeaway: everyone needs a emojifier.\nPyVlov’s Dog\n\nPyVlov’s Dog is a simulation software created to dynamically train neural networks for the control of basic robots. When we started this project we had a limited understanding of neural networks, so we simplified the problem by visualising it as training a dog.\n\n\nWe developed this project as a way to streamline the concept of neural networks through this metaphor of training a dog, so as to facilitate its use as an education tool. In this talk we will talk about how we made the software, the difficulties we faced in creating it and the way in which it works. We will also discuss the development of the robot’s hardware and the way in which it is represented within the simulation. Finally, we will cover the way in which we developed the principles of ease of use, and visual clarity within the program, to allow for use by people of all skill sets.\n\n\npurpose of project was to understand neural networks, and did this through training a dog to do what its told to do (by creating a set of rules)\nsensors are the input to a NN, which gives a output to a motor\nmade a simulation to train the NN - user defines a dog and an environment, once happy with the NN, throw it into the actual robot\nuses arduino to control motors, raspberry pi to run NN\nhardware is hard!\nused pymunk to handle the physics of the sim, but is only 2d, so they had to work around that\nneeded to use tkinter with pygame\nhttps://pyvlov.wordpress.com/\n\ntakeaway: interesting project to follow. The future is training your own robot to do something.\neducational talks wrap up\n\nhigh school students are doing amazing stuff\nlearnt a lot from throwing themselves in the deep end to do a project.\npython is a fantastic learning language, don’t babyfeed students, support them in the deep end\n\n\nSaturday talks\n\n\n\n\nAnnie is a co-founder of Techfugees Australia - a global movement connecting the technology ecosystem together with newly arrived refugees here in Australia to help them integrate into their new communities. In this talk, Annie will be sharing her experience of how Techfugees works and some of the success stories they’ve had along the way #\n\n\n\ntechnology has a huge impact on changing peoples lives\n68.5M displaced ppl around the world, with 25M refugees\nthe tech community can help, done 5 hackathons so far\n6th hackathon 24/25th Nov in Blacktown - signup\n\nlooking for ppl who can put togther really early stage stuff and know whats out there to use\ndon’t need to be deeply skilled\n\nthey’ve made significant changes - new companies, connected investors to customers, jobs, community impact, media stories, inspiration\ncase studies:\n\nrefugeetalent created at first hackathon in 2015, placed 150 ppl into jobs\nonestep founded at second hackathon, connects refugees\nmuralisto - makes community art\n\nShaqaeq Rezai ln, twtr - 18yr hazara Afghan refugee, moved to Australia from Iran where she faced a lot of discrimination - attended a hackathon, built an AI to track moods, got scholarship, started Directa Australia. Met the CEO of the Iconic through the startup scene, who is also Aghan.\n\nwants to help ppl through tech\n\nmost refugees have good knowledge of tech, smart phones\n50% of business in Australia have been started by a refugee or a migrant\n\nmy takeaway: this is awesome, go to their hackathon in Nov. Don’t need ninja skills, a lot is just about knowing what is out there in the tech space which can help.\n\n\n\n\nDescriptors are a little known feature of Python. They provide a way for a programmer to customize the storage and retrieval of different instance variables. In this talk, you will learn about the descriptor protocol, what it can be used for, and how to implement a descriptor. #\n\n\n\nMatthew Egan web twtr\nwhy descriptors? often overlooked, used primarily by library developers\nso we have a basic class, Person, which stores info like name, etc\nour dream Person class makes sure names are capitilized properly\n__setattr__(self, attr, value) is called inside a class when an attribute is being set, so if Person.name is assigned, it runs through the setattr method which can can modify the variable from name to Name\ncan use setter methods like @name.setter inside the class, but this only applies to the specific class\nthe descriptor protocl allows a more general solution, has four attributes: get, set, deelte, set_name\n\n__get__(self, instance, owner)\n__set__(self, instance, value)\n__delete__(self, ??)\n__set_name__(self, owner, name) - doesn’t take instance argument as its only called on class creation\n\nwhen you access an attribute in python, it goes through a lookup process, which gets it from a dict, a data descriptor or a non_data descriptor\nnon-data descrptors: staticmethod, classmethod, abc.abstractmethod\nif we have a Person class and tons of Person objects, we write just one descriptor which covers all the objects\nfrom weakref import WeakKeyDictionary - when we delete an object which is also in the dict, it automatically removes it from the dictionary\nlook up some example descriptors using weakref - he gave an example of using a descriptor to make sure names are capitilized\ndescriptors have use cases like:\n\nreplicating customization for multiple attrs without caring about name\ndjango generic foreign keys\ncustom validation e.g only non negative integers for age\nbetter error msgs - can prepend the name of attribute being set to the error msg so its more descriptive\n\ntalk code: https://github.com/mattjegan/describing-descriptors\n\nq & a:\n\nruntime perf\n\ndepends on impementation, external calls\n\ncan you chain descriptors?\n\nshould be possible, but haven’t done it.\n\nhow do u replace the descriptor itself?\n\ndelete it off the class itself\n\nwhat happens if you pickle an object with a descriptor?\n\npickle is unsecure so don’t use it\n\n\nmy takeaway: descriptors are useful, can apply across classes, so makes for better code\n\n\n\n\nFinding the most common street name in Australia may sound like a simple thing to do - but it quickly devolves into a scenic tour of all the things that go wrong when doing data analytics. I’ll be giving advice on how to avoid these speed bumps along with how to work with OpenStreetMaps in Python. #\n\n\n\nRachel git, twtr, ln works at Solar Analytics\nread an article about most common street names in America, got her interested about Australian streets\nG-NAF is australian geocoded data containing every address/street\nused OSM, since it has every country, instead of G-NAF\ncouldn’t just export all of Australia data from OSM\nused metro extracts to download geojson, which has now gone offline\ngeojson includes a geometery feature e.g roads defined as a series of points (a line!)\nused geopandas - works like pandas\nosm has all kinds of road types, so have to think about it, so what exactly is a street? She went with a street is something with a name which a car can drive on\nsee her blogpost on cleaning OSM data\nshe got victoria rd and pacific highway as most common street names in Sydney, which didn’t sound right\nso drew a plot, and found out OSM saw streets as segments, so long streets appeared many times\nshapely is a package which merges line segments together, but didn’t do a great job\nso she wrote her own function to knit streets together - get road segments with same name, find ones which are close together, merge\nthere is a distance func to find distance b/w geometeries\nhad to parse names, as street names have a name and a descriptor, like parade, street, avenue, etc. This was not easy as there are a lot of variations, too many!\nwhat about Little Street? is that same as Street?\nshe wanted to use this for different countries, but each country has different street descriptors, so the Australia model didnt’ work for other countries\nused overpass turbo api to get all of Australia’s data - but was hitting api limits so had to break Au into a grid and make multiple calls\nmost common street names in Australia: Park, Railway, George, Church, Victoria\nlearnings:\n\nstart small - Sydney over Australia\nchoose something familiar\ncheck your biases\nconstant vigilant - test, draw stuff (she plotted streets on a map)\nknow your problem\n\n\nmy takeaway: a suprisingly simple question can lead to a whole lot of learning. Answer a few simple q’s myself using python.\n\n\n\n\nBy now you have probably all heard about Python static typing. But why should you care? Are types in Python even Pythonic? Is Python turning into Java? Type annotations are Pythonic, trust Guido’s word for it, and Python is definitely not turning into Java.\n\n\nThe greatest benefit of types in large Python codebases is the fact that the input and output structures of a function are obvious from just looking at the signature. In the untyped world the definition for the class you are looking for may be N jumps away, hidden somewhere deep in the codebase, and you don’t have a direct reference to it. In the best possible case grepping for it will yield just a few results and you will be able to spot what you are looking for. In the worst case though, you will have hundreds of hits and you will have to start your application and inspect the type at runtime to figure out what is going on, which make the development cycle slow and tedious.\n\n\nCome to this talk if you want to know more about the typing system in Python, how to gradually add it to your codebase and what benefits will your team get in the long run! I will also cover some advanced tools like the runtime type collection system, MonkeyType, and the just open sourced type checker, Pyre\n\n\n\nLuka Sterbic git - works at Facebook\ntalk slides and code\nwhy should I care about typing?\n\nyou can call modules from far away, in a large codebase can be many imports away, so its not clear what kind of data object gets returned\nwith typed codes, its obvious what the input and output is, and in a modern IDE like PyCharm its just one click away\nfor new engineers, typed code makes it much easier to start coding as its both obvoius from the code and there is more context provided by the IDE\n\nAre Python types Pythonic?\n\ntyping in python is optional and doesn’t impact runtime\nzen of python says explicit is better, reduce ambiguity and readability counts - and type hints delivers all this\nGuido said so\n\ntyping 101\n\nwhat needs to be annotated - variables going into a function\ncovers most: from typing import List, Set, Dict, Tuple\nUnion from typing import Union, Optional gives a way to return one of a list of objects like Union[User, Page, None] indicates any one of those three objects can be returned\n\ncan also use Optional to indicate return is optional\n\nfrom typing import Type, TypeVar\n\nTypeVar are placeholders\n\n\nmypy is the most commonly used type checker\nadvanced topics: forward references, TYPE_CHECKING, @overload\n\nPyre is facebook’s open source type checker, much faster than mypy\n\ntyping in the real world\n\nnew project, just type everything from day 1, use a type checker in CI\nexisiting projects: understand gradual typing. if a function isn’t annotated, then if it calls a annotated function then all type errors are ignored. You delay triggering errors until the calling function is also type hinted.\n\ntype common libraries, like utils.py which is used all over the place\nkeep running the type checker\nadd type checker into CI\nadd types to new code\n\nMonkeyType - created by Instagram\n\ncollects typing info at runtime and logs them, generates type stubs then applies them\n\n\ntips and tricks:\n\nsay you have a class, but you want to pass in a superclass - that will fail. use Protocol from typing_extensions\ntypeshed\ntype: ignore - silence typing error on a statement\n\n\nmy takeaway: types are pythonic, more readable, safer. use gradual typing and go all in!\n\n\n\n\nAn introduction on running Python web applications in Docker, covering how to structure your project, running the project in both development and production, testing the project, and compiling static assets for your frontend. #\n\n\n\nTim Heap @tim_heap git\ntalk code and slides\nDocker is a determinisitic application image, runs in a isolated execution environment, composable containers\nhow to structure your python app to make it easy to deploy in docker\n\nbuilt as normal, with some constraints\nruns as a WSGI app, connects to a web service like apache\nproject code all in one top level module, which can have submodules\na seperate directory for deployment and development\nhave one config file in the main project directory with all the base settings the app needs to run, seperate ones for development\ndev just hard code config variables, production pull in from the environment os.environ\n\nTesting\nyou might want some other tasks running in a seperate container to the web server\n\nq & a:\n\nwhy seperate config files for dev and prod?\n\ncause in dev you often need some weird things which you don’t need/want for production.\n\nhow do u manage logging in an app?\n\nconfigure it the normal way, then pipe it somewhere. its just running python so just do it in the normal way\n\nwhy we wouldn’t use docker-compose in production\n\nthere isn’t a way to gracefully bring it down and back up. In production use something like kubernetes which does all the magic\n\nprocess monitoring like supervisord\n\nsay u have a webapp image, a celery image, the supervisord runs those images. you would rarely run supervisord inside a docker container\n\n\nmy takeaway: Docker is convuluted. I wish the world would just rather clean up linux so we can go back to running things directly instead of having to faff around with docker containers.\n\n\n\n\nDid you know context managers go beyond with open(‘myfile.txt’, ‘r’) as f? In fact, you can even write your own! Context managers are an amazing tool for managing resources safely. They make your code look great, and they’re now easier to write than ever thanks to contextlib! Come get contextual! #\n\n\n\nDan git web\ntalk slides: dport.me/pycon.pdf\ncontext managers manage your context, like the famous with file as :\nwhy use them: you can’t forget to close resources and they make code prettier by abstracting\ncontexlib has all kind of goodies in it, so look it up\n\nfrom contextlib import suppress\n\ndef kill_process(pid):\nwith suppress(ProcessLookupError):\n  os.kill(pid, signal.SIGKILL)\n\nwrite your own context managers - see slide deck for examples\nits just a class with a dunder enter and a dunder exit method\ndecorator is just a func which takes a func and returns a new func\n\n@decorator on top of a func is just syntactic sugar for decorator(func)\n\ngenerators return one value at a time until they have nothing else to return\n\nthey maintain state until exhausted\n\nfrom contextlib import contextmanager decorator builds the enter and exit methods for you - the enter method is everything before yield, the the exit method is evreything after yield\nscope - variables defined inside context managers still exist after its closed\ncontext managers can deal with exceptions, can use suppress\n\nmy takeaway: use more decorators and context managers\n\n\n\n\nWhen thinking about where to start with python and games, the first thing that might come to mind is pygame. However, python has been used in many well known commercial games titles and can be used in many different ways throughout the game development process. This talk will examine a range of game titles, genres and platforms, from AAA to Indie, to show how python is being used in each; discussing the strength and weaknesses of using python, how it has been done, and how it might be in the future.\n\n\n\n@ducky_tape\nlots of python gaming platforms like Ren’Py, PyVida\nopen source tools like Gimp have python plugins\nPyAudioGame, Audio Game Kit for Python\n3d engines: panda3d - but need tools like Maya to make animations - python is very handy for scripting animations\npython is used a lot for automation inside games\npost release: for some big titles python is used to make addons\n\nmy takeaway: there is a lot of python out there.\n\n\n\n\nhow we learn to get better at our craft, and also how we – all too easily – do the opposite. #\n\n\n\ntwtr web\nbeen using Python since 2002! Python was hip way back then\njoined a super smart team, felt out of place\nnew knowledge deepens your understanding of what you already know\nwe are poor judges of whne we’re learning well or not\nreal learning feels sucky, like when trying to debug broken code with multiple layers of abstraction\neffortful retreival - have to write down stuff in your own words to understand. don’t absorb the words, absorb the knowledge\nusing is the the only way to make it stick.\nuse spaced repitition. flashcards are highly effective.\n\nbuid my own flashdeck\n\ndid a contract job for which had to learn ruby in 4 days. got two big fat books on ruby, used a pomodoro app on phone, and read both books cover to cover.\n\nrestate things in terms you understand\nabsorbed more than had expected\nprimed by layout of boks, which makes it faster to look up bits as needed\nlearning an unfamiliar language made him a better python developer\n\nsticking to only one language deprives you of experience\nself sabotage - when you think you’re smart, you tend to avoid things which make you look not smart. internalized that effort is whats needed when you’re not talented.\nCarol Dweck’s “fixed vs growth mindset” - are our abilities fixed or can we grow them?\nGrowth mindset: we can improve with effort, this is the one we want\nbe careful about excuses\nthinking I should have been been good at this by now is deafeatist\nyou can have a fixed mindset for ages about many things\nsitting down and working on a thing is an amazingly powerful way to learn that thing\ntakeaways:\n\neffort of learning isn’t comfy - if feeling comfortable learning its a warning sign\ndon’t feel bad about being bad at something\n\n\nmy takeaway: think you can do it, and learn by doing. don’t get comfy and don’t feel bad about sucking at something.\n\n\n\n\nDigital Earth Australia\n\narchive of Australia’s space/earth/ocean data\nthey try and figure out how to display things to see changes over time, showed impressive animationg of pictures alongside a graph of flow changing over time\ncode is all open source: GeoscienceAustralia github\n\nFuPy\n\nweb\nhardware is hard, FPGA makes hardware into software, making it easier to fix bugs\nFPGA generally don’t support python, but along came Migen and Misoc and LiteX which support python by putting a cpu in the FPGA\nFuPy project is to run MicroPython on FPGA\n\nflip flop operators in Ruby\n\n@merxplat\nflips b/w two statements\nits from perl, which got it from sed/awk, who got it from electronics\nbeing removed from ruby, no one uses it\nlooks pretty handy and terse to me!\n\npython in the classroom\n\nmath teacher who now teaches python to kids in a low income area\ncoding keeps kids engaged\nthey use grok learning\n\nDRF Model Pusher\n\nsends realtime updates using Pusher over websockets\ndjango stuff\n\nWatching water from space\n\nClaire, climate scientist at Geoscience Australia\nworks with agricultural data from space\nwhen are farm storage dams being filled and emptied\n\ndams were manually labeleed on satellite photos before\nnow algorithim to detect dams, run automatically every time there is new imagery\nthey build a flagging system to alert for drainage\n\nworking to improve this and pass it to stake holders\n\nCaptcha Cracker\n\nyr 12 student\nwe have so much info to learn from, but programs have very limited info\nwriting a program that uses the SIFT Algorithm to get through a ReCaptcha\nsee their website\n\nRocketry\n\nMEMS is making things a lot cheaper\nused MEMS sensors with ESP32 to make lots of telemetry modules\nof course used micropython\nlook up MQTT\nthe feeling of something actually happening is awesome\n\ncyrpto money\n\nbitcoin uses tons of electricity\ntheft\nethereum contracts cause software\ngreat overview of cycrptocurrencies\n\nPeter Lovett\n\nteaches Python\npython is readable, everyone loves it for that\npep505\npython needs to grow, but we don’t want to loose sight of what we have\n\nGiving thanks\n\nproject to fund the packages you use: phildini/thanks\n\n\n\n\n\n\nTracy Osborn is the author, designer, and self-publisher of three books and the solo founder of a venture-backed startup. Each of these achievements has something in common — being completely clueless about the work and problems involved in each. In this keynote, Tracy will tell stories about how she launched her projects and what she learned (after already being neck-deep.) #\n\n\n\ntwtr\nwrote easy/friendnly hello web app books\nunusual background to pycon. went through high school building websites.\nquit computer science in college cause bad experience, switched to art\nended up working as a designer and web developer - design/html/css - scared of javascript\nlearnt from cs experience how difficult it is to pick up programming and how important teaching styles are\nsee her djangocon 2017 keynote\nwanted to do a startup, didn’t want to code but couldn’t find a technical cofounder\nhusband works in python, introduced her to django. learnt django and launched her first webapp in 6 weeks\nyou don’t have to make things perfect, or understand how they work under the hood\nlessons learned:\n\nbarring security issues, ugly code can be ok\nlearn by doing - picked up programming much faster by having a real project to work on\nstartups are stressful but if you like to constantly be learning and doing something different most days they are great\n\nshe didn’t like the way django was taught, which is where her book came from\ncustomizable tutorials, project based - she teaches how to build a web app, not how to code\nwrote book instead of tutorial cause “go big or go home”. low royalites with publishers, so used kickstarter to raise 12k to self publish.\nlessons learned:\n\nkeep marketing in mind when building a product\ntook way longer then planned - 1yr late. build a big buffer\nkeep it simple- instead of markdown etc, used google docs - very useful for edit help, or sending drafts to ppl\nused indesign to lay out the book - used skillshare to pick it up.\n\nare paperback books worth it?\neasypost - api’s for shipping\n\nmy takeaway: pick a simple project and just do it.\n\n\n\n\nWhat do you think of when you hear “artificial intelligence”? Perhaps self-driving cars, autonomous robots and Siri, Alexa or Google Home? But it doesn’t have to be that complex. You can build a powerful image classification model within a topic that inspires and interests you - with 3 easy steps. #\n\n\n\nNorah Klintberg Sakal ln - data scientist doing a masters thesis at Chan Zuckerberg Biohub\ntalk github, includes code\nususally when u think of AI or DL u think of self driving cars, Alexa, cancer diagnosis\nbut we can apply things to day to day problems\nIdea - Data - Training\nwanted to look at makeup tutorials, too many on the internet, so how do you know which one of the 30M videos is relevant ($400B industry)\nIdea: use deep learning to answer this\nData: looked at videos, eye shapes jumped out - there are 4 distinct shapes\n\ncreated dataset by looking at celeb images and cropping out eye shapes - 200 training, 100 validation\n\nTrain: used transfer learning - pretrained weights from Imagenet (architecture VGG16)\n\nthere are a lot of models to choose from, used VGG-16 since its fairly simple to understand and easy to play around with\nlocked all the conv layers since they already knew how to understand images\nusing keras to grab VGG16 minus the top layer\ngot 93% accuracy\n\nApp: Flask to take photo and predict image\nan important part of learning is to make something end to end. take a simple problem and make a app for it\n\ntakeaway: this was suprisingly easy for something which sounds so complex. build my own mini app using keras and flask\n\n\n\n\nWe’ll start with the current crop of microframeworks, showing how to achieve the same task in each, before progressing to “Batteries included” and then the more specialised async frameworks. For developers who perhaps have only used a single framework or even none at all, this talk gives them an opportunity to get out and explore the world (of web frameworks) and broaden their horizons, with plenty of Jules Verne inspired fun along the way. #\n\n\n\nAaron Bassett twtr dev advocate at Nexmo - an api company for telephony\ntask: return a json string\nFlask - uses functional based views (does support class views too)\ncherrypy - uses class based views\nfalcon - api first framework, designed to do json based microservices and be really fast 5x faster than flask, 10x django\nhug - make development driven python apis as simple as possible - uses falcon under the hood, just adds a simplified interface to falcon.\nmicroframeworks make it very easy to do something simple, you also have the freedom to change things or bolt on whatever bits you want.\nbut with great power comes great responsibility - you start with great intentions and end up building a monolithic app\nin real world situations your fantastic ORM isn’t going to be usable by other ppl or well documented\nDjango in the real world, every other project is essentially structured the same. which leads us to django, which is batteries included.\n\nbut its somewhat complex - basic project has a bunch of stuff, and for a basic api you end up with multiple files to deal with\nprefers class based views, though some controversy there\n\nPyramid - start small - for basic stuff not much code, finish big - includes batteries if needed\nTornado - async framework, pretty small code\nSanic - flask like async, but can’t recommend it at the moment. prone to crashes and vulnerabilities\n\nmy takeaway: use any, ignore differences in syntax - whats important is documentation, bug/issue tracker activity, release management, community, and of course the scope of the project and the number of “batteries” you need. Don’t end up rebuilding a bastardized undocumented django on top of a microframework, just use django if thats where you’re headed.\n\n\n\n\nHave you ever eavesdropped on FP developers talking about programming and wondered which planet you landed on? I attended LambdaJam 2018 and felt your pain! Let’s demystify Either, Semigroups, Monoids, Functors, Monads, Traversable, Natural transformations etc. by implementing them in Python. #\n\n\n\nEugene Van den Bulke - twtr\nwent to lambdajam\nEugenia Cheng - read her books\nProfessor Frisby introduces functional programming in javascript - we are going to cover it in python\nFunctor: something that can be mapped over.\nCurrying: this makes no sense. instead of a func which takes in multiple arguments you have some weird chain of funcs which take in arguments one at a time\nApplicative functors: put a func in a box and apply to objects in other boxes\nEither Left of Right: basically a flipfloperator\nMonoid - ok now things are getting into the deep end\n\nmy takeaway: functional programming is interesting but deeply unpythonic. don’t use unless there is a clear need to.\n\n\n\n\nNot every design pattern makes sense in Python. This talk builds up design patterns commonly used in enterprise languages, and shows the features in Python that make these approaches unnecessary. #\n\n\n\nChristopher Neugebauer twtr web works at AlpaSigts, Director of PSF\nDeisign Patterns let you express ideas that are hard to express… in a familiar way\n\nmost patterns are object oriented ones from the mid 90’s, back when C and Java were the hot languages\n\nso how would design patterns be done pythonically?\nthe simplest design pattern is a Singleton\nSingleton: a class that can only be constructed once\n\ncan use del to delete class definition after having called it once to make sure it never gets called again\n\nbut why do we want a singleton? what do we want to achieve?\nback in the good old days Java needed singletons for namespaces, but python already has pretty good namespace seperation\npython can achieve this by using modules - so instead of classes we can have modules to hold data. Python makes sure you always get the same object regardless of how many times we import it.\nModules are objects, so we don’t need Singletons\nDependency infection: provide dependecies to classes as constructor arguments (useful for unit testing)\nMocks - pretend version of functions, most lanugages have mocking frameworks\nUnit Testing - unittest.mock has a patch function, so we don’t need to write our own mock funcs\nIterators - python provides for loops to consume iterators. But what are we trying to achieve? use yield to write generators for your own iterators\nperform a common operation on a collection - python does this natively with for item in Class\npython doesn’t need Visitors becuase generators\nthere are some design patterns which do make sense in python\nFactories - used all through the python library\nbook on useful patterns in python: python-patterns.guide\n\ntakeaway: a lot of design patterns exist becuase they were needed in some language or other. (java, shudder). Use python’s pythonic features, don’t reach for older patterns unless its truly a good idea.\n\n\n\n\nAudience members will be asked to go to a webpage on their phone that reads accelerometer data and transmits it to the presentation. This data will then be used to highlight the issues of collecting a processing data from distributed sensors - what happens when all the data is not received at once and not perfectly in time? what happens if there is an outage? How do you turn all this noise into something tha t can be managed? #\n\n\n\nMike Leonard twtr Reposit Power\npresentation demo fail - was hosted on kubernets cluster. Reminder that all this clustering business is hard.\ncollecting IOT sensor data is hard - too small a polling interval will ddos your server, but too long means loosing out on realtime data.\nduring an outage you want to keep data loss to a minimum - so log data on the device, but when they came back online you don’t want to get ddos’d. so each time a device fails to transmit, it doubles the delay, also introduce some randomness so each device is sending data at different times\nrealtime is tricky, scale is hard, distributed sensors are the worst\npython backend using Flask, Flask-SocketIO, React & Socket.IO + browser api\nrecommends using socket.io over websockets - python implementation\n\ntakeaway: think orchestration. how many devices, what when how do they log/send data and how to deal with loss/latency.\n\n\n\n\nThe presentation itself will go into the details of example security vulnerabilities, explain why it’s important to fix them, and show how integrating these two tools into your process will better protect you and your software. Beginners will get an appreciation for the kinds of security problems that can occur, and an introduction to continuous integration workflows. #\n\n\n\nTennessee Leeuwenburg twtr Head of Secure Coding at the Australian Bureau of Meteorology.\nuse python tools called Safety and Bandit\ntools like CVE track security vulnerabilities - there are heaps all the time in commonly used packages\nSafety tells you all the secure packages you are using using their cli tool or hosted service\nsome security vulnerablities are just “whoops I forgot to…”\nBandit helps us with these problems - its a security linter which checks for common security issues in your code\n\nvery handy for going through code to find code smells\ncan put into code so it runs with CI and raises alerts\n\nthe tools are noisy so you have understand what to pay attention to and what to tune out\n\ntakeaway: integrate Safety Bandits into CI.\n\n\n\n\nchunks() the story of a generator\n\nbreak an iterable into chunks of n - lots of implementations from stackoverflow\nitertools.islice slices iterators but returns an emptry iterator if gets an empty iterator\ncode at gist.github.com/timheap\n\nPyCon Anthology\n\nwrite a short story, art, poetry, pg-13, deadline Sep 1\nsubmit through github\n\nTracking trucks in Africa\n\nLori Systems\ntracks trucks, but how to do it best? truckers in Africa don’t have reliable phones, handing out phones didn’t really work\nTraccar - java based IOT gps tracking system for trucks\nZappa is an alternative to celery\n\nSoftware release reports with Python Sphinx & Jira\n\nCochlear, heavily regulated, need a software release report: bugs fixed, improvements, known bugs\nused MS Word, so time to automate. Data in Jira, which has a python interface.\npython script gets info from Jira\n\nwhy text encoding\n\nwe need to encode text - so whats text encoding\n\nphy py physics\n\n@cormacKikert\nA 3D physics sandbox game built using pygame\ncool demo - [code on github]9https://github.com/cormackikkert/Py-PHY)\n\nPython Bugs\n\nfirst lightening talk\n\nBad code\n\nexamples of over engineered code\n\nGoto\n\nin yr10! doing a arduino assigment\nmissed goto, wrote his own in python\n\nFlip Flip Face Offerator\n\na flipfloperator faceoff\n\nMicropython.. Jupyter.. Live\n\nmicropython is re-inventing embedded development\nbut theres no debugging in micropython\ncan talk to micropython from jupyter, use interactive widgets\nlook up jupyter-micropyton-remote - very impressive\n\nNick - core CPython dev\n\ntalk about Peps which introduced now accepted features"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#how-python-saved-a-rescue-dog---a-foster-fail-story",
    "href": "posts/writing-about-code/PyConAu2018.html#how-python-saved-a-rescue-dog---a-foster-fail-story",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "This talk will tell the story of a foster fail, how Python helped to save the life of a rescue dog and how the initial medication feeder grew from a single IoT device into a full Internet of Dog (IoD) madness. I will show the design and implementation of the initial Python-based medication feeder, what we have learned from running it over the summer and how it has continued to grow into a multi IoT device, Python-powered, full dog carer solution. There will be microservice architecture drawings, Python code, and, of course, pictures of the dog! #\n\n\n\nAndreas Moll - twtr - Works at ANSTO on scientific software in Python\nwere fostering a dog, Willow - fostering: look after for a limited time and prepare for adoption\nWillow was badly treated by previous owner and needed anti-anxiety medication every 8 hrs\n4 weeks of meds, improved Willow a lot. But then they both went back to work, and nobody had adopted Willow. So they had two issues: monitor Willow and afternoon medication\nmonitoring was easy - used a webcam. Considered off the shelf dispensers, but didn’t like\nOnion Omega2+ attached to a servo which dropped the treat+drugs on time\nFirst drug feeder:\n\nVM in the cloud - wrote a flask webapp, backed by a Redis database. The webapp sent actions Feed|Home to the VM. The Onion has an endless loop which polls the VM every second asking for an action, and then does it.\npolling is a very simple scheme, and if anything breaks down, nothing bad happens, but there is a lag b/w button and action, as well as the dependency on the IOT and VM in cloud\nusb audio using alsa libraries which would call the dog\n\nthe IOT was only part of the puzzle - they had to train the dog as well. Ran this setup for a couple of months, worked suprisingly well. Sometimes failed becuase of power outages, so they added a UPS.\nWillow didn’t get adopted - so they adopted her. Now that they were going to have her, they decided to get serious about the IOT\nSplit into infrastructure and service\n\nDevice: Raspberry Pi Zero connected to I/O, sensors, a Pi3 running gateway services, docker registery, consul server\nService: Web service exposing a RESTful API running on flask, service discovery using Consul, packaged in a docker container and depolyed using docker-compose\n\npi zero is nice and simple, is using one per physical device/sensor\nthe gateway service polls the VM for actions, then does a REST call to the relevant PiZero to do the action\nFuture: websockets instead of polling, wants things event driven and real time readings for sensors. web interface with react\n\nQ&A\n\nWhy not docker swarm?\n\nwanted to use consul\n\n\ntakeaway: genius AND way overkill. but infrastructure is hard. consider using orchestration tools."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#lighting-macro-photographs-with-circuitpython",
    "href": "posts/writing-about-code/PyConAu2018.html#lighting-macro-photographs-with-circuitpython",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "LED lighting rigs are expensive. Worse, they have little to no controls aside from on/off. Most are not dimmable and changing colors requires the use of gels. In this talk I will discuss how CircuitPython was used in conjunction with LEDs and microcontrollers to make a custom LED photo lighting rig. #\n\n\n\nStacey Morse - geekgirlbeta, web is an artist who also does macro photography, web dev in python\nart is awesome, but led lights are very expensive and crap. the intersection b/w art and tech: both are very process drivenl\nhad a conversation with a python programmer who suggested python controlled lights could solve her lighting issues - and thus she started to learn how to program in micropython\nmacro photography is very technical - there is a lot of math involved, understanding light, lens dynamics, lighting\nlights: on/off, select number of leds, control brightness, colour\nhardware: microcontroller, leds, rotary encoder, buttons, ir remote\nsoftware: CircuitPython (very beginner friendly), MicroPython\nwent with neopixels for the lights - gives true white light as its RGBW\nused Feather M4 Express for the microcontroller\nthe code is very straightforward\n\nQ&A:\n\nhot shoe\n\nfigures she can use the hot shoe in a camera to communicate with the light - so she can write light settings in the exif data\n\nwhat can she do with this which she can’t do with a traditional flash kit?\n\nled’s blink, so u need them in blink in coordination with your camera settings. this allows her to focus on a specific part of an object. traditional flashes are bigger, whole panels of lights\n\n\ntakeaway: building a real tangible project is fun. Curcuit Python looks super easy to use."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#writing-fast-and-efficient-micropython",
    "href": "posts/writing-about-code/PyConAu2018.html#writing-fast-and-efficient-micropython",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "MicroPython is a reimplementation of Python which is specifically designed to run on computing devices that have very few resources, such as CPU power, RAM and storage. Often when you write scripts in MicroPython you want to make the most of your available resources, and have code run as fast as possible (faster code usually saves power, which is important when running from a battery!) and there are certain ways of writing MicroPython code that are more efficient than others. In this talk I will go over the tricks and techniques for writing fast and efficient Python code in MicroPython. As part of this I will delve into some technical details of how MicroPython works, in order to better understand what it’s doing behind the scenes and how to make the most of it. I will discuss general techniques for making things run faster (some of which would be applicable to normal Python), as well as ways to completely avoid memory allocation, which is important for both efficiency and making code execution deterministic. The talk will include some hardware demos to show off the techniques, including five different ways to blink an LED fast. #\n\n\n\nDamien George - creator of MicroPython\nloading local variables is fast, global variables is slow. So try to use local variables.\nits slow to call functions from within a function\nsome things allocate memory on the heap, others don’t - try not to allocate memory.\ndon’t allocate memory: All the basic statements like if/else, local variables, int arthimetic, some builtin funcs like any, len, etc\ndoes allocate memory: importing, defining functions and classes, global variables, data structures\ntips to reduce cpu time:\n\nuse funcs - for example put a loop within a func and call the func\ndon’t use global, try to use local variables as much as possible\ncache funcs\ncache variables as local\nprefer longer expressions, not split up ones\nuse consts from micropython import const\n\ntips to reduce ram usage:\n\ndon’t use heap when possible\nuse short variable names\ntemp buffers\nuse XXX_into methods\ndon’t use 8 or ** args\nuse const, minify scripts\nuse mpy-cross to produce .mpy - this saves a lof ot time in compiling the script on the device\nultimate solution is to freeze scripts into the firmware (bytecode gets stored into flash storage, advanced)\n\nwithin a func, preload methods on = led.on and then use on(). So when calling a method multiple times this is much faster.\nfor small loops, unroll the loop\nviper mode - undocumented, being written, writes registers directly to say turn on/off leds\ncan also write inline assembler in python syntax\npre allocate a buffer and write to it\n\ntakeaway: micropython can be fast. don’t use globals, but everyone knows that already, so instead apply some of the tricks from above, like caching methods, and for micropython don’t have too many funcs."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#asyncio-in-micropython",
    "href": "posts/writing-about-code/PyConAu2018.html#asyncio-in-micropython",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Asyncio provides a way to achieve concurrency in a relatively simplistic fashion. However, first-time users still struggle with the concepts so let’s sort them out! Then we’ll see why it’s especially useful in an embedded environment.#\n\n\n\nMatt Trentini - a software engineer, has worked with lots of projects with embedded firmware with c compilers, has been looking for better alternatives\nmicropython is the way forward for embedded development\nCoroutines - the ability to run things concurrently\n\nuse yield points to relingquish control and an event loop to scedule tasks\nprovides cooperative multitasking\nconcurrency isn’t parallelism\n\na lot of async funcs is similar - just needs async def func and await where are are awaiting stuff.\ncoroutines (tasks) --&gt;&lt;-- event loop --&gt;&lt;--- blocking operations (network access, sleep, database queries)\nwhat about threads? wasn’t this a solved problem? threads provid pre-emptive multi-tasking, but they get complex, are heavy (each thread needs its own stack) and hesienbergs\nCoroutines are better cause:\n\nminimal overhead, high performance\nlocking is less problematic\nmore closely reemble synchrous code\n\nCoroutines in python are biult on generators and use async/await keywords\nPEP 492 in Python 3.5 brought async\nPython needed async to stay relevant and competitive with other async languages\nasync needs all the funcs to support async code, so hard to use with older synchronus code\nasync is in active development\ncoroutines in micropython is a subset but for practical use the same\nasync in embedded devices is exciting as threading support is limited and many embedded operations are cncurrent by nature\nclassic problem in embedded space: Debouce\n\nneed to wait for a button to stop bouncing - since they’re typically not digital\nasync lets us write a loop which pools the button, and when a button is pressed, it can await the sleep so the controller can do other things instead of just waiting on the button\n\nthere is micropython-async - look up tutorial from Peter Hinch\nuPTV - counts down minutes to next train\n\nNTP - time support import ntptime\nHTTP request - uses a requests library - python has aiohttp, micropython has a very crude version\nLED control\nasyncio to create loop and run it forver - sync time, poll for train, update LED\n\nLED strip controller\n\nrotary encoder to set brightness and push on/off\nuses a async fade routine to smoothly fade to brightness\nasync means can change the brightness while still fading, so no waiting\n\n\nQ&A\n\nppl using coroutines for real time?\n\nin experimental stage - if u request 5ms sleep, it can be less or more depending on what the other coroutines are doing\n\nhow is it implemented?\n\nin micropython uses queues implemnted in C\n\ntiming and checking length of executable paths\n\nup to you to define - for example a fade routine might have a 5ms delay b/w levels so await sleep(5ms)\n\nhow do you deal with a blocking library?\n\nput timeouts on things - call a func, if it takes too long to run, interrupt it, or use threads - async can push things into threads\n\nwith asyncio, do you apprach problems differently from threads?\n\nasync feels more natural - you can state what you want to do. With threading you have to think a lot about synchronization and how long things will take.\n\n\ntakeaway: get micropython hardware and start coding. Understand async better, it seems pretty straightforward, and on slow hardware a godsend - i.e async sending logs + async doing other stuff makes for easy real time monitering even if the machine is stuck on something.."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#demystifying-lorawan-with-pycom",
    "href": "posts/writing-about-code/PyConAu2018.html#demystifying-lorawan-with-pycom",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Connecting IoT devices using low power over wide area wireless (LoRaWAN) makes sense. But how LoRaWAN works, duty cycles, frequency plans, receive windows, etc. doesn’t. #\n\n\nThis talk will demystify how LoRaWAN works using PyCom devices.\n\n\n\ntalk by Brian Danilko of Likeable Software\nLoPy\nPycom has lots of examples for LoRa and LoRaWAN\nLoRa is the transport layer - patented, developed by Cycleo, Semtech owns it\n\nuses a Chirp Spread Spectrum (CSS) to transmit info reliably using low power\nuses different power levels, frequencies, bandwidths, spread factors to optimize power use vs time on air vs distance\n\nLoRoWAN adds the wide area network on top of LoRa\n\nconnects end nodes to the internet through gateways\nencrypts for secure transmission, controls what devices can join the network, confirms and retries (but this doubles power and clogs airways so don’t use if not needed)\n\nyou have multiple end nodes talking to one gateway, which connects to the internet via a wired link (can be wireless too)\ncan have multiple gateways for backup - so nodes can connect to multiple gateways\njoin a LoRaWAN network by OTAA (over the air, same keys embedded in all devices) or ABP (unique keys)\nfrequencies for gateways and nodes have to be the same - something to be aware of when setting up devices\n\ntakeaway: look at LoRo/PyCom if using wireless IOT away from wifi."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#workplace-environment-sensing-with-python",
    "href": "posts/writing-about-code/PyConAu2018.html#workplace-environment-sensing-with-python",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Have you often wondered where the quietest spot in the office is right now? In this talk, we explain how we built a real-time system that does just that using CircuitPython. #\n\n\n\nAiden Ray, works at BVN\nbuilding awards are handed out before ppl use them - very little feedback from real users\nthe web is totally instrumented, you know exactly whats going on\nthe physical world is a lot less instrumented\nAmaon Go is sensored everywhere - this is where the world is moving, you get tons of data, can see where to improve\nCircuit Playground Express by Adafruit\nC/C++ vs Python boils down to runtime speed vs development speed\nCircuit Python is Adafruits fork of MicroPython, more beginner focused, doesn’t have access to async, so you have a basic event loop\nif sys.implementation == 'circuitpython' to check implementation\n\ntakeaway: pervasive monitoring is creepy but its going to happen. Circuit Python is amazingly easy. Make something with it."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#automating-your-home-with-python-raspberry-pi-and-homekit",
    "href": "posts/writing-about-code/PyConAu2018.html#automating-your-home-with-python-raspberry-pi-and-homekit",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Home Automation is a fun new field for the modern Pythonista. In this talk I will be walking through how a developer can leverage a python library to use the HomeKit service and automate the devices in their home. I will be covering topics like hardware selection for local and remote access, HomeKit service registration and management and potential security concerns around IoT. #\n\n\n\nSean Johnson twtr ln\nhomekit is a software framework for controlling smart hardware, built on Apples HomeKit Accessory Protocol\nthere are a few opensource implementations in node and python\napple predefines hardware configurations, you can’t define your own\nHome App allows manual and automated control of devices\nthere are many existing hardware, from ikea to phillips\nRaspberry Pi - supports python\nuses HAP-Python, demo of a controlling an aircon via infrared\npretty straightforward, except tied to Apple and Siri\n\nq & a\n\nIR is unidirectional so what happens if u miss a packet?\n\nevery packet is single state, so contains all the info in one go. So if a packet fails, the next one doesn’t rely on it. Also, leep transmitter close to the receiver so the signal is clear.\n\n\ntakeaway: No Apple, hence no homekit for me."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#education-seminar-student-showcase",
    "href": "posts/writing-about-code/PyConAu2018.html#education-seminar-student-showcase",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Friday August 24 2018, Education Track, C3.4 & C3.5, 16:00 AEST\n\nEight short (10 min) talks from high school students across Australia. They’ll be talking about projects they’ve built with Python using machine learning, robotics, natural language processing, and more #\n\n\nMENACE - building a learning matchbox machine in Python\n\nmachine learning is the use of programming to make a computer learn from data\ninspired by Matt Parker from Standup Math channel on youtube\nproblems with indexing errors leading to endless loop\nwants to use pygame or kivy to make a actual game\n\ntakeaway: awesome.\nOptimising Memory Retention via a Machine Learning based Flashcard System built in Python\n\nThis project aims to leverage Python’s machine learning capabilities, combined with psychological theories of learning and forgetting, to construct predictive models of human memory in order to improve upon traditional flashcard systems.\n\n\nIn this talk, I will share my experience of: (1) utilising Python’s sci-kit-learn package, alongside the Latent Skill Embedding package, to train, evaluate and visualise the performance of various models; (2) implementing the model into a web-based flashcard application built in Flask - a popular Python micro framework; and (3) testing the effectiveness of the system through a classroom experiment on 36 Japanese language students.\n\n\nhttp://josephtey.com/\nresearched various techniques for the optimal time to review\nexperimented on 40 students\nstarted with R, but how do u deploy with R? so moved on to Python as it can both train a model and an app to serve it with\n\ntakeaway: build my own flash cards to learn stuff with spaced repetition.\nText Summariser\n\nThe Text Summariser is a program I built for when one is unable or unwilling to summarise information from a large block of text themselves. In my talk, I will discuss how it works, what inspired the project, and how I overcame the (many) challenges of building my program. I will also talk about computational linguistics and Natural Language Processing (NLP), two big components of how the text summariser works. After listening to this talk, you will have learnt some basic Natural Language Processing, and how you can apply it in Python Programs.\n\n\nsorts words by type - verbs, nouns, etc\nuses markov chains to summarize\n\ntakeaway: too much cleverness going on to summarize.\nNOR: creating generated worlds on iPad\n\nNOR is a 2d puzzle exploration game for iPad that I made over the course of year 10. It features procedurally generated landscapes that collide with and can be edited by the player. The landscapes are host to procedurally generated bushes, trees and puzzles. My talk will discuss how I reached the point where I could set off on a large scale python coding project, how I built up the game and made the systems work, and how anyone can pick up an iPad and start developing.\n\n\nlearned calculus for the game movements etc\nthings are based on each other\nuses monte carlo to generate gaphics, of a random seed, which are dependend on the ground and level number\nused pythonista\nuses momemtum + inertia for players movement\n\ntakeaway: procedural generation is awesome.\nRule-Based Machine Translation\n\nThe Text Summariser is a program I built for when one is unable or unwilling to summarise information from a large block of text themselves. In my talk, I will discuss how it works, what inspired the project, and how I overcame the (many) challenges of building my program. I will also talk about computational linguistics and Natural Language Processing (NLP), two big components of how the text summariser works. After listening to this talk, you will have learnt some basic Natural Language Processing, and how you can apply it in Python Programs.\n\n\nuses machine translation to switch from english to latin\nfour main ways to translate: rule based, example based, statistical, neural\nGoogle Translate sucks at Latin translation\nvery impressive work\n\ntakeaway: NLP for the win.\nSVG Graph Calculator\n\nThe name of my project is somewhat self-explanatory, it is an SVG Graph calculator. I know right? My talk is going to be about how I decided to do this project, and my struggles and innovations in making this project happen.\n\n\nturns an equation into a SVG graph\nWhy? had to draw graphs for yr9 math class. There are online tools which do this already, but decided it would be fun to do this\nthought of writing it in python turtle, but too slow\nwanted something which worked on web\nhad to reverse engineer SVG file\nif looking for inspiration, just copy something\n\ntakeaway: good basis for building a equation solver.\nEmojifer in @ school\n\nEmojifer is an implementation of a sequence model in Machine Learning. It will analyse the meaning of a sentence and give it the appropriate emoji. Emojifier plays an important role in @ school which is a cloud-based learning management system written in React with Flask served as the server. Come along to this talk, if you want to know what’s under the hood of Emojfier and how I make it happen. Additionally, I’ll talk about some of the problems I’ve encountered so far and how I overcame it. This talk will give you an idea of how to get started in Machine Learning as well as full-stack web development if you’re new to the area.\n\n\nhttp://emojifier.surge.sh/#/\nhttps://github.com/anhphamduy/emojifier_model\nuses RNN and LSTM\nkeras and flask\nmade his own training data\nlessons: best way to learn is to teach, jump in head first, pair program, stack overflow is awesome\n\ntakeaway: everyone needs a emojifier.\nPyVlov’s Dog\n\nPyVlov’s Dog is a simulation software created to dynamically train neural networks for the control of basic robots. When we started this project we had a limited understanding of neural networks, so we simplified the problem by visualising it as training a dog.\n\n\nWe developed this project as a way to streamline the concept of neural networks through this metaphor of training a dog, so as to facilitate its use as an education tool. In this talk we will talk about how we made the software, the difficulties we faced in creating it and the way in which it works. We will also discuss the development of the robot’s hardware and the way in which it is represented within the simulation. Finally, we will cover the way in which we developed the principles of ease of use, and visual clarity within the program, to allow for use by people of all skill sets.\n\n\npurpose of project was to understand neural networks, and did this through training a dog to do what its told to do (by creating a set of rules)\nsensors are the input to a NN, which gives a output to a motor\nmade a simulation to train the NN - user defines a dog and an environment, once happy with the NN, throw it into the actual robot\nuses arduino to control motors, raspberry pi to run NN\nhardware is hard!\nused pymunk to handle the physics of the sim, but is only 2d, so they had to work around that\nneeded to use tkinter with pygame\nhttps://pyvlov.wordpress.com/\n\ntakeaway: interesting project to follow. The future is training your own robot to do something.\neducational talks wrap up\n\nhigh school students are doing amazing stuff\nlearnt a lot from throwing themselves in the deep end to do a project.\npython is a fantastic learning language, don’t babyfeed students, support them in the deep end\n\n\nSaturday talks"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#keynote-annie-parker-on-techfugees",
    "href": "posts/writing-about-code/PyConAu2018.html#keynote-annie-parker-on-techfugees",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Annie is a co-founder of Techfugees Australia - a global movement connecting the technology ecosystem together with newly arrived refugees here in Australia to help them integrate into their new communities. In this talk, Annie will be sharing her experience of how Techfugees works and some of the success stories they’ve had along the way #\n\n\n\ntechnology has a huge impact on changing peoples lives\n68.5M displaced ppl around the world, with 25M refugees\nthe tech community can help, done 5 hackathons so far\n6th hackathon 24/25th Nov in Blacktown - signup\n\nlooking for ppl who can put togther really early stage stuff and know whats out there to use\ndon’t need to be deeply skilled\n\nthey’ve made significant changes - new companies, connected investors to customers, jobs, community impact, media stories, inspiration\ncase studies:\n\nrefugeetalent created at first hackathon in 2015, placed 150 ppl into jobs\nonestep founded at second hackathon, connects refugees\nmuralisto - makes community art\n\nShaqaeq Rezai ln, twtr - 18yr hazara Afghan refugee, moved to Australia from Iran where she faced a lot of discrimination - attended a hackathon, built an AI to track moods, got scholarship, started Directa Australia. Met the CEO of the Iconic through the startup scene, who is also Aghan.\n\nwants to help ppl through tech\n\nmost refugees have good knowledge of tech, smart phones\n50% of business in Australia have been started by a refugee or a migrant\n\nmy takeaway: this is awesome, go to their hackathon in Nov. Don’t need ninja skills, a lot is just about knowing what is out there in the tech space which can help."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#describing-descriptors",
    "href": "posts/writing-about-code/PyConAu2018.html#describing-descriptors",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Descriptors are a little known feature of Python. They provide a way for a programmer to customize the storage and retrieval of different instance variables. In this talk, you will learn about the descriptor protocol, what it can be used for, and how to implement a descriptor. #\n\n\n\nMatthew Egan web twtr\nwhy descriptors? often overlooked, used primarily by library developers\nso we have a basic class, Person, which stores info like name, etc\nour dream Person class makes sure names are capitilized properly\n__setattr__(self, attr, value) is called inside a class when an attribute is being set, so if Person.name is assigned, it runs through the setattr method which can can modify the variable from name to Name\ncan use setter methods like @name.setter inside the class, but this only applies to the specific class\nthe descriptor protocl allows a more general solution, has four attributes: get, set, deelte, set_name\n\n__get__(self, instance, owner)\n__set__(self, instance, value)\n__delete__(self, ??)\n__set_name__(self, owner, name) - doesn’t take instance argument as its only called on class creation\n\nwhen you access an attribute in python, it goes through a lookup process, which gets it from a dict, a data descriptor or a non_data descriptor\nnon-data descrptors: staticmethod, classmethod, abc.abstractmethod\nif we have a Person class and tons of Person objects, we write just one descriptor which covers all the objects\nfrom weakref import WeakKeyDictionary - when we delete an object which is also in the dict, it automatically removes it from the dictionary\nlook up some example descriptors using weakref - he gave an example of using a descriptor to make sure names are capitilized\ndescriptors have use cases like:\n\nreplicating customization for multiple attrs without caring about name\ndjango generic foreign keys\ncustom validation e.g only non negative integers for age\nbetter error msgs - can prepend the name of attribute being set to the error msg so its more descriptive\n\ntalk code: https://github.com/mattjegan/describing-descriptors\n\nq & a:\n\nruntime perf\n\ndepends on impementation, external calls\n\ncan you chain descriptors?\n\nshould be possible, but haven’t done it.\n\nhow do u replace the descriptor itself?\n\ndelete it off the class itself\n\nwhat happens if you pickle an object with a descriptor?\n\npickle is unsecure so don’t use it\n\n\nmy takeaway: descriptors are useful, can apply across classes, so makes for better code"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#what-is-the-most-common-street-name-in-australia",
    "href": "posts/writing-about-code/PyConAu2018.html#what-is-the-most-common-street-name-in-australia",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Finding the most common street name in Australia may sound like a simple thing to do - but it quickly devolves into a scenic tour of all the things that go wrong when doing data analytics. I’ll be giving advice on how to avoid these speed bumps along with how to work with OpenStreetMaps in Python. #\n\n\n\nRachel git, twtr, ln works at Solar Analytics\nread an article about most common street names in America, got her interested about Australian streets\nG-NAF is australian geocoded data containing every address/street\nused OSM, since it has every country, instead of G-NAF\ncouldn’t just export all of Australia data from OSM\nused metro extracts to download geojson, which has now gone offline\ngeojson includes a geometery feature e.g roads defined as a series of points (a line!)\nused geopandas - works like pandas\nosm has all kinds of road types, so have to think about it, so what exactly is a street? She went with a street is something with a name which a car can drive on\nsee her blogpost on cleaning OSM data\nshe got victoria rd and pacific highway as most common street names in Sydney, which didn’t sound right\nso drew a plot, and found out OSM saw streets as segments, so long streets appeared many times\nshapely is a package which merges line segments together, but didn’t do a great job\nso she wrote her own function to knit streets together - get road segments with same name, find ones which are close together, merge\nthere is a distance func to find distance b/w geometeries\nhad to parse names, as street names have a name and a descriptor, like parade, street, avenue, etc. This was not easy as there are a lot of variations, too many!\nwhat about Little Street? is that same as Street?\nshe wanted to use this for different countries, but each country has different street descriptors, so the Australia model didnt’ work for other countries\nused overpass turbo api to get all of Australia’s data - but was hitting api limits so had to break Au into a grid and make multiple calls\nmost common street names in Australia: Park, Railway, George, Church, Victoria\nlearnings:\n\nstart small - Sydney over Australia\nchoose something familiar\ncheck your biases\nconstant vigilant - test, draw stuff (she plotted streets on a map)\nknow your problem\n\n\nmy takeaway: a suprisingly simple question can lead to a whole lot of learning. Answer a few simple q’s myself using python."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#why-you-should-care-about-types-how-python-typing-helped-my-team-scale",
    "href": "posts/writing-about-code/PyConAu2018.html#why-you-should-care-about-types-how-python-typing-helped-my-team-scale",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "By now you have probably all heard about Python static typing. But why should you care? Are types in Python even Pythonic? Is Python turning into Java? Type annotations are Pythonic, trust Guido’s word for it, and Python is definitely not turning into Java.\n\n\nThe greatest benefit of types in large Python codebases is the fact that the input and output structures of a function are obvious from just looking at the signature. In the untyped world the definition for the class you are looking for may be N jumps away, hidden somewhere deep in the codebase, and you don’t have a direct reference to it. In the best possible case grepping for it will yield just a few results and you will be able to spot what you are looking for. In the worst case though, you will have hundreds of hits and you will have to start your application and inspect the type at runtime to figure out what is going on, which make the development cycle slow and tedious.\n\n\nCome to this talk if you want to know more about the typing system in Python, how to gradually add it to your codebase and what benefits will your team get in the long run! I will also cover some advanced tools like the runtime type collection system, MonkeyType, and the just open sourced type checker, Pyre\n\n\n\nLuka Sterbic git - works at Facebook\ntalk slides and code\nwhy should I care about typing?\n\nyou can call modules from far away, in a large codebase can be many imports away, so its not clear what kind of data object gets returned\nwith typed codes, its obvious what the input and output is, and in a modern IDE like PyCharm its just one click away\nfor new engineers, typed code makes it much easier to start coding as its both obvoius from the code and there is more context provided by the IDE\n\nAre Python types Pythonic?\n\ntyping in python is optional and doesn’t impact runtime\nzen of python says explicit is better, reduce ambiguity and readability counts - and type hints delivers all this\nGuido said so\n\ntyping 101\n\nwhat needs to be annotated - variables going into a function\ncovers most: from typing import List, Set, Dict, Tuple\nUnion from typing import Union, Optional gives a way to return one of a list of objects like Union[User, Page, None] indicates any one of those three objects can be returned\n\ncan also use Optional to indicate return is optional\n\nfrom typing import Type, TypeVar\n\nTypeVar are placeholders\n\n\nmypy is the most commonly used type checker\nadvanced topics: forward references, TYPE_CHECKING, @overload\n\nPyre is facebook’s open source type checker, much faster than mypy\n\ntyping in the real world\n\nnew project, just type everything from day 1, use a type checker in CI\nexisiting projects: understand gradual typing. if a function isn’t annotated, then if it calls a annotated function then all type errors are ignored. You delay triggering errors until the calling function is also type hinted.\n\ntype common libraries, like utils.py which is used all over the place\nkeep running the type checker\nadd type checker into CI\nadd types to new code\n\nMonkeyType - created by Instagram\n\ncollects typing info at runtime and logs them, generates type stubs then applies them\n\n\ntips and tricks:\n\nsay you have a class, but you want to pass in a superclass - that will fail. use Protocol from typing_extensions\ntypeshed\ntype: ignore - silence typing error on a statement\n\n\nmy takeaway: types are pythonic, more readable, safer. use gradual typing and go all in!"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#running-python-web-applications-in-docker",
    "href": "posts/writing-about-code/PyConAu2018.html#running-python-web-applications-in-docker",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "An introduction on running Python web applications in Docker, covering how to structure your project, running the project in both development and production, testing the project, and compiling static assets for your frontend. #\n\n\n\nTim Heap @tim_heap git\ntalk code and slides\nDocker is a determinisitic application image, runs in a isolated execution environment, composable containers\nhow to structure your python app to make it easy to deploy in docker\n\nbuilt as normal, with some constraints\nruns as a WSGI app, connects to a web service like apache\nproject code all in one top level module, which can have submodules\na seperate directory for deployment and development\nhave one config file in the main project directory with all the base settings the app needs to run, seperate ones for development\ndev just hard code config variables, production pull in from the environment os.environ\n\nTesting\nyou might want some other tasks running in a seperate container to the web server\n\nq & a:\n\nwhy seperate config files for dev and prod?\n\ncause in dev you often need some weird things which you don’t need/want for production.\n\nhow do u manage logging in an app?\n\nconfigure it the normal way, then pipe it somewhere. its just running python so just do it in the normal way\n\nwhy we wouldn’t use docker-compose in production\n\nthere isn’t a way to gracefully bring it down and back up. In production use something like kubernetes which does all the magic\n\nprocess monitoring like supervisord\n\nsay u have a webapp image, a celery image, the supervisord runs those images. you would rarely run supervisord inside a docker container\n\n\nmy takeaway: Docker is convuluted. I wish the world would just rather clean up linux so we can go back to running things directly instead of having to faff around with docker containers."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#context-managers-you-can-write-your-own",
    "href": "posts/writing-about-code/PyConAu2018.html#context-managers-you-can-write-your-own",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Did you know context managers go beyond with open(‘myfile.txt’, ‘r’) as f? In fact, you can even write your own! Context managers are an amazing tool for managing resources safely. They make your code look great, and they’re now easier to write than ever thanks to contextlib! Come get contextual! #\n\n\n\nDan git web\ntalk slides: dport.me/pycon.pdf\ncontext managers manage your context, like the famous with file as :\nwhy use them: you can’t forget to close resources and they make code prettier by abstracting\ncontexlib has all kind of goodies in it, so look it up\n\nfrom contextlib import suppress\n\ndef kill_process(pid):\nwith suppress(ProcessLookupError):\n  os.kill(pid, signal.SIGKILL)\n\nwrite your own context managers - see slide deck for examples\nits just a class with a dunder enter and a dunder exit method\ndecorator is just a func which takes a func and returns a new func\n\n@decorator on top of a func is just syntactic sugar for decorator(func)\n\ngenerators return one value at a time until they have nothing else to return\n\nthey maintain state until exhausted\n\nfrom contextlib import contextmanager decorator builds the enter and exit methods for you - the enter method is everything before yield, the the exit method is evreything after yield\nscope - variables defined inside context managers still exist after its closed\ncontext managers can deal with exceptions, can use suppress\n\nmy takeaway: use more decorators and context managers"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#snakes-in-your-games",
    "href": "posts/writing-about-code/PyConAu2018.html#snakes-in-your-games",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "When thinking about where to start with python and games, the first thing that might come to mind is pygame. However, python has been used in many well known commercial games titles and can be used in many different ways throughout the game development process. This talk will examine a range of game titles, genres and platforms, from AAA to Indie, to show how python is being used in each; discussing the strength and weaknesses of using python, how it has been done, and how it might be in the future.\n\n\n\n@ducky_tape\nlots of python gaming platforms like Ren’Py, PyVida\nopen source tools like Gimp have python plugins\nPyAudioGame, Audio Game Kit for Python\n3d engines: panda3d - but need tools like Maya to make animations - python is very handy for scripting animations\npython is used a lot for automation inside games\npost release: for some big titles python is used to make addons\n\nmy takeaway: there is a lot of python out there."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#keynote-tom-eastman-on-getting-better",
    "href": "posts/writing-about-code/PyConAu2018.html#keynote-tom-eastman-on-getting-better",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "how we learn to get better at our craft, and also how we – all too easily – do the opposite. #\n\n\n\ntwtr web\nbeen using Python since 2002! Python was hip way back then\njoined a super smart team, felt out of place\nnew knowledge deepens your understanding of what you already know\nwe are poor judges of whne we’re learning well or not\nreal learning feels sucky, like when trying to debug broken code with multiple layers of abstraction\neffortful retreival - have to write down stuff in your own words to understand. don’t absorb the words, absorb the knowledge\nusing is the the only way to make it stick.\nuse spaced repitition. flashcards are highly effective.\n\nbuid my own flashdeck\n\ndid a contract job for which had to learn ruby in 4 days. got two big fat books on ruby, used a pomodoro app on phone, and read both books cover to cover.\n\nrestate things in terms you understand\nabsorbed more than had expected\nprimed by layout of boks, which makes it faster to look up bits as needed\nlearning an unfamiliar language made him a better python developer\n\nsticking to only one language deprives you of experience\nself sabotage - when you think you’re smart, you tend to avoid things which make you look not smart. internalized that effort is whats needed when you’re not talented.\nCarol Dweck’s “fixed vs growth mindset” - are our abilities fixed or can we grow them?\nGrowth mindset: we can improve with effort, this is the one we want\nbe careful about excuses\nthinking I should have been been good at this by now is deafeatist\nyou can have a fixed mindset for ages about many things\nsitting down and working on a thing is an amazingly powerful way to learn that thing\ntakeaways:\n\neffort of learning isn’t comfy - if feeling comfortable learning its a warning sign\ndon’t feel bad about being bad at something\n\n\nmy takeaway: think you can do it, and learn by doing. don’t get comfy and don’t feel bad about sucking at something."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#saturday-lightening-talks",
    "href": "posts/writing-about-code/PyConAu2018.html#saturday-lightening-talks",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Digital Earth Australia\n\narchive of Australia’s space/earth/ocean data\nthey try and figure out how to display things to see changes over time, showed impressive animationg of pictures alongside a graph of flow changing over time\ncode is all open source: GeoscienceAustralia github\n\nFuPy\n\nweb\nhardware is hard, FPGA makes hardware into software, making it easier to fix bugs\nFPGA generally don’t support python, but along came Migen and Misoc and LiteX which support python by putting a cpu in the FPGA\nFuPy project is to run MicroPython on FPGA\n\nflip flop operators in Ruby\n\n@merxplat\nflips b/w two statements\nits from perl, which got it from sed/awk, who got it from electronics\nbeing removed from ruby, no one uses it\nlooks pretty handy and terse to me!\n\npython in the classroom\n\nmath teacher who now teaches python to kids in a low income area\ncoding keeps kids engaged\nthey use grok learning\n\nDRF Model Pusher\n\nsends realtime updates using Pusher over websockets\ndjango stuff\n\nWatching water from space\n\nClaire, climate scientist at Geoscience Australia\nworks with agricultural data from space\nwhen are farm storage dams being filled and emptied\n\ndams were manually labeleed on satellite photos before\nnow algorithim to detect dams, run automatically every time there is new imagery\nthey build a flagging system to alert for drainage\n\nworking to improve this and pass it to stake holders\n\nCaptcha Cracker\n\nyr 12 student\nwe have so much info to learn from, but programs have very limited info\nwriting a program that uses the SIFT Algorithm to get through a ReCaptcha\nsee their website\n\nRocketry\n\nMEMS is making things a lot cheaper\nused MEMS sensors with ESP32 to make lots of telemetry modules\nof course used micropython\nlook up MQTT\nthe feeling of something actually happening is awesome\n\ncyrpto money\n\nbitcoin uses tons of electricity\ntheft\nethereum contracts cause software\ngreat overview of cycrptocurrencies\n\nPeter Lovett\n\nteaches Python\npython is readable, everyone loves it for that\npep505\npython needs to grow, but we don’t want to loose sight of what we have\n\nGiving thanks\n\nproject to fund the packages you use: phildini/thanks"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#tracy-osborn-clueless",
    "href": "posts/writing-about-code/PyConAu2018.html#tracy-osborn-clueless",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Tracy Osborn is the author, designer, and self-publisher of three books and the solo founder of a venture-backed startup. Each of these achievements has something in common — being completely clueless about the work and problems involved in each. In this keynote, Tracy will tell stories about how she launched her projects and what she learned (after already being neck-deep.) #\n\n\n\ntwtr\nwrote easy/friendnly hello web app books\nunusual background to pycon. went through high school building websites.\nquit computer science in college cause bad experience, switched to art\nended up working as a designer and web developer - design/html/css - scared of javascript\nlearnt from cs experience how difficult it is to pick up programming and how important teaching styles are\nsee her djangocon 2017 keynote\nwanted to do a startup, didn’t want to code but couldn’t find a technical cofounder\nhusband works in python, introduced her to django. learnt django and launched her first webapp in 6 weeks\nyou don’t have to make things perfect, or understand how they work under the hood\nlessons learned:\n\nbarring security issues, ugly code can be ok\nlearn by doing - picked up programming much faster by having a real project to work on\nstartups are stressful but if you like to constantly be learning and doing something different most days they are great\n\nshe didn’t like the way django was taught, which is where her book came from\ncustomizable tutorials, project based - she teaches how to build a web app, not how to code\nwrote book instead of tutorial cause “go big or go home”. low royalites with publishers, so used kickstarter to raise 12k to self publish.\nlessons learned:\n\nkeep marketing in mind when building a product\ntook way longer then planned - 1yr late. build a big buffer\nkeep it simple- instead of markdown etc, used google docs - very useful for edit help, or sending drafts to ppl\nused indesign to lay out the book - used skillshare to pick it up.\n\nare paperback books worth it?\neasypost - api’s for shipping\n\nmy takeaway: pick a simple project and just do it."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#guide-to-your-own-artificial-intelligence-application-in-3-easy-steps",
    "href": "posts/writing-about-code/PyConAu2018.html#guide-to-your-own-artificial-intelligence-application-in-3-easy-steps",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "What do you think of when you hear “artificial intelligence”? Perhaps self-driving cars, autonomous robots and Siri, Alexa or Google Home? But it doesn’t have to be that complex. You can build a powerful image classification model within a topic that inspires and interests you - with 3 easy steps. #\n\n\n\nNorah Klintberg Sakal ln - data scientist doing a masters thesis at Chan Zuckerberg Biohub\ntalk github, includes code\nususally when u think of AI or DL u think of self driving cars, Alexa, cancer diagnosis\nbut we can apply things to day to day problems\nIdea - Data - Training\nwanted to look at makeup tutorials, too many on the internet, so how do you know which one of the 30M videos is relevant ($400B industry)\nIdea: use deep learning to answer this\nData: looked at videos, eye shapes jumped out - there are 4 distinct shapes\n\ncreated dataset by looking at celeb images and cropping out eye shapes - 200 training, 100 validation\n\nTrain: used transfer learning - pretrained weights from Imagenet (architecture VGG16)\n\nthere are a lot of models to choose from, used VGG-16 since its fairly simple to understand and easy to play around with\nlocked all the conv layers since they already knew how to understand images\nusing keras to grab VGG16 minus the top layer\ngot 93% accuracy\n\nApp: Flask to take photo and predict image\nan important part of learning is to make something end to end. take a simple problem and make a app for it\n\ntakeaway: this was suprisingly easy for something which sounds so complex. build my own mini app using keras and flask"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#hello-to-the-world-in-8-web-frameworks",
    "href": "posts/writing-about-code/PyConAu2018.html#hello-to-the-world-in-8-web-frameworks",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "We’ll start with the current crop of microframeworks, showing how to achieve the same task in each, before progressing to “Batteries included” and then the more specialised async frameworks. For developers who perhaps have only used a single framework or even none at all, this talk gives them an opportunity to get out and explore the world (of web frameworks) and broaden their horizons, with plenty of Jules Verne inspired fun along the way. #\n\n\n\nAaron Bassett twtr dev advocate at Nexmo - an api company for telephony\ntask: return a json string\nFlask - uses functional based views (does support class views too)\ncherrypy - uses class based views\nfalcon - api first framework, designed to do json based microservices and be really fast 5x faster than flask, 10x django\nhug - make development driven python apis as simple as possible - uses falcon under the hood, just adds a simplified interface to falcon.\nmicroframeworks make it very easy to do something simple, you also have the freedom to change things or bolt on whatever bits you want.\nbut with great power comes great responsibility - you start with great intentions and end up building a monolithic app\nin real world situations your fantastic ORM isn’t going to be usable by other ppl or well documented\nDjango in the real world, every other project is essentially structured the same. which leads us to django, which is batteries included.\n\nbut its somewhat complex - basic project has a bunch of stuff, and for a basic api you end up with multiple files to deal with\nprefers class based views, though some controversy there\n\nPyramid - start small - for basic stuff not much code, finish big - includes batteries if needed\nTornado - async framework, pretty small code\nSanic - flask like async, but can’t recommend it at the moment. prone to crashes and vulnerabilities\n\nmy takeaway: use any, ignore differences in syntax - whats important is documentation, bug/issue tracker activity, release management, community, and of course the scope of the project and the number of “batteries” you need. Don’t end up rebuilding a bastardized undocumented django on top of a microframework, just use django if thats where you’re headed."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#functional-programming-demystified",
    "href": "posts/writing-about-code/PyConAu2018.html#functional-programming-demystified",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Have you ever eavesdropped on FP developers talking about programming and wondered which planet you landed on? I attended LambdaJam 2018 and felt your pain! Let’s demystify Either, Semigroups, Monoids, Functors, Monads, Traversable, Natural transformations etc. by implementing them in Python. #\n\n\n\nEugene Van den Bulke - twtr\nwent to lambdajam\nEugenia Cheng - read her books\nProfessor Frisby introduces functional programming in javascript - we are going to cover it in python\nFunctor: something that can be mapped over.\nCurrying: this makes no sense. instead of a func which takes in multiple arguments you have some weird chain of funcs which take in arguments one at a time\nApplicative functors: put a func in a box and apply to objects in other boxes\nEither Left of Right: basically a flipfloperator\nMonoid - ok now things are getting into the deep end\n\nmy takeaway: functional programming is interesting but deeply unpythonic. don’t use unless there is a clear need to."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#you-dont-need-that",
    "href": "posts/writing-about-code/PyConAu2018.html#you-dont-need-that",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Not every design pattern makes sense in Python. This talk builds up design patterns commonly used in enterprise languages, and shows the features in Python that make these approaches unnecessary. #\n\n\n\nChristopher Neugebauer twtr web works at AlpaSigts, Director of PSF\nDeisign Patterns let you express ideas that are hard to express… in a familiar way\n\nmost patterns are object oriented ones from the mid 90’s, back when C and Java were the hot languages\n\nso how would design patterns be done pythonically?\nthe simplest design pattern is a Singleton\nSingleton: a class that can only be constructed once\n\ncan use del to delete class definition after having called it once to make sure it never gets called again\n\nbut why do we want a singleton? what do we want to achieve?\nback in the good old days Java needed singletons for namespaces, but python already has pretty good namespace seperation\npython can achieve this by using modules - so instead of classes we can have modules to hold data. Python makes sure you always get the same object regardless of how many times we import it.\nModules are objects, so we don’t need Singletons\nDependency infection: provide dependecies to classes as constructor arguments (useful for unit testing)\nMocks - pretend version of functions, most lanugages have mocking frameworks\nUnit Testing - unittest.mock has a patch function, so we don’t need to write our own mock funcs\nIterators - python provides for loops to consume iterators. But what are we trying to achieve? use yield to write generators for your own iterators\nperform a common operation on a collection - python does this natively with for item in Class\npython doesn’t need Visitors becuase generators\nthere are some design patterns which do make sense in python\nFactories - used all through the python library\nbook on useful patterns in python: python-patterns.guide\n\ntakeaway: a lot of design patterns exist becuase they were needed in some language or other. (java, shudder). Use python’s pythonic features, don’t reach for older patterns unless its truly a good idea."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#there-is-no-now-and-sensor-datas-the-worst",
    "href": "posts/writing-about-code/PyConAu2018.html#there-is-no-now-and-sensor-datas-the-worst",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "Audience members will be asked to go to a webpage on their phone that reads accelerometer data and transmits it to the presentation. This data will then be used to highlight the issues of collecting a processing data from distributed sensors - what happens when all the data is not received at once and not perfectly in time? what happens if there is an outage? How do you turn all this noise into something tha t can be managed? #\n\n\n\nMike Leonard twtr Reposit Power\npresentation demo fail - was hosted on kubernets cluster. Reminder that all this clustering business is hard.\ncollecting IOT sensor data is hard - too small a polling interval will ddos your server, but too long means loosing out on realtime data.\nduring an outage you want to keep data loss to a minimum - so log data on the device, but when they came back online you don’t want to get ddos’d. so each time a device fails to transmit, it doubles the delay, also introduce some randomness so each device is sending data at different times\nrealtime is tricky, scale is hard, distributed sensors are the worst\npython backend using Flask, Flask-SocketIO, React & Socket.IO + browser api\nrecommends using socket.io over websockets - python implementation\n\ntakeaway: think orchestration. how many devices, what when how do they log/send data and how to deal with loss/latency."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#watch-out-for-safety-bandits",
    "href": "posts/writing-about-code/PyConAu2018.html#watch-out-for-safety-bandits",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "The presentation itself will go into the details of example security vulnerabilities, explain why it’s important to fix them, and show how integrating these two tools into your process will better protect you and your software. Beginners will get an appreciation for the kinds of security problems that can occur, and an introduction to continuous integration workflows. #\n\n\n\nTennessee Leeuwenburg twtr Head of Secure Coding at the Australian Bureau of Meteorology.\nuse python tools called Safety and Bandit\ntools like CVE track security vulnerabilities - there are heaps all the time in commonly used packages\nSafety tells you all the secure packages you are using using their cli tool or hosted service\nsome security vulnerablities are just “whoops I forgot to…”\nBandit helps us with these problems - its a security linter which checks for common security issues in your code\n\nvery handy for going through code to find code smells\ncan put into code so it runs with CI and raises alerts\n\nthe tools are noisy so you have understand what to pay attention to and what to tune out\n\ntakeaway: integrate Safety Bandits into CI."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2018.html#sunday-lightening-talks",
    "href": "posts/writing-about-code/PyConAu2018.html#sunday-lightening-talks",
    "title": "PyCon Australia 2018",
    "section": "",
    "text": "chunks() the story of a generator\n\nbreak an iterable into chunks of n - lots of implementations from stackoverflow\nitertools.islice slices iterators but returns an emptry iterator if gets an empty iterator\ncode at gist.github.com/timheap\n\nPyCon Anthology\n\nwrite a short story, art, poetry, pg-13, deadline Sep 1\nsubmit through github\n\nTracking trucks in Africa\n\nLori Systems\ntracks trucks, but how to do it best? truckers in Africa don’t have reliable phones, handing out phones didn’t really work\nTraccar - java based IOT gps tracking system for trucks\nZappa is an alternative to celery\n\nSoftware release reports with Python Sphinx & Jira\n\nCochlear, heavily regulated, need a software release report: bugs fixed, improvements, known bugs\nused MS Word, so time to automate. Data in Jira, which has a python interface.\npython script gets info from Jira\n\nwhy text encoding\n\nwe need to encode text - so whats text encoding\n\nphy py physics\n\n@cormacKikert\nA 3D physics sandbox game built using pygame\ncool demo - [code on github]9https://github.com/cormackikkert/Py-PHY)\n\nPython Bugs\n\nfirst lightening talk\n\nBad code\n\nexamples of over engineered code\n\nGoto\n\nin yr10! doing a arduino assigment\nmissed goto, wrote his own in python\n\nFlip Flip Face Offerator\n\na flipfloperator faceoff\n\nMicropython.. Jupyter.. Live\n\nmicropython is re-inventing embedded development\nbut theres no debugging in micropython\ncan talk to micropython from jupyter, use interactive widgets\nlook up jupyter-micropyton-remote - very impressive\n\nNick - core CPython dev\n\ntalk about Peps which introduced now accepted features"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html",
    "href": "posts/writing-about-code/google-ml-best-practice.html",
    "title": "Googles Best Practices for ML Engineering",
    "section": "",
    "text": "Notes on Google’s best practices guide for ML engineering:\n\nThis document is intended to help those with a basic knowledge of machine learning get the benefit of Google’s best practices in machine learning."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-1-dont-do-ml-unless-there-is-enough-data-and-ml-is-actually-needed.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 1: don’t do ML unless there is enough data and ML is actually needed.",
    "text": "Rule 1: don’t do ML unless there is enough data and ML is actually needed.\n\nwill human hueristics do well enough?\nis there enough data?"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-2-first-write-down-what-you-want-the-ml-to-do-and-what-data-you-might-want.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 2: first write down what you want the ML to do and what data you might want.",
    "text": "Rule 2: first write down what you want the ML to do and what data you might want.\n\ndesign and implement metrics\nhave a way to run experiments"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-3-use-ml-over-complex-heristics.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-3-use-ml-over-complex-heristics.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 3: Use ML over complex heristics.",
    "text": "Rule 3: Use ML over complex heristics.\n\nOnce enough data and goals, move on to ML. ML models are easier to update and maintain then complex heuristics."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-4-start-with-simple-ml-and-concentrate-on-infrastructure",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-4-start-with-simple-ml-and-concentrate-on-infrastructure",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 4: Start with simple ML and concentrate on infrastructure:",
    "text": "Rule 4: Start with simple ML and concentrate on infrastructure:\n\nhow is data getting to the model\nwhat does it mean to be good or bad for us\nhow are results delivered to our app/server/user whatever\nuse simple features and models to start off with."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-5-test-data-flows-and-infrastructure-seperately-from-ml",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-5-test-data-flows-and-infrastructure-seperately-from-ml",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 5: Test data flows and infrastructure seperately from ML",
    "text": "Rule 5: Test data flows and infrastructure seperately from ML\n\nhave a way to check how data is getting to the algo\nmanually check inputs\nhave statistics for the inputs\nget models, see if they behave the same in testing and production\nML algorithims can be unpredictable, so have tests for code and use a fixed model in production\nUnderstand your data"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-6-dont-drop-data-if-copying-pipelines",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-6-dont-drop-data-if-copying-pipelines",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 6: don’t drop data if copying pipelines",
    "text": "Rule 6: don’t drop data if copying pipelines\nWhen setting up data pipelines, be carefuly about copying existing ones - every data pipeline is serving a specifc need."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-7-use-existing-knowledge",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-7-use-existing-knowledge",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 7: use existing knowledge",
    "text": "Rule 7: use existing knowledge\nMost ML is applied to problems with existing solutions. Use the older hueristics and rules and incorporate them into the ML algo by:\n\npreprocess using the hueristics\ncreate features from the heuristic - i.e if we have a function computing a score, make that score a feature\nlook at the inputs of a heuristic and feed them as features into the ML\nmodify the label, e.g multiply downloads by number of stars to make quality apps stand out more\n\nSee if there is a simpler way to incorporate old heuristics."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-8-freshness",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-8-freshness",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 8: freshness",
    "text": "Rule 8: freshness\n\nhow frequently does the model need to be updated? monitor and alert accordingly."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-9-sanity-check-models-before-using",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-9-sanity-check-models-before-using",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 9: sanity check models before using",
    "text": "Rule 9: sanity check models before using\n\nmake sure the model’s perf is reasonable on validation data - e.g check area under roc curve."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-10-watch-for-silent-failures",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-10-watch-for-silent-failures",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 10: watch for silent failures",
    "text": "Rule 10: watch for silent failures\nSupid things happen, like part of the incoming data is being dropped somewhere. the ML system will keep chugging along as they adjust to feature changes and will decay gradually.\n\ntracking statistics for data helps here, for example if a certain feature column has less populated data then before"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-11-document-features-and-assign-owners",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-11-document-features-and-assign-owners",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 11: document features and assign owners",
    "text": "Rule 11: document features and assign owners\n\ndocument all the features, duh\nfor larger systems, assign ownership and responsbilities"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-12-dont-overthink-which-objective-to-optimize",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-12-dont-overthink-which-objective-to-optimize",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 12: dont overthink which objective to optimize",
    "text": "Rule 12: dont overthink which objective to optimize\nDon’t optimize a single metric early, initially all metrics we care about should typically be going up. Consider the overall ‘health’ of the ml process first."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-13-use-a-simple-observable-and-attributtable-metric-as-a-first-objective",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 13: Use a simple, observable and attributtable metric as a first objective",
    "text": "Rule 13: Use a simple, observable and attributtable metric as a first objective\nThis should be straightward and serve as a proxy for the true objective, in fact, often there is no ‘true’ objective. Model direct things, leave indirect things for later or to a different layer altogether."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-14-start-with-an-interpretable-model-to-make-debugging-easier",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-14-start-with-an-interpretable-model-to-make-debugging-easier",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 14: Start with an interpretable model to make debugging easier",
    "text": "Rule 14: Start with an interpretable model to make debugging easier\nLinear regression, logistic regression etc are straightforward to understand and debug vs the more complicated models."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-15-seperate-spam-filtering-and-quality-ranking",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-15-seperate-spam-filtering-and-quality-ranking",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 15: Seperate spam filtering and quality ranking",
    "text": "Rule 15: Seperate spam filtering and quality ranking\nRank content seperately from how you determine what kind of content it is - for example have a different model for post quality score, and another one for categorzing spam.\ni.e don’t do something overly simple like assuming posts with a low score are spam."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-16-launch-and-iterate",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-16-launch-and-iterate",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 16: Launch and iterate",
    "text": "Rule 16: Launch and iterate\nAs you build the model, keep adding, removing and recombining features and launching new models. Don’t wait to do all the features in one go, iterate."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-17-start-with-directly-observed-and-reported-features-as-opposed-to-learned-features",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 17: Start with directly observed and reported features as opposed to learned features",
    "text": "Rule 17: Start with directly observed and reported features as opposed to learned features\nLearned features can be coming from elsewhere can have a lot of issues and should not be in the first model. These outside features come with their own objectives which could be only weakly correlated to our objective, and there is no gurantee they will lead to optimal solutions.\nWe can get excellent baseline performance without deep deatures."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-18-explore-with-features-of-content-that-generalizes-across-contexts",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-18-explore-with-features-of-content-that-generalizes-across-contexts",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 18: Explore with features of content that generalizes across contexts",
    "text": "Rule 18: Explore with features of content that generalizes across contexts"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-19-user-very-specific-features-when-you-can",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-19-user-very-specific-features-when-you-can",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 19: User very specific features when you can",
    "text": "Rule 19: User very specific features when you can\nwith big data, its easier to learn millions of simple features than a few complex features. Consider making groups of features if each feature only applies to a small fraction of data, but the group coverage is &gt; 90%."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-20-combine-and-modify-existing-features-to-create-new-features-in-human-understandable-ways",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 20: combine and modify existing features to create new features in human understandable ways",
    "text": "Rule 20: combine and modify existing features to create new features in human understandable ways\nThere are many ways to do this, for example turning an age feature into infant/child/teenager/adult/senior using age ranges. Don’t overthink - basic quantiles gives the most impact.\nAlso consider cross combining features like {male,female} with say a country feature like {US,Canada} to get a feature like (male, US). This takes a lot of data, but can be useful in some contexts, while overfitting in others."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-21-number-of-feature-weights-is-roughly-proportional-to-the-amount-of-data",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 21: number of feature weights is roughly proportional to the amount of data",
    "text": "Rule 21: number of feature weights is roughly proportional to the amount of data\nThere is a lot of stats theory about the appropriate level of complexity for a model, but this rule is the most important to know. Scale the learning to the size of your data.\nWith small numbers of training data, you probably have to human engineer features. With larger data sets, regularization and feature selection helps to cut down features."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-22-clean-up-features-no-longer-in-use",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-22-clean-up-features-no-longer-in-use",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 22: clean up features no longer in use",
    "text": "Rule 22: clean up features no longer in use\ndrop useless features. keep infrastructure clean so that promising features can be tried fast. Add features back if necesary.\nConsider the coverage of features, like how many examples are covered? If a feature only covers 8% of the data, it might be useless, but on the other hand a feature covering only 1% data could have a 90% prediction rate."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-23-youre-not-a-typical-end-user",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-23-youre-not-a-typical-end-user",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 23: you’re not a typical end user",
    "text": "Rule 23: you’re not a typical end user\nYou’re too close to the code and results, so its important to bring in outside users - from within the company, lay people or a crowdsourcing platform.\nDevelop user experience methodologies, and create user personas and do usablity testing."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-24-measure-the-dela-within-models",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-24-measure-the-dela-within-models",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 24: measure the dela within models",
    "text": "Rule 24: measure the dela within models\nmeasure both results b/w different models, and also results with the same model to make sure its stable."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-25-utilitarian-performance-trumps-predictive-power",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-25-utilitarian-performance-trumps-predictive-power",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 25: utilitarian performance trumps predictive power",
    "text": "Rule 25: utilitarian performance trumps predictive power\nthe key question is what we are doing with the prediction - for example when ranking something the quality of the final ranking matters more than the quality of the prediction itself. But in a spam filter which has a cutoff on what is blocked, the precision matters more."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-26-look-for-patterns-in-the-measured-errors-and-create-new-features",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 26: look for patterns in the measured errors and create new features",
    "text": "Rule 26: look for patterns in the measured errors and create new features\nSay for some reason the system is demoting longer posts, then a post lengh feature could be useful. Don’t overthink it - add relevant features and let the model figure it out."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-27-try-to-quantify-observed-behaviour",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-27-try-to-quantify-observed-behaviour",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 27: try to quantify observed behaviour",
    "text": "Rule 27: try to quantify observed behaviour\nif the model has ‘bad’ properties, like recommending gag apps too often, try to measure or get humans to add category labels and add as a feature.\n\nIf your issues are measurable, then you can start using them as features, objectives, or metrics. The general rule is “measure first, optimize second”."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-28-be-aware-that-identical-short-term-behaviour-does-not-imply-identical-long-term-behaviour",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 28: be aware that identical short-term behaviour does not imply identical long-term behaviour",
    "text": "Rule 28: be aware that identical short-term behaviour does not imply identical long-term behaviour\nA model for say predicting apps will work great in practice but in production turn out to not recommend any new apps, since in practice there wasn’t a way to learn that new apps should be shown too."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-29-best-way-to-make-sure-you-train-like-you-serve-is-to-save-the-set-of-features-used-at-serving-time-and-pipe-those-features-to-a-log-to-use-them-at-training-time",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 29: best way to make sure you train like you serve is to save the set of features used at serving time and pipe those features to a log to use them at training time",
    "text": "Rule 29: best way to make sure you train like you serve is to save the set of features used at serving time and pipe those features to a log to use them at training time\nIf not for all examples, log a fraction of the data."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-30-importance-weight-sampled-data-dont-arbitarily-drop-it",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 30: Importance-weight sampled data, don’t arbitarily drop it",
    "text": "Rule 30: Importance-weight sampled data, don’t arbitarily drop it\ndon’t arbitarily decide to take the first ten results, rather weight by importance - say if we want to sample X with a 30# probablity, give it a weight of 10/3."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-31-beware-if-you-join-data-from-a-table-at-training-and-serving-time-the-data-in-the-table-may-change",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 31: beware if you join data from a table at training and serving time, the data in the table may change",
    "text": "Rule 31: beware if you join data from a table at training and serving time, the data in the table may change\nb/w training and serving time, features in a table may be changed. Avoid this by logging features at training time."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-32-reuse-code-bw-training-and-serviing-pipelines-whenever-possible",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 32: reuse code b/w training and serviing pipelines whenever possible",
    "text": "Rule 32: reuse code b/w training and serviing pipelines whenever possible\nWhile how data is arriving is different, requiring different code, try to transform initial data into a human readable object which can be tested and transformed by a common method for use by the machine learning system. Try to use the same programming language for training and serving."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-33-if-you-make-a-model-on-data-till-jan-5th-test-it-on-data-after-jan-6th",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 33: if you make a model on data till Jan 5th, test it on data after Jan 6th",
    "text": "Rule 33: if you make a model on data till Jan 5th, test it on data after Jan 6th\nthis better reflects how the model will do in production."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-34-in-binary-classification-for-filtering-make-small-short-term-sacrifices-in-performance-for-very-clean-data",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 34: in binary classification for filtering make small short-term sacrifices in performance for very clean data",
    "text": "Rule 34: in binary classification for filtering make small short-term sacrifices in performance for very clean data\nSay our spam filter blocks 75%, and we might be tempted to learn from the 25% which gets through and the user labels spam. This introduces sampling bias, we will get cleaner data by labelling 1% of traffic as “held-out” and send all those to the users, and we can than use those labelled samples as training data."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-35-beware-of-inherent-skew-in-ranking-problems",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-35-beware-of-inherent-skew-in-ranking-problems",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 35: Beware of inherent skew in ranking problems",
    "text": "Rule 35: Beware of inherent skew in ranking problems\nWhen we change a ranking algorithim so that different results show up, this effectively changes the data the algorithim is going to see in the future. Design algos around this:\n\nhave higher regularization for features that cover more queries, over features that are only for one query. This allows the model to favour features that are specific to one or a few queries, and helps from preventing very popular results into leaking into irrelevant quries.\nonly allow features to have positive weights\ndon’t have document only features"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-36-aviod-feedback-loops-with-positional-features",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-36-aviod-feedback-loops-with-positional-features",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 36: Aviod feedback loops with positional features",
    "text": "Rule 36: Aviod feedback loops with positional features"
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-37-measure-trainingserving-skew",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-37-measure-trainingserving-skew",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 37: Measure Training/Serving skew",
    "text": "Rule 37: Measure Training/Serving skew\n\ndifference in performance on training and holdout data - will always exist, so doesn’t necessarily mean something bad.\ndifference in performace on holdout data and the nextday data - large drops in perf here could indicate some features are time-sensitive and possibly degrading model perf.\ndifference in performance bw nextday day and live data. differences here could indicate model error."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-38-dont-waste-time-on-new-features-if-unaligined-objectives-have-become-the-issue",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 38: don’t waste time on new features if unaligined objectives have become the issue",
    "text": "Rule 38: don’t waste time on new features if unaligined objectives have become the issue\nas measurements plateau, teams often look at issues outside the scope of the current machine learning system. If those goals aren’t covered by the exisiting ml system, change the system or the goals."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-39-launch-decisions-are-a-proxy-for-long-term-product-goals",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 39: launch decisions are a proxy for long-term product goals",
    "text": "Rule 39: launch decisions are a proxy for long-term product goals\nFor example, adding a feature might increase installs, but drop daily active use. Launch decisions depend on multiple factors, only some of which are optimizable by ML.\nMetrics are a proxy for more longterm goals."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-40-keep-ensembles-simple",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-40-keep-ensembles-simple",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 40: Keep ensembles simple",
    "text": "Rule 40: Keep ensembles simple\neach model should either be ensemble taking input of other models, or a base model taking in features, not both. Don’t pile on models on top of models - this can result in bad behaviour.\nMake sure that in crease in the predicted probality of a underlying classifier doesn’t decrease the predicted probability of the classifier."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-41-when-performance-plateaus-look-for-qualitatively-new-sources-of-information-to-add-rather-than-refining-existing-signals.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.",
    "text": "Rule 41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.\nadd more relevant signals, build out infrastructure for radically different features, use more deep learning, weigh benefits of new features against increased complexity."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-42-dont-expect-diversity-personalization-or-relevance-to-be-as-correlated-with-popularity-as-you-think-they-are.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.",
    "text": "Rule 42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.\npopularity is easy to measure, diversity, personalization and relavance is harder - adding features for these can turn out to not work very well, i.e they get less weight in the model.\nPostprocessing can help, or directly modifing the objective to increase diversity or relevance."
  },
  {
    "objectID": "posts/writing-about-code/google-ml-best-practice.html#rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be.",
    "href": "posts/writing-about-code/google-ml-best-practice.html#rule-43-your-friends-tend-to-be-the-same-across-different-products.-your-interests-tend-not-to-be.",
    "title": "Googles Best Practices for ML Engineering",
    "section": "Rule 43: Your friends tend to be the same across different products. Your interests tend not to be.",
    "text": "Rule 43: Your friends tend to be the same across different products. Your interests tend not to be.\nWhile one thing might be close to another, doesn’t mean it applies across products."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html",
    "href": "posts/writing-about-code/advent-of-code-2016.html",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "todo: convert this into a jupyter notebook and repost.\nNotes for the Advent of Code 2016 programming challange.\n\nAdvent of Code is a series of small programming puzzles for a variety of skill levels. They are self-contained and are just as appropriate for an expert who wants to stay sharp as they are for a beginner who is just learning to code. Each puzzle calls upon different skills and has two parts that build on a theme.\n\nI solved each puzzle in a jupyter notebook for each day saved in a github repo. The solutions are all a bit verbose as I’m trying to show all the steps taken to solve the puzzles.\n\n\n\nregex’s can make hard things easy at the cost of the regex itself being hard. luckily there are lots great regex sites.\ndispatch tables are great\nwrite little functions. Some of the problems seemed very hard, and I used the famous anti-procrastination advice of just start with the smallest thing you can do and lo and behold in the middle of writing the most basic two line function I would see how to write the next one, and the next, and soon enough the entire bigger problem was done\nlittle functions really help with being able to read the code and troubleshoot.\nmany problems are just graphs, which can be implemented many ways, from using classes to lists to dicts. but its still all graphs.\npython has lots of great tools in libraries like collections which should just be in the language. why do I have to import things like defaultdict which it is so useful that it should be there in the first place\nimplementing something myself makes it stick in my brain, vs googling a solution\nif/else in list comprehensions can be great: \"\".join([c if c != False else \"_\" for c in password]\nthe algo matters more than the machine speed. I moved my Day 11 solution to a super beefy high memory machine to brute force the solution - but it was taking the same runtime as on a small 1gb mem shared server. The fault was with the overly slow code, not the machine speed.\n\n\n\n\n\n\n\npath finding\nI found the simple things like turning directions tougher than the bigger problem. So doing it in pieces really helped.\n\n\n\nmove around a numpad\nThis problem had a diamond shaped keypad, so one way to move around the keypad was to have the shape of the keypad and move according, but it was much easier to use a dummy character to represent the off keypad points:\n.......\n...1...\n..234..\n.56789.\n..ABC..\n...D...\n.......\nKey takeaway: use a char to demarcate edges, and try to write more general code - in this case my part 1 could only deal with a square numpad, but it was just as easy to code it up to deal with a numpad of any shape.\n\n\n\nsimple math and slicing a grid. I used numpy for the win, basically use numpy if there is anything like a grid to deal with.\nThis involved using numpy slicing of a grid, which is always a bit tricky:\ndef transpose(tri):\n    \"\"\"generates all the column wise trianges in a list of triangles\"\"\"\n    for i in range(len(t[0])):          # the columns, could just use 3 here\n        for j in range(0, len(t)-2, 3): # now going down the entire length of the array\n            yield t[j:j+3,i]\n\nsum([is_tri(i) for i in transpose(t)])\n\n\n\nNothing interesting here,just following directions, a few notes:\n\nthere are lots of little tricks in these puzzles, like sorting the same list in two different orders\nnamedtuples are great, much easier to read than a list\nis there a better way to build a string? Right now I append chars to a list then join them into a string.\n\n\n\n\nfind a password. I used hashlib, was an interesting problem since I used hashlib for the first time.\nI liked this bit of code:\npassword = [False for _ in range(8)]\n\nwhile False in password:\n  # fills in one letter of the password\n\n\n\nDay 7 | move around a numpad | used both numpy and lists. Key takeaway: use a char to demarcate edges Day 8 | simple math and slicing a grid | numpy for the win, basically use numpy if there is anything like a grid to deal with. Day 9 | ? | use Counter and namedtuples Day 10 | parsing instructions | was a challange to fully comprehend the problem, though easy to code. Day 11 | building the right kind of graph with breadth first search | solved part 1 using a dumb BFS, but part two the search space is so big that I need to optimize. #TODO Day 12 | parse instructions to update registers | sort of like building a vm? Day 13 | build a map and find a path | used bfs, easy enough, but need to implement a generic path solver. its got pics too! Day 14 | find a password | used hashlib, was an interesting problem Day 15 | solve a system | find positions of a moving system at time t. Could have used math instead. Day 16 | find a password | used hashlib, was an interesting problem Day 17 | Get shortest, than longest path | looked similar to day 13, but different.\nNote: link jupyter notebooks and highlight the interesting part of each days challange."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#general-takeaways-from-2016s-advent-of-code",
    "href": "posts/writing-about-code/advent-of-code-2016.html#general-takeaways-from-2016s-advent-of-code",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "regex’s can make hard things easy at the cost of the regex itself being hard. luckily there are lots great regex sites.\ndispatch tables are great\nwrite little functions. Some of the problems seemed very hard, and I used the famous anti-procrastination advice of just start with the smallest thing you can do and lo and behold in the middle of writing the most basic two line function I would see how to write the next one, and the next, and soon enough the entire bigger problem was done\nlittle functions really help with being able to read the code and troubleshoot.\nmany problems are just graphs, which can be implemented many ways, from using classes to lists to dicts. but its still all graphs.\npython has lots of great tools in libraries like collections which should just be in the language. why do I have to import things like defaultdict which it is so useful that it should be there in the first place\nimplementing something myself makes it stick in my brain, vs googling a solution\nif/else in list comprehensions can be great: \"\".join([c if c != False else \"_\" for c in password]\nthe algo matters more than the machine speed. I moved my Day 11 solution to a super beefy high memory machine to brute force the solution - but it was taking the same runtime as on a small 1gb mem shared server. The fault was with the overly slow code, not the machine speed."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-1-no-time-for-a-taxicab",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-1-no-time-for-a-taxicab",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "path finding\nI found the simple things like turning directions tougher than the bigger problem. So doing it in pieces really helped."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-2-bathroom-security",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-2-bathroom-security",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "move around a numpad\nThis problem had a diamond shaped keypad, so one way to move around the keypad was to have the shape of the keypad and move according, but it was much easier to use a dummy character to represent the off keypad points:\n.......\n...1...\n..234..\n.56789.\n..ABC..\n...D...\n.......\nKey takeaway: use a char to demarcate edges, and try to write more general code - in this case my part 1 could only deal with a square numpad, but it was just as easy to code it up to deal with a numpad of any shape."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-3-squares-with-three-sides",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-3-squares-with-three-sides",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "simple math and slicing a grid. I used numpy for the win, basically use numpy if there is anything like a grid to deal with.\nThis involved using numpy slicing of a grid, which is always a bit tricky:\ndef transpose(tri):\n    \"\"\"generates all the column wise trianges in a list of triangles\"\"\"\n    for i in range(len(t[0])):          # the columns, could just use 3 here\n        for j in range(0, len(t)-2, 3): # now going down the entire length of the array\n            yield t[j:j+3,i]\n\nsum([is_tri(i) for i in transpose(t)])"
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-4-security-through-obscurity",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-4-security-through-obscurity",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "Nothing interesting here,just following directions, a few notes:\n\nthere are lots of little tricks in these puzzles, like sorting the same list in two different orders\nnamedtuples are great, much easier to read than a list\nis there a better way to build a string? Right now I append chars to a list then join them into a string."
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-5-how-about-a-nice-game-of-chess",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-5-how-about-a-nice-game-of-chess",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "find a password. I used hashlib, was an interesting problem since I used hashlib for the first time.\nI liked this bit of code:\npassword = [False for _ in range(8)]\n\nwhile False in password:\n  # fills in one letter of the password"
  },
  {
    "objectID": "posts/writing-about-code/advent-of-code-2016.html#day-6-signals-and-noise",
    "href": "posts/writing-about-code/advent-of-code-2016.html#day-6-signals-and-noise",
    "title": "Advent of Code 2016",
    "section": "",
    "text": "Day 7 | move around a numpad | used both numpy and lists. Key takeaway: use a char to demarcate edges Day 8 | simple math and slicing a grid | numpy for the win, basically use numpy if there is anything like a grid to deal with. Day 9 | ? | use Counter and namedtuples Day 10 | parsing instructions | was a challange to fully comprehend the problem, though easy to code. Day 11 | building the right kind of graph with breadth first search | solved part 1 using a dumb BFS, but part two the search space is so big that I need to optimize. #TODO Day 12 | parse instructions to update registers | sort of like building a vm? Day 13 | build a map and find a path | used bfs, easy enough, but need to implement a generic path solver. its got pics too! Day 14 | find a password | used hashlib, was an interesting problem Day 15 | solve a system | find positions of a moving system at time t. Could have used math instead. Day 16 | find a password | used hashlib, was an interesting problem Day 17 | Get shortest, than longest path | looked similar to day 13, but different.\nNote: link jupyter notebooks and highlight the interesting part of each days challange."
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "",
    "text": "This hackathon is about developing tech solutions to the challanges refugees face.\n\nHackathon Details, twtr, #Hack4Refugees"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#hackathon-themes",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#hackathon-themes",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "hackathon themes",
    "text": "hackathon themes\nDour - works wth SSI. Works with communities to empower ppl to change their lives - 800+ staff. Spoke about themes for the hackathon:\n\nhow to support ppl past the 5yr refegee suport eligibilit period?\n\nprograms like language, documents, jobs, social networks, career dev, etc etc\n\nhow to address the culture gap?\nAccess to housing\nissues with justice and crime - lack of support for ppl who come out of juvie or jail\ndisability - increasing disability in newly arrived refugees\nlanguage\nmental health\ndomestic and family violence\ntranstion from education to jobs/life - outcomes aren’t great"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#refugee-stories",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#refugee-stories",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "Refugee stories",
    "text": "Refugee stories\nMentors for teams to understand the usefullness of what they’re building:\nEnya:\n\nstarted misssahara.com - beauty pageant for African women\ngrew up in Blacktown, met lots of barriers. studied at UNSW, Masters in Intl Dev\na lot of young ppl can’t get involved in programs and services as they have been here for more than 5 years\n\nLuqman\n\nfrom Malaysia, was in a kids refugee for 7 years, came to Australia at age 11 in 2013\nwent to school for the first time (no schooling in kids refugee) in Australia, tough, hung out with wrong ppl\nnow at Homebush Bay, hacker, age 15\n\nengineer lady\n\nstudying to be a civil engineer, wrote a poetry book\nnot eligible for HECS - why does she need to be certified as a refugee when she is already here as a refugee?\nplatform to find and match scholarships?\nrefugees need equality\n\nDour\n\nfrom south Sudan\ngot scholarship from an org in Kings Cross, changed his life\nsupport in the local settlement areas is very important, defines the kind of things they end up doing\n\nSalima and ? (from Afghanistan)\n\nNurse, youth worker, help refugees to settle in\nbiggest challanges: english, isolated becuase of the that, helped to meet and befriend others from Afghanistan\nDidn’t know about services avaialble to her - found out about SSI 4 yrs after arriving in Australia\nWhat supports do they wish existed?"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#plan",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#plan",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "plan",
    "text": "plan\n[x] get prototype up and running [ ] setup python/flask powered API endpoint which takes in data and returns answers, forms etc [ ] TALK TO THE RIGHT PPPL then [ ] make a top ten list of useful qs the bot can answer and put info in airtable/gsheets - scholarships, schools, nearby services - list of useful free apps for migrants and what they can do"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#tools",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#tools",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "Tools",
    "text": "Tools\n\nhttps://manychat.com to do all the chat stuff\nhttps://glitch.com to run the backend to take in input, parse it and return a answer\nGoogle Sheets - API\nhttps://airtable.com - database backend\nconsider using rasa stack for the chatbot - its an opensource AI bot framework which makes it easy to customize & plugin other services like translation, answers, wikipedia, everything\nflask-restplus - build apis using flask"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#look-at",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#look-at",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "look at",
    "text": "look at\n\nhttps://refugeetalent.com/ - conencting refugees to jobs\nonestepapp - connects ppl for walks\ndonotpay bots"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#the-belong-program",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#the-belong-program",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "the belong program",
    "text": "the belong program\n\nsupplement existing migrant programs\nprovide confidence to refugees\nconnects refugees to others, volunteers\njudges loved it, special mention"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#help-migrant-children-integrate-in-schools",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#help-migrant-children-integrate-in-schools",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "help migrant children integrate in schools",
    "text": "help migrant children integrate in schools\n\nrefugee children drop out of school at 5x\nbuild profiles for kids so teachers can understand them better and how to help in class\nprovide customized lesson plans for refugees\ncultural context, sourced from multiple ngo’s\noptional: migrant’s personal story"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#firstpath",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#firstpath",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "firstpath",
    "text": "firstpath\n\nschool is tough for new migrants - simple things like selecting subjects is impossible\ndon’t know where to get help, end up choosing the wrong subjects for them, lots of regret\nsolution: provide user friendly content, scrape in real time from sources\nmake content for younger kids, not enough help content\nfirst prize winners"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#amplifyunify-techfugees-website-outreach",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#amplifyunify-techfugees-website-outreach",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "Amplify/Unify techfugees website & outreach",
    "text": "Amplify/Unify techfugees website & outreach\n\nfixing/updating techfugees website and social presence\nunify all their challanges\nmuch needed work which needed to happen"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#recruit-refugees-from-where-they-are",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#recruit-refugees-from-where-they-are",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "recruit refugees from where they are",
    "text": "recruit refugees from where they are\n\nworks with partner companies to find refugees\nrecruit refugees remotely\nsix visas lodged, couple in the pipeline"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#sheila-caceres",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#sheila-caceres",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "sheila caceres",
    "text": "sheila caceres\n\nhttp://sheilacaceres.com\nteach language immersively by using AR glasses to project names on objects\nobjects are labelled in your language, you can hear too"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#young-h4ck3rs",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#young-h4ck3rs",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "young h4ck3rs",
    "text": "young h4ck3rs\n\npair ppl with relevant others using a chatbot\nhelp refugees connect, build community\n#findmyself\nthird place"
  },
  {
    "objectID": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#elavate",
    "href": "posts/writing-about-code/techfugees-sydney-hackathon-2018.html#elavate",
    "title": "Techfugees Sydney Hackathon 2018",
    "section": "elavate",
    "text": "elavate\n\nmatch migrants to internships, opportunities\ndatabase of opportnities, tailors to ppl, translates to their language\nalready got one refugee an internship in the field she wanted - IT\nuses a website which asks questions to narrow down the universe of internships/jobs, and presents a list along with adivce videos tailored to help apply for that kind of job\nsecond place"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html",
    "href": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html",
    "title": "deeplearning.ai: Improving Deep Neural Networks",
    "section": "",
    "text": "Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization\nCourse Resources\n\nDiscussion forum\nYouTube playlist for Course 1\ncourse reviews: 1\n\n\n\nTrain / Dev / Test sets\n\ntraining is a highly iterative process as you test ideas and parameters\neven highly skilled practicioners find it impossible to predict the best parameters for a problem, so going through\nsplit data into train/dev/test sets\n\ntraining set - the actual dataset used to train the model\ncross validation or dev set - used to evaluate the model\ntest set - only used once the model is finally trained to get a unbiased estimate of the models performance\n\ndepending on the dataset size, use different splits, eg:\n\n100 to 1M ==&gt; 60/20/20\n1M to INF ==&gt; 98/1/1 or 99.5/0.25/0.25\n\nyou can mismatch train/test distribution, so make sure they come from the same distribution\n\nBias / Variance\n\n\nhigh bias - model is under fitting, or not even fitting the training set\nhigh variance - model is over fitting the training and dev set\nyou can have both high bias AND high variance\nbalance the bias and variance, and also consider the underlying problems tractibility, as are we aiming for 99% accuracy or 70%? Compare with other models and human performance to get a sense of a baseline error\n\nBasic recipe for machine learning\n\nfor high bias errors: bigger NN,more layers, different model, try different activations\nhigh variance: more data, regularize appropirately, try a different model\ntraining a bigger NN almost never hurts\n\nRegularization\n\n\nvectors often have many dimensions, so model sizes get BIG. regulariation helps to select features and reduce dimensions, as well as reduce overfitting\nL1 regularization (or Lasso) adds a penalty equal to the sum of absoulte coefficients\nL2 regularization (or Ridge) adds a penalty equal to the sum of squared coefficients\n\n\nL2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n\n\nas a rule of thumb, L2 almost always works better than L1 .\n\nDropout regularization\n\nfor each iteration, use a probablity to determine whether to “drop” a neuron. So each iteration through the NN drops a different, random set of neurons.\neach layer in the NN can have a different dropout probability, the downside is that we have more hyperparameters to tweak\nin computer vision, we almost always use dropout becuase we are almost always overfitting in vision problems\na downside of dropout is that the cost function is no longer well defined, so turn off dropout to see that the loss is dropping, which implies that our model is working, then turn on dropout for better training\nremove dropout at test time\ndropout intuitions:\n\ncan’t rely on on any given neuron, so have to spread out weights\ncan work similar to L2 regularization\nhelps prevent overfitting\n\n\nData augmentation\n\ninexpensive way to get more data - e.g with images flip, distory, crop, rotate etc to get more images\nthis helps as a regularization technique\n\n\nEarly stopping\n\nstop training the NN when the dev set and training set error start diverging - get the huperparameters from the lowest dev and training set cost.\nthis prevents the NN from overfitting on the training set but stops gradient descent early\nAndrew NG generally prefers using L2 regularization instead of early stopping as that\n\nDeep NN suffer from vanishing and exploding gradients\n\nthis happens when gradients become very small or big\nin a deep NN if activations are linear, than the activations and derivates will increase exponentially with layers\n\nWeight Initialization for Deep Networks\n\nthis is a paritial solution to vanishing/exploding gradients\ninitialize the weights with a variance equal to 1/n - where n is the number of input features - sure weights are not too small or not to large\n\nNumerical approximation of gradients\nGradient Checking\n\ncheck our gradient computation functions - this helps debug backprop\n\n\n\n\n\nMini-batch gradient descent\n\ntraining big data is slow - breaking it up into smaller batches speeds things up\nvectorization allows us to put our entire data set of m examples into a huge matrix and process it all in one go\nbut this way we have to process the entire set before our gradient descent can make a small step in the right direction\ntraining on mini batches allows gradient descent to work much faster, as in one iteration over the dataset we would have taken m / batch_size gradient descent steps\n\nmakes the leaning more ‘noisy’ since each mini-batch is new data (compared to going over the entire dataset)\n\na typical mini-batch size is 64, 128, 256, 512\n\nshould be a power of 2 as thats how computer memory is setup\nmake sure the batch fits inside cpu/gpu memory\n\n\n\nExponentially weighted averages\n\nalso called exponentially weighted moving averages in statistics\nthis decreases the weight of older data exponentially, enhancing the effect of more recent numbers which makes it easy to spot new trends. Of course the parameters can be tweaked, like changing beta depending on how many data points to average(1 / (1 - beta)).\nkey component of several optimization algos\n\nV(t) = beta * v(t-1) + (1-beta) * theta(t)\nbias correction in exponentially weighted averages:\n\nthe moving avg in the beginning is low since there isn’t past data to refer from and it starts from zero.\nso add a bias term, dividing the above equation by (1 - beta^t):\n\nv(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)\nGradient descent with momentum\n\nmodify gradient descent so on each iteration compute the exponential weighted averages of the gradients and update weights\nthis takes us faster to the min point, and dampens out oscillations\nmost common value of beta is 0.9. generally we don’t bother with bias correction since we do so many iterations\nthis explains why momentum works:\n\n\nWith Stochastic Gradient Descent we don’t compute the exact derivate of our loss function. Instead, we’re estimating it on a small batch. Which means we’re not always going in the optimal direction, because our derivatives are ‘noisy’. Just like in my graphs above. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations.\n\n\nalso see Nesterov Momentum\n\nRMSprop or Root mean square prop\n\nwe want learning to go fast horizontally and slower vertically - so we divide updates in the vertical direction by a large number and updates in the horizontal direction by a much smaller number\nthis dampens oscillations, so we can use a faster learning rate\nfirst proposed in Hinton’s coursera course\n\nAdam optimization algorithim\n\nmashes together momemtum and RMSprop, works very well\nparameters:\n\nlearning rate alpha\nbeta1: moving avg or momemtum parameter, same as 0.9 above\nbeta2: RMSprop, 0.999 works well\nepsilon 10^8 - less important, can leave it at default\ngenerally leave values at default, just try out different learning rates\n\n\nLearning rate decay\n\nslowly decrease learning rate over epochs\nthis makes intuitive sense as in the beginning bigger steps are ok and as the NN starts converging, we need smaller steps\nother methods: exponential decay, discrete steps, etc\nmanual decay - watch the model as it trains, pause and manually change the learning rate - works if running a small number of models which take a long time to train\nthis is lower down on the list of things to try when tuning\n\nThe problem of local optima\n\npeople used to worry a lot about NN getting in local optima, but in multi-dimensional space most points of zero gradient are saddle points so its very unlikely to get stuck in a local optima\na lot of our intuitions about low dimensional spaces don’t transfer over the high dimensional space practically all NN’s use - i.e if we have 20K parameters, the NN is operating in a 20K dimensional space\nbut plateaus can slow down learning, so techniques like momentum, Adam help here\n\n\n\n\n\n\nThe Tuning Process\n\nthere are tons of hyperparameters to tune, but some are more important than others\nalpha or the learning rate is the most important parameter most the time\nthen the second most important:\n\nmomentum (0.9 being a good default,\nmini-batch size\nhidden units\n\nthird in important:\n\nnum of layers -learning rate decay\n\nwhen using Adam, you pretty much never have to tune beta1, beta2 and epsilon\nof course, this depends on the NN, dataset etc\ndon’t use a grid of one val vs the other - this works when you have a small number of hyperparameters, but in Deep Learning choose hyperparameter combinations at random\nuse coarse to fine sampling - find the range where a parameter is working, then do finer grained sampling to get the best val\n\nUsing an appropriate scale to pick hyperparameters\n\npick the appropriate scale for hyperparameters, generally better to use log scale rather than linear\n\nHyperparameters tuning in practice: Pandas vs. Caviar\n\nintuitions about hyperparameters often don’t transfer to other domains e.g logistcs, nlpo, vision, speech will all have different best parameters\ntwo major ways to find the best parameters:\n\nbabysit one model - as its training, tweak the parameters and see how its doing on metrics. This is helpful when we don’t have enough computation capcity (panda approach)\ntrain many models in parallel - run many models in parallel, with different parameters (caviar approach)\n\n\n\n\n\nNormalizing activations in a network\n\ninstead of just normalizing the inputs to a NN, we also normalize the outputs of each layer of a NN - this is called batch norm.\nusing the standard (intermediate val - mean) / (std dev + epsilon). Epsilon is needed for numerical stability if variance is zero\nbatch norm sets the hidden layer to have mean 0 and variance 1, but sometimes we might want it to have a different distribution, so we can add a parameter beta to the hidden units to change the shape of the distribution. (like we might want a larger variance to take advantage of the nonlinearity of the sigmoid function).\nbatch norm makes the NN more robust and speeds up learning\nwhile batch norm can be applied before or after the activation function, in practice it is generally applied before the activation funciton.\n\nFitting Batch Normalization into a neural network\n\nDeep learning frameworks have batch norm built in, like tensorflow, keras etc.\nimplementing this ourself is straightforward, as we add in a norm step just before applying the activation function at each layer in the NN.\nIn each mini-batch, for each hidden layer compute the mean and the variance in that mini-batch, normalize, then apply the activation function as before.\n\nWhy does Batch normalization work?\nThree main reaons:\n\n1: normalizing input features speeds up learning, so one intuition is that this is doing a similar thing for each hidden layer\n2: makes weights in deeper layers more robust to changes in weights in earlier layers\n\nfor example, we train a network on black cats, and we try to classify a coloured cat. Every input is shifted, but the decision boundaries haven’t changed. This has a fancy name, coviariate shift.\nallows each layer to learn more independently\n\n3: regularization of hidden units\n\nadds some noise to each mini-batch, as each one is regularized on its own mean/variance, which has a slight regularization effect (bigger batches reduce noise/regularization)\nbut don’t rely on batch norm for regularization, as its just a unintended side effect, use other techniques like L2 or dropout\n\n\nBatch normalization at test time\n\nwe often predict one example at a time at test time, so the idea of a mean/variance doesn’t apply - we no longer have a mini-batch to process\nwe calculate the exponentially weighted average across all the mini-batches and use that to get a mean/variance to apply at test time on the one test example\n\n\n\n\nSoftmax regresssion\n\nis generalization of logistic regression which can predict multiple classes rather than just a binary.\n\nTraining a Softmax classifier\n\na hard max would look at a vector and put 1 for the max and zero for everything else\nsoft max takes in a vector and puts in a probability for each value such that they all sum up to 1\nloss function is trying to make the probablity of the “right” class as high as possible\n\n\n\n\nDeep learning frameworks\n\nimplementing a basic NN libary is a great learning framework\nas we implemnt complex/large models its not practical implement everything from scratch - there are many good frameworks to choose from\nthe DL frameworks not only speed up coding but implement many optimizations\nchoose one by looking at programning ease, running speed and how open it is\nmy own research has led my to tensorflow/keras or pytorch\n\nTensorflow\n\ncovers the very basics of tensorflow, though the tensorflow guide is better.\nwe implement forward prop, tf automatically does the backprop\na tf program looks like:\n\nmake tensors\nwrite operations to those tensors\ninitialize tensors\ncreate and run a Session\n\ntf.placeholder is a variable to which we assign a value later\n\n\n\n\n\nheads China’s National Deep Learning Research Lab\nbuilding a really large deep learning platform"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-1-practical-aspects-of-deep-learning",
    "href": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-1-practical-aspects-of-deep-learning",
    "title": "deeplearning.ai: Improving Deep Neural Networks",
    "section": "",
    "text": "Train / Dev / Test sets\n\ntraining is a highly iterative process as you test ideas and parameters\neven highly skilled practicioners find it impossible to predict the best parameters for a problem, so going through\nsplit data into train/dev/test sets\n\ntraining set - the actual dataset used to train the model\ncross validation or dev set - used to evaluate the model\ntest set - only used once the model is finally trained to get a unbiased estimate of the models performance\n\ndepending on the dataset size, use different splits, eg:\n\n100 to 1M ==&gt; 60/20/20\n1M to INF ==&gt; 98/1/1 or 99.5/0.25/0.25\n\nyou can mismatch train/test distribution, so make sure they come from the same distribution\n\nBias / Variance\n\n\nhigh bias - model is under fitting, or not even fitting the training set\nhigh variance - model is over fitting the training and dev set\nyou can have both high bias AND high variance\nbalance the bias and variance, and also consider the underlying problems tractibility, as are we aiming for 99% accuracy or 70%? Compare with other models and human performance to get a sense of a baseline error\n\nBasic recipe for machine learning\n\nfor high bias errors: bigger NN,more layers, different model, try different activations\nhigh variance: more data, regularize appropirately, try a different model\ntraining a bigger NN almost never hurts\n\nRegularization\n\n\nvectors often have many dimensions, so model sizes get BIG. regulariation helps to select features and reduce dimensions, as well as reduce overfitting\nL1 regularization (or Lasso) adds a penalty equal to the sum of absoulte coefficients\nL2 regularization (or Ridge) adds a penalty equal to the sum of squared coefficients\n\n\nL2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.\n\n\nas a rule of thumb, L2 almost always works better than L1 .\n\nDropout regularization\n\nfor each iteration, use a probablity to determine whether to “drop” a neuron. So each iteration through the NN drops a different, random set of neurons.\neach layer in the NN can have a different dropout probability, the downside is that we have more hyperparameters to tweak\nin computer vision, we almost always use dropout becuase we are almost always overfitting in vision problems\na downside of dropout is that the cost function is no longer well defined, so turn off dropout to see that the loss is dropping, which implies that our model is working, then turn on dropout for better training\nremove dropout at test time\ndropout intuitions:\n\ncan’t rely on on any given neuron, so have to spread out weights\ncan work similar to L2 regularization\nhelps prevent overfitting\n\n\nData augmentation\n\ninexpensive way to get more data - e.g with images flip, distory, crop, rotate etc to get more images\nthis helps as a regularization technique\n\n\nEarly stopping\n\nstop training the NN when the dev set and training set error start diverging - get the huperparameters from the lowest dev and training set cost.\nthis prevents the NN from overfitting on the training set but stops gradient descent early\nAndrew NG generally prefers using L2 regularization instead of early stopping as that\n\nDeep NN suffer from vanishing and exploding gradients\n\nthis happens when gradients become very small or big\nin a deep NN if activations are linear, than the activations and derivates will increase exponentially with layers\n\nWeight Initialization for Deep Networks\n\nthis is a paritial solution to vanishing/exploding gradients\ninitialize the weights with a variance equal to 1/n - where n is the number of input features - sure weights are not too small or not to large\n\nNumerical approximation of gradients\nGradient Checking\n\ncheck our gradient computation functions - this helps debug backprop"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-2optimizatoni-algorithims",
    "href": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-2optimizatoni-algorithims",
    "title": "deeplearning.ai: Improving Deep Neural Networks",
    "section": "",
    "text": "Mini-batch gradient descent\n\ntraining big data is slow - breaking it up into smaller batches speeds things up\nvectorization allows us to put our entire data set of m examples into a huge matrix and process it all in one go\nbut this way we have to process the entire set before our gradient descent can make a small step in the right direction\ntraining on mini batches allows gradient descent to work much faster, as in one iteration over the dataset we would have taken m / batch_size gradient descent steps\n\nmakes the leaning more ‘noisy’ since each mini-batch is new data (compared to going over the entire dataset)\n\na typical mini-batch size is 64, 128, 256, 512\n\nshould be a power of 2 as thats how computer memory is setup\nmake sure the batch fits inside cpu/gpu memory\n\n\n\nExponentially weighted averages\n\nalso called exponentially weighted moving averages in statistics\nthis decreases the weight of older data exponentially, enhancing the effect of more recent numbers which makes it easy to spot new trends. Of course the parameters can be tweaked, like changing beta depending on how many data points to average(1 / (1 - beta)).\nkey component of several optimization algos\n\nV(t) = beta * v(t-1) + (1-beta) * theta(t)\nbias correction in exponentially weighted averages:\n\nthe moving avg in the beginning is low since there isn’t past data to refer from and it starts from zero.\nso add a bias term, dividing the above equation by (1 - beta^t):\n\nv(t) = (beta * v(t-1) + (1-beta) * theta(t)) / (1 - beta^t)\nGradient descent with momentum\n\nmodify gradient descent so on each iteration compute the exponential weighted averages of the gradients and update weights\nthis takes us faster to the min point, and dampens out oscillations\nmost common value of beta is 0.9. generally we don’t bother with bias correction since we do so many iterations\nthis explains why momentum works:\n\n\nWith Stochastic Gradient Descent we don’t compute the exact derivate of our loss function. Instead, we’re estimating it on a small batch. Which means we’re not always going in the optimal direction, because our derivatives are ‘noisy’. Just like in my graphs above. So, exponentially weighed averages can provide us a better estimate which is closer to the actual derivate than our noisy calculations.\n\n\nalso see Nesterov Momentum\n\nRMSprop or Root mean square prop\n\nwe want learning to go fast horizontally and slower vertically - so we divide updates in the vertical direction by a large number and updates in the horizontal direction by a much smaller number\nthis dampens oscillations, so we can use a faster learning rate\nfirst proposed in Hinton’s coursera course\n\nAdam optimization algorithim\n\nmashes together momemtum and RMSprop, works very well\nparameters:\n\nlearning rate alpha\nbeta1: moving avg or momemtum parameter, same as 0.9 above\nbeta2: RMSprop, 0.999 works well\nepsilon 10^8 - less important, can leave it at default\ngenerally leave values at default, just try out different learning rates\n\n\nLearning rate decay\n\nslowly decrease learning rate over epochs\nthis makes intuitive sense as in the beginning bigger steps are ok and as the NN starts converging, we need smaller steps\nother methods: exponential decay, discrete steps, etc\nmanual decay - watch the model as it trains, pause and manually change the learning rate - works if running a small number of models which take a long time to train\nthis is lower down on the list of things to try when tuning\n\nThe problem of local optima\n\npeople used to worry a lot about NN getting in local optima, but in multi-dimensional space most points of zero gradient are saddle points so its very unlikely to get stuck in a local optima\na lot of our intuitions about low dimensional spaces don’t transfer over the high dimensional space practically all NN’s use - i.e if we have 20K parameters, the NN is operating in a 20K dimensional space\nbut plateaus can slow down learning, so techniques like momentum, Adam help here"
  },
  {
    "objectID": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks",
    "href": "posts/writing-about-code/deeplearning-ai-part-2-of-5.html#week-3-hyperparameter-tuning-batch-normalization-and-programming-frameworks",
    "title": "deeplearning.ai: Improving Deep Neural Networks",
    "section": "",
    "text": "The Tuning Process\n\nthere are tons of hyperparameters to tune, but some are more important than others\nalpha or the learning rate is the most important parameter most the time\nthen the second most important:\n\nmomentum (0.9 being a good default,\nmini-batch size\nhidden units\n\nthird in important:\n\nnum of layers -learning rate decay\n\nwhen using Adam, you pretty much never have to tune beta1, beta2 and epsilon\nof course, this depends on the NN, dataset etc\ndon’t use a grid of one val vs the other - this works when you have a small number of hyperparameters, but in Deep Learning choose hyperparameter combinations at random\nuse coarse to fine sampling - find the range where a parameter is working, then do finer grained sampling to get the best val\n\nUsing an appropriate scale to pick hyperparameters\n\npick the appropriate scale for hyperparameters, generally better to use log scale rather than linear\n\nHyperparameters tuning in practice: Pandas vs. Caviar\n\nintuitions about hyperparameters often don’t transfer to other domains e.g logistcs, nlpo, vision, speech will all have different best parameters\ntwo major ways to find the best parameters:\n\nbabysit one model - as its training, tweak the parameters and see how its doing on metrics. This is helpful when we don’t have enough computation capcity (panda approach)\ntrain many models in parallel - run many models in parallel, with different parameters (caviar approach)\n\n\n\n\n\nNormalizing activations in a network\n\ninstead of just normalizing the inputs to a NN, we also normalize the outputs of each layer of a NN - this is called batch norm.\nusing the standard (intermediate val - mean) / (std dev + epsilon). Epsilon is needed for numerical stability if variance is zero\nbatch norm sets the hidden layer to have mean 0 and variance 1, but sometimes we might want it to have a different distribution, so we can add a parameter beta to the hidden units to change the shape of the distribution. (like we might want a larger variance to take advantage of the nonlinearity of the sigmoid function).\nbatch norm makes the NN more robust and speeds up learning\nwhile batch norm can be applied before or after the activation function, in practice it is generally applied before the activation funciton.\n\nFitting Batch Normalization into a neural network\n\nDeep learning frameworks have batch norm built in, like tensorflow, keras etc.\nimplementing this ourself is straightforward, as we add in a norm step just before applying the activation function at each layer in the NN.\nIn each mini-batch, for each hidden layer compute the mean and the variance in that mini-batch, normalize, then apply the activation function as before.\n\nWhy does Batch normalization work?\nThree main reaons:\n\n1: normalizing input features speeds up learning, so one intuition is that this is doing a similar thing for each hidden layer\n2: makes weights in deeper layers more robust to changes in weights in earlier layers\n\nfor example, we train a network on black cats, and we try to classify a coloured cat. Every input is shifted, but the decision boundaries haven’t changed. This has a fancy name, coviariate shift.\nallows each layer to learn more independently\n\n3: regularization of hidden units\n\nadds some noise to each mini-batch, as each one is regularized on its own mean/variance, which has a slight regularization effect (bigger batches reduce noise/regularization)\nbut don’t rely on batch norm for regularization, as its just a unintended side effect, use other techniques like L2 or dropout\n\n\nBatch normalization at test time\n\nwe often predict one example at a time at test time, so the idea of a mean/variance doesn’t apply - we no longer have a mini-batch to process\nwe calculate the exponentially weighted average across all the mini-batches and use that to get a mean/variance to apply at test time on the one test example\n\n\n\n\nSoftmax regresssion\n\nis generalization of logistic regression which can predict multiple classes rather than just a binary.\n\nTraining a Softmax classifier\n\na hard max would look at a vector and put 1 for the max and zero for everything else\nsoft max takes in a vector and puts in a probability for each value such that they all sum up to 1\nloss function is trying to make the probablity of the “right” class as high as possible\n\n\n\n\nDeep learning frameworks\n\nimplementing a basic NN libary is a great learning framework\nas we implemnt complex/large models its not practical implement everything from scratch - there are many good frameworks to choose from\nthe DL frameworks not only speed up coding but implement many optimizations\nchoose one by looking at programning ease, running speed and how open it is\nmy own research has led my to tensorflow/keras or pytorch\n\nTensorflow\n\ncovers the very basics of tensorflow, though the tensorflow guide is better.\nwe implement forward prop, tf automatically does the backprop\na tf program looks like:\n\nmake tensors\nwrite operations to those tensors\ninitialize tensors\ncreate and run a Session\n\ntf.placeholder is a variable to which we assign a value later\n\n\n\n\n\nheads China’s National Deep Learning Research Lab\nbuilding a really large deep learning platform"
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#vs-code",
    "href": "posts/writing-about-code/setup-ubuntu.html#vs-code",
    "title": "Setup Linux",
    "section": "vs code",
    "text": "vs code\ncurl -L https://go.microsoft.com/fwlink/?LinkID=760868 &gt; vscode.deb\nsudo apt install ./vscode.deb\nUseful plugins:\n\nMarkdown All in One\nPython"
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#markdown-app",
    "href": "posts/writing-about-code/setup-ubuntu.html#markdown-app",
    "title": "Setup Linux",
    "section": "markdown app",
    "text": "markdown app\nCurrently I’m using Caret - a chromeos text editor app which runs lightening fast.\nvs code can handle markdown nicely, but it be slow. I prefer Typora.io or Caret.io for markdown and vs code is for coding. BUT all these apps are based on electron.\nInstall Caret by downloading the latest beta release .deb and:\nsudo apt install ./caret-beta.deb\nInstall Typora - haven’t tested this out, especially adding repo. Apparently you have to sudo apt install software-properties-common first before adding a repo.\n# add Typora's repository\nsudo add-apt-repository 'deb https://typora.io/linux ./'\nsudo apt update\n\n# install typora\nsudo apt install typora"
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#install-anaconda-for-a-better-python",
    "href": "posts/writing-about-code/setup-ubuntu.html#install-anaconda-for-a-better-python",
    "title": "Setup Linux",
    "section": "install anaconda for a better python",
    "text": "install anaconda for a better python\nSee"
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#nodejs",
    "href": "posts/writing-about-code/setup-ubuntu.html#nodejs",
    "title": "Setup Linux",
    "section": "nodejs",
    "text": "nodejs\nStep 1: install nvm, a script to install nodejs\ncurl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash\nstep 2: install nodejs itself by:\nnvm install node"
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#make-the-terminal-nicer-to-use",
    "href": "posts/writing-about-code/setup-ubuntu.html#make-the-terminal-nicer-to-use",
    "title": "Setup Linux",
    "section": "make the terminal nicer to use",
    "text": "make the terminal nicer to use\nthis is really important, cause if the terminal doesn’t look like something out of a movie, are you really doing something?\nI’m using Tilix, install by\nsudo apt -t stretch-backports install tilix\n\nmultiplex all the things\nIf using Tilix, no need to do this, but for the native terminal, install tmux:\nsudo apt -t stretch-backports install tmux\nthe only thing I really do with tmux is to split the terminal horizontally, then splitting one horizontal terminal vertically, for a total of three windows. Now there is a lot more about sessions and whats not, but the bare basics are:\nstart tmux by typing tmux, then press ctrl+b to enter command mode. \" splits the window horizontally and % splits it vertically. To move around, press ctrl-b arrow-key\nFor more customizatoin, make a .tmux.conf in the home directory and add:\n# Enable mouse mode (tmux 2.1 and above)\nset -g mouse on\n\n\njazz up the shell\nConsider oh-my-bash or bash-it for hacker level coding.\nalso install powerline-fonts and select a powerline font for the terminal.\nsudo apt -t stretch-backports install fonts-powerline\nUseful shell tools:\ntldr shows a short and useful help page for commands, e.g type tldr curl to get a synopsis of how to use curl.\n\nPreferred install: npm install -g tldr\nIf node not installed: pip install tldr\n\nbat a replacement for cat, displays files with syntax highlighting in the terminal. Install by downloading the .deb and sudo apt install ./bat_file.deb."
  },
  {
    "objectID": "posts/writing-about-code/setup-ubuntu.html#download-all-my-git-repos",
    "href": "posts/writing-about-code/setup-ubuntu.html#download-all-my-git-repos",
    "title": "Setup Linux",
    "section": "Download all my git repos",
    "text": "Download all my git repos\nThis command will grab json output of the first (or last?) 200 repos in my github and git clone them all one by one into the directory this command was run.\ncurl -s https://api.github.com/users/khalido/repos?per_page=200 | grep \\\"clone_url\\\" | awk '{print $2}' | sed -e 's/\"//g' -e 's/,//g' | xargs -n1 git clone"
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html",
    "href": "posts/writing-about-code/setup-mac.html",
    "title": "MacOS",
    "section": "",
    "text": "If a new mac, just copy paste the script below and go, otherwise consider a re-install if on a crusty handmedown.\n\n\ncopy paste this line into terminal on any mac big sur or newer and it will setup and install almost all the things:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/khalido/dotfiles/master/setup_mac.sh)\"\nAnd thats it! After a restart it should be ready to go. The setup script is pretty self explanatory.\nThe only thing I left out was python, which I’m installing through mambaforge if necessary.\n\n\n\nIf starting with an old mac, I recommend doing a factory reset and installing the latest macOS.\nPower on the mac and keep holding down Option/Alt + Command + R until a spinning globe appears. This boots you into recovery mode and offers to reinstall the latest version of OSX, which for me is Big Sur, with Monterey dropping in late ’21.\nI had to format the disk before installing as the installer wasn’t recognizing it. The eraser defaulted to the old MAC disk format, change that to AFPS with scheme GUID Partition Map.\nAlso see:\n\nApple reset page and recovery page.\n\n\n\n\nOlder notes below, no longer needed as the setup file does all this.\n\n\n\nautohide the dock: Its just a waste of space. No one should be looking or clicking on the dock anyways.\nDon’t press, tap! System Preferences &gt; Trackpad &gt; Tap to click\nthree finger drag is awesome. Accessibility -&gt; Mouse & Trackpad -&gt; Options -&gt; Enable dragging (choose 3 finger drag)\n\n\n\n\n\n\n\nhomebrew installs all the packages.\nInstall it by pasting this in a terminal:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" downloads and installs xcode cli tools and brew.\n\n\n\n\n\n\nKarabiner Elements to modify keyboard shortcuts, in particular make the capslock key into a super key which can launch alred.\nRaycast is a super duper launcher for macos. I remapped capslock to launch it. Even though spotlight is getting better, raycast is faster and lots more configurable.\nVisual Studio code is my cross-platform editor of choice. its awesome.\nMicrosoft Teams gotta talk to all the work people.\nSpotify gotten listen to music\nZoom - video chat service of choice in 2021, even for companies paying tons of money to other companies for video calls!\n\nMy script above installs them all in one go."
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html#do-the-setup",
    "href": "posts/writing-about-code/setup-mac.html#do-the-setup",
    "title": "MacOS",
    "section": "",
    "text": "copy paste this line into terminal on any mac big sur or newer and it will setup and install almost all the things:\nsh -c \"$(curl -fsSL https://raw.githubusercontent.com/khalido/dotfiles/master/setup_mac.sh)\"\nAnd thats it! After a restart it should be ready to go. The setup script is pretty self explanatory.\nThe only thing I left out was python, which I’m installing through mambaforge if necessary."
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html#install-the-latest-macos-if-needed",
    "href": "posts/writing-about-code/setup-mac.html#install-the-latest-macos-if-needed",
    "title": "MacOS",
    "section": "",
    "text": "If starting with an old mac, I recommend doing a factory reset and installing the latest macOS.\nPower on the mac and keep holding down Option/Alt + Command + R until a spinning globe appears. This boots you into recovery mode and offers to reinstall the latest version of OSX, which for me is Big Sur, with Monterey dropping in late ’21.\nI had to format the disk before installing as the installer wasn’t recognizing it. The eraser defaulted to the old MAC disk format, change that to AFPS with scheme GUID Partition Map.\nAlso see:\n\nApple reset page and recovery page."
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html#old-stuff-below",
    "href": "posts/writing-about-code/setup-mac.html#old-stuff-below",
    "title": "MacOS",
    "section": "",
    "text": "Older notes below, no longer needed as the setup file does all this.\n\n\n\nautohide the dock: Its just a waste of space. No one should be looking or clicking on the dock anyways.\nDon’t press, tap! System Preferences &gt; Trackpad &gt; Tap to click\nthree finger drag is awesome. Accessibility -&gt; Mouse & Trackpad -&gt; Options -&gt; Enable dragging (choose 3 finger drag)"
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html#app-setup",
    "href": "posts/writing-about-code/setup-mac.html#app-setup",
    "title": "MacOS",
    "section": "",
    "text": "homebrew installs all the packages.\nInstall it by pasting this in a terminal:\n\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\" downloads and installs xcode cli tools and brew."
  },
  {
    "objectID": "posts/writing-about-code/setup-mac.html#essential-apps",
    "href": "posts/writing-about-code/setup-mac.html#essential-apps",
    "title": "MacOS",
    "section": "",
    "text": "Karabiner Elements to modify keyboard shortcuts, in particular make the capslock key into a super key which can launch alred.\nRaycast is a super duper launcher for macos. I remapped capslock to launch it. Even though spotlight is getting better, raycast is faster and lots more configurable.\nVisual Studio code is my cross-platform editor of choice. its awesome.\nMicrosoft Teams gotta talk to all the work people.\nSpotify gotten listen to music\nZoom - video chat service of choice in 2021, even for companies paying tons of money to other companies for video calls!\n\nMy script above installs them all in one go."
  },
  {
    "objectID": "posts/writing-about-code/jupyterlab.html",
    "href": "posts/writing-about-code/jupyterlab.html",
    "title": "Jupyter Lab tips and tricks",
    "section": "",
    "text": "My collection of tips and tricks for using jupyter lab.\nJupyterlab is already installed with Anaconda, but if using miniconda install it by:\nconda install -c conda-forge jupyterlab ipywidgets\n\n\nlist all magic commands: %lsmagic\nMy shortlist:\n\nLoad a file: %load ./hello_world.py\ntime a cell: %%time\n\n\n\n\n\nTable of Contents - displays a toc in the sidebar.\nGo to Definition - ctrl-alt-b jumps to the point in the notebok the variable or function is defined\n\nJupyterlab extensions are built using nodejs, so install that by:\nStep 1: install nodejs using volta:\n# install Volta\ncurl https://get.volta.sh | bash\n\n# install Node\nvolta install node\n\n# start using Node\nnode\nInstall all the extensions then build in one go by:\njupyter labextension install @jupyter-widgets/jupyterlab-manager --no-build && \\\njupyter labextension install @jupyterlab/toc --no-build && \\\njupyter lab build --minimize=False\nNote: the build flag is because building was taking too long without this.\n\n\n\nSo there are tools like Pelican which are setup directly with jupyter notebooks, but I had already set up a hugo powered blog since its free, easy and just works, so my mini shortcut is to put all my notebooks the hugo content directory and run a little script there which converts the jupyter notebooks to markdown files:\nimport os\n\n# path to jupyter notebooks \npath = 'content/'\n\n# Find all jupyter notebooks in the content folder (and its subfolders)\nall_ipynb_files = [os.path.join(root, name)\n                   for root, dirs, files in os.walk(path)\n                       for name in files\n                           if name.endswith((\".ipynb\"))]\n\n# Remove all notebooks from checkpoint folders\nipynb_files = [x for x in all_ipynb_files if \".ipynb_checkpoints\" not in x]\n\n# converting to markdown, one at a time\nfor notebook in ipynb_files:\n    os.system(f'jupyter nbconvert --to markdown {notebook}')\n\nprint(f\"Converted {len(ipynb_files)} jupter notebooks to markdown.\")\nThe above works just fine, but i had a problem: the web server wasn’t displaying images from the jupyter notebook posts.\nOne solution is to go through every markdown file and fix the link so it points to the right file, but the easiest solution Google found me was this:\nIn the nbconvert config file ~/.jupyter/jupyter_nbconvert_config.py put the following line:\nc.NbConvertApp.output_files_dir = '{notebook_name}'\nThis saves the output file in a folder which is the same as the notebook name - I’ve found that way then the netlify webserver just serves up the image files without having to rewrite the image path inside the markdown file.\nNote: this post is a work in progress…"
  },
  {
    "objectID": "posts/writing-about-code/jupyterlab.html#magic-commands",
    "href": "posts/writing-about-code/jupyterlab.html#magic-commands",
    "title": "Jupyter Lab tips and tricks",
    "section": "",
    "text": "list all magic commands: %lsmagic\nMy shortlist:\n\nLoad a file: %load ./hello_world.py\ntime a cell: %%time"
  },
  {
    "objectID": "posts/writing-about-code/jupyterlab.html#extensions",
    "href": "posts/writing-about-code/jupyterlab.html#extensions",
    "title": "Jupyter Lab tips and tricks",
    "section": "",
    "text": "Table of Contents - displays a toc in the sidebar.\nGo to Definition - ctrl-alt-b jumps to the point in the notebok the variable or function is defined\n\nJupyterlab extensions are built using nodejs, so install that by:\nStep 1: install nodejs using volta:\n# install Volta\ncurl https://get.volta.sh | bash\n\n# install Node\nvolta install node\n\n# start using Node\nnode\nInstall all the extensions then build in one go by:\njupyter labextension install @jupyter-widgets/jupyterlab-manager --no-build && \\\njupyter labextension install @jupyterlab/toc --no-build && \\\njupyter lab build --minimize=False\nNote: the build flag is because building was taking too long without this."
  },
  {
    "objectID": "posts/writing-about-code/jupyterlab.html#blog-with-jupyter-notebooks",
    "href": "posts/writing-about-code/jupyterlab.html#blog-with-jupyter-notebooks",
    "title": "Jupyter Lab tips and tricks",
    "section": "",
    "text": "So there are tools like Pelican which are setup directly with jupyter notebooks, but I had already set up a hugo powered blog since its free, easy and just works, so my mini shortcut is to put all my notebooks the hugo content directory and run a little script there which converts the jupyter notebooks to markdown files:\nimport os\n\n# path to jupyter notebooks \npath = 'content/'\n\n# Find all jupyter notebooks in the content folder (and its subfolders)\nall_ipynb_files = [os.path.join(root, name)\n                   for root, dirs, files in os.walk(path)\n                       for name in files\n                           if name.endswith((\".ipynb\"))]\n\n# Remove all notebooks from checkpoint folders\nipynb_files = [x for x in all_ipynb_files if \".ipynb_checkpoints\" not in x]\n\n# converting to markdown, one at a time\nfor notebook in ipynb_files:\n    os.system(f'jupyter nbconvert --to markdown {notebook}')\n\nprint(f\"Converted {len(ipynb_files)} jupter notebooks to markdown.\")\nThe above works just fine, but i had a problem: the web server wasn’t displaying images from the jupyter notebook posts.\nOne solution is to go through every markdown file and fix the link so it points to the right file, but the easiest solution Google found me was this:\nIn the nbconvert config file ~/.jupyter/jupyter_nbconvert_config.py put the following line:\nc.NbConvertApp.output_files_dir = '{notebook_name}'\nThis saves the output file in a folder which is the same as the notebook name - I’ve found that way then the netlify webserver just serves up the image files without having to rewrite the image path inside the markdown file.\nNote: this post is a work in progress…"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html",
    "href": "posts/writing-about-code/PyConAu2019.html",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "update this post to fix youtube video links\nMy notes for the PyConAU 2019 talks I went to. This blog is an attempt to try and capture some useful info from the talks.\nFuture challenge: Make a jupyter notebook for each talk to implement some of the learnings or tools or thingamajig learned.\nThe pyconAU19 videos are up on youtube.\n\nYou dont always need NumPy\nFlying by the seat of our pants: our journey of teaching python using drones\nUsing Dash by Plotly for Interactive Visualisation of Crime Data\nBuilding, designing, teaching and training simulation environments for Machine Learning\nForecasting Australias 2019 Election with PyMC3\ncuDF: RAPIDS GPU Accelerated Dataframe Library\nUnderstanding GPUs\nNot a long time ago, in a galaxy not very far away, an astronomer and a computer scientist walk into a bar…\nLearn to control your brain: Brain Computer Interfacing with Python\nMachine Learning and Cyber Security: Detecting malicious URLs in the haystack\nCreating Lasting Change\nLessons learned building Python microservices\nTunnel Snakes Rule! Bringing the many worlds of Python together to monitor Melbournes biggest infrastructure project.\nUsing python programmed microcontrollers to build comedy props\nPython Applications in Infrastructure Planning and Civil Engineering\nFantastic Blocks And Where To Hide Them\nProfiling Pathogens with (micro) Python\nThe Antipodes\nSaturday Lightning Talks\n\nSam Bishop: Computational World Building\ndata driven look at code comments\nlogging\n\nThe real costs of Open Source Sustainability\nShipping your first Python package and automating future publishing\nInsights into Social Media Data using Entropy Theory\nIt’s dark and my lights arent working (an asyncio success story)\nThe universe as balls and springs: molecular dynamics in Python\nInstant feedback, instant-debugging Python coding\nOrchestrating complex (not complicated) tasks using AWS serverless and Python\nSunday Lightning Talks\n\nPre european potatoes\nQuantum Computing\nBatteries included but leaking\n\nmega takeaway\nmisc notes\n\n\n\n\nThe numerical Python ecosystem and communities are mature and powerful, but sometimes we can be too quick to reach for the numerical hammer when simpler options exist. This talk will outline some areas where the numerical stack may not be the best starting point, and survey some alternatives. #\n\n\nSam Hames, software dev at QUT’s Digital Observatory.\nthe numerical python stack is very complicated, and its a differnt idiom to the other types of python\nnumpy is basically arrays and vectorized expressions on these arrays - but how to translate - for example using a dict to represent a bag of words {'the': 21, 'bag': 2} - its clear, there is a one to one mapping b/w word and its count - but using numpy arrays breaks that, you end up with two arrays like [21,2] and ['the', 'bag']\nwhen writing numerical python, remember that the rest of python exists\npythons built in data structures- lists, sets, dicts and tuples exist and are very very good - you can use them in many applications instead of numpy!\n\ndepending on the task, sometimes you need that numpy speed for certain ops\n\nlists - appends are fast, you can have dicts of lists\nsets - adding, deleting and checking membership are all fast\ntuples - work very well for things like metadata description, and you can have a set of tuples (you can’t have a set of lists)\ncombining python data structures is easier to understand\nthe stdlib has lots of things built in:\n\ncollections:\n\ndefaultdict can simplify code\nCounter - count all the things\n\nheapq\nbisect\n\nGenerators and streaming - you will run out of memory at some point\n\nexample pandas.read_csv is used a lot but it fits the whole file into memory - instead you can read a file line by line by using\n\n\n\nwith open('very_big_file.txt', 'r') as f:\n  for line in f:\n    results = (do_something(line) for line in f)\n\nmaybe don’t do everything in python, use a database?\n\nsqlite can do a lot of stuff, example compute and store a bag of words\n\nthere are lots of domain specific databases, both local and hosted, and they can be a great complement to a python stack\n\nthis way you only have to worry about a subset of the original python\n\nlook for task specific libs\n\ntakeaway: document all the numerical things, and don’t be afraid to mix and match. Whats the actual problem you’re solving - does it matter if its taking 1ms in numpy vs 5ms in python?\n\n\n\n\nA few years ago we found ourselves teaching students everything from Python to Arduino. It was our plan to replace all the other languages with Python as we streamlined our course. Learn why we introduced drones, what we learned not to do and what we were surprised about. #\n\n\nKylie Mathers - teacher at Marymount catholic school in Qld\nthe school used a veritable kitchen sink of languages, wanted to switch to one language, preferrably open source, which was useful after students left school and worked with prototyping electronics (beeps and leds makes kids heads light up)\nsome considerations:\n\nblock based programming is too..ugh\nApple swift and playgrounds need ipads, those are expensive and lock you in\nthey had drones, wanted to use them, and guess what, you have libs like pyparrot to program drones in python\n\ndrumroll…. they choose python!\nthe talk covered introducing students to the big picture concept of drones, their issues and applications and things to think about\nso after ages, the students finally went hands on with python and drones\n\nused psuedo code, flowcharts to explain concepts\nthen real code, including functions like make_it_rock, circle etc etc\nseeing the drone do stuff in real life becuase of their code was very motivating\n\nhighly visible learning is great.\n\ntakeaway: theres a lot to research and think about before even starting to code and fly a drone. I guess that applies to programing in general - so much problems in python are best approached after a literature research for best practices.\nHighly visibile reactions are great, so try and use that with real projects as well.\n\n\n\n\nDash is a great Python framework for building data visualisation websites. In this talk I discuss the framework basics, explore a sample site and describe its use for an enterprise application to graph crime statistics. I finish with clear pros and cons of the Dash framework from our perspective. #\n\n\nLeo Broska github, software dev at ACIS, works with criminal data\nwith any numerical data, eventually you have to put it into a graph or dashboard\nDash is a python framework for building analytical websites, with no javascript required\ndash example apps:\nplotly does all the drawing and charts in dash, and react powers the interactivity layer for dash, flask actually delivers the webapp - every dash site is a flask app\n\nthere is not need to call the internal components of dash for simple things\n\nMake a simple dash app:\n\nimport your data, make a figure object which graphs/maps it using plotly express. use fig.update_traces to tweak the fig object.\nthen make a dash app, give it a layout - this defines what the app is serving, which is typically html or markdown text and the plot objects created earlier\nnow run the app!\n\nNote: use a server like gunicorn for production\ndash is great but some issues:\n\nslow, rendering is slow, underlying javascript code very big\nmulti-page sites awkward to code\n\n\ntakeaway: Just tried out plotly on my laptop, and it is a bit slow. Use it with caution as needed. It looks great for internal dashboard where you have beefy machines and fast connections to users, for personal use to dump on the internet a dash app is a hefty thing to host.\n\n\n\n\nImagine you’re building a fancy robot-driven warehouse. Your pick, place, and packing robots need to get around quickly, find the right item and put it to the right place without colliding with each other, shelves, or people. But you don’t have any robots yet, and you need to start. Try simulations! #\n\n\nParis Buttfield-Addison, Phd in CS, professional game dev and ml fan\nUnity isn’t open source but is free\nUnity Machine Learning Agents Toolkit (ML-Agents) - use python to connect to unity for using it as a training environment\nblog post explaining the talk is up at his website\nunity can be programmed using blocks, its very friendly, so don’t get scared\n\ntakeaway: Unity is easy to use, try it out with python!\n\n\n\n\nCan we predict the result of an Australian election before it occurs? How certain of the outcome can we be? My talk will use the 2019 Australian federal election as a case study to provide an entry-level introduction to the benefits of probabilistic forecasting and PyMC3. #\n\n\nMartin Burgess, github buckleysandnone\nnotebook for talk, includes slides and code\nprob forecasting tries to estimate te relative prob of all possible outcomes e.g weather forecasting\nthe future is uncertain, and prob forecasting gives us an estimate of how likely different outcomes are\nppl have different starting assumptions, probabilistic forecasting defines them up front\nPyMC3\n\ntakeaway: forecasts with probabilities are much more useful than just a plain number. With the last Australian elections, practically every channel had just the one number for wins - but if they had attaced a likeihood to it and shown the other likely scenarios it would have been a lot more useful.\nApply probabilistic forecasting to a future issue. Read this book and ThinkBayes.\n\n\n\n\nRAPIDS open-source software enables end-to-end data science and analytics pipelines to run entirely on GPUs. Key to RAPIDS is cuDF, a pandas-like Python data frame library with a high-performance CUDA C++ implementation. cuDF and RAPIDS enable large speedups for end-to-end data science using GPUs. #\n\n\nMark Harris, engr at rapids.\nmoving and transforming data is slow\ndata processing evolution for query, ETL and ML train steps:\n\nhadoop would read and write data to disk at every step\napache spark kept data in memory all the time\ntraditional gpu processing would read data into gp, do something then write to cpu. a lot of gpu speed gains were lost in the read write steps\napache arrow provides a common in memory data format so different tools can use the same format and save the data conversion steps\nrapids uses arrow to skip many of the data conversion steps, much faster\n\nrapids aims to accelerate existing python tools, like the pydata chain\nthe average data scientist spends 90% of their time in ETL\nenter cdDF: rapids dataframes to save the day by drastically speeding up ETL\n\nruns on libcuDF, a low level CUDA C++ lib which does all the work on the GPU\ncuDF provides a pandas like api, creates gpu dataframes from numpy arrays, pandas dataframes or pyarrow tables\nbridges python (a dynamic language) with C++ & Cuda (static languages)\ngpu accelerated i/o - 10x faster than pandas\n\nrapids speeds up workflows on a single pc with one gpu - for bigger data it works with dask for distributed computing\ncuML algorithms are largely compatible with sklearn, just much faster\n\n1 v100 gpu is 5-100x faster than 2x20 core cpu.\na regular desktop gpu is plenty fast too for a normal user\n\nRapids works in Google Colab, see getting started.\n\ntakeaway: should drastically speed up many of the bigger datasets I’ve tried in pandas, where a clean and transform pipeline would take many minutes. cuDF promises to at drastically speed up this process, making it easier to iterate faster.\nSo look into getting a Nvidia powered computer suitable for using with rapids, this would have drastically sped up some of my machine learning projects.\n\n\n\n\nWith torch and tensorflow we have begun to rely on GPUs to speed up computations. Deep Learning or not, GPUs can provide massive computation speed ups but it’s not a panacea as NVIDIA would have you believe. Understanding how GPUs work can tell us where we should and shouldn’t use them. #\n\n\nVarun Nayyar, mathematician - linkedin\nwhat are GPU’s good for? graphics can be fundamentally reduced to matrix operations, DL is just matrix multiplication with a non linearity.\nCUDA: gpu’s are made up of streaming multiprocessors (SM’s) each with many cuda cores, say 64-128. They run thousands of threads simultanesouly vs 10s for a cpu.\nreal world cuda threads are divided into blocks of 32 threads called a warp, and threads in a warp run at the same time.\n\na rtx2080ti can run over 4K threads in one go\n\nGPU’s are generally memory bound, not compute bound.\nDeep learning: each forward pass is just a matrix multiply, as is backprop\nGradient Descent is a sequential algorithim - gpu compute hasn’t changed how it works, just makes it faster by 20-30x on a fully connected network.\nConvolutions are compute bound on a gpu, easy to parralize\nRNN’s are memory bound\ngpu’s work well with GradientBoosting\ngpu’s have variable perf depending on the algo, so its not straigtforward to put algorithms on the gpu. CPU and GPU implementations can differ.\ngpu sync is slow - stick with single gpu’s for personal use, you need a strong engineering team for multi-gpus.\nlocal compute is great - pays for itself soon over paying for cloud. don’t skimp - go for a rtx2080ti\n\ntakeaways: very impressive talk. Worth rewatching if doing something with GPU’s. For my next ML project consider getting my own gpu box running linux and look at libraries like rapids.\n\n\n\n\nPython is one of the most popular programming languages in astronomy. In this talk, I will tell a story about how Python helped me to develop a software tool for galaxy modelling, and tackle the scientific and technical challenges that arise in the Big Data era of astronomy. #\n\n\nGeorgios Bekiaris, astrocoder, makes software for astronomy\nwrote gbkfit - tool for galaxy kinematic modelling\n\ninitially in c++, rewrote twice, finally rewrote in python\n\ngalaxy kinematics refers to the motions of stars, clouds of gas\ndoppler effect: approaching: higher freq, receding: lower freq\nkinematics combines spectroscopy with doppler effects\nsee talk…\ncommonly used libs: Astropy, scipy and matplotlib.\nnumpy code ran as fast or faster as c++ on a single cpu. numba makes it easy to speed up python code by applying a numba decorator to a function. Also supports gpus.\n\ntakeaway: Astronomers are using python for everything, and python can be fast with the right tools.\n\n\n\n\nNeurofeedback is a brain-computer interface where a person’s own brain waves are audio/visually presented back in real-time after they’ve been recorded and filtered within a few milliseconds. We present methods to allow people to see their own brainwaves with python. #\n\n\nJohan van der Meer, neuroscientist at the QIMR Berghofer Medical Research Institute in Australia. github\nbasics: grab signals from the brainusing EEG, do real time analysis, send a feedback signal\nbrain has many neurons, when a neuron fires a electrical field is generated which is pretty tiny - but when many neurons fire together it makes a big field which you can measure from even outside the brain\nsignal changes with action - exampls eyes open/close.\neeg resolution is quite poor/limited\nEEG signals can have rythyms, so thus you can do frequency analysis\nmne - open source python lib for neurophysical data\nfor better signals - you have to measure from inside the brain - see neuralink.\neeg is non-invasive, portable and cheap\nlabstreaminglayer is a good library to stream realtime data from a measuring device.\nso we have devices, we have libararies to connect to them and python to glue it all up.\nDecoding all this data: this is where the research is, figuring out what it means and using it do useful things, like control a wheelchair etc.\nneurofeedback learning: with some mental disorders ppl have irregular brain rythyms. Idea is that regularzing them with “training” will help improve disorders.\nEncoding: how much data can we transmit?\nopenbci looks really cool.\n\ntakeaway: lots of interesting work being done. Interesting thing is that the neural data is pretty straightforward, no wonder teenagers are building brain controlled prosthetic arms these days!\nExplore openbci further and hack my own neuro thingamajig.\n\n\n\n\nToday, security teams are in an increasingly one-sided battle to defend against a myriad of cyber attacks. Web-based attacks are often devastating, with conventional blacklists and reputation-based defence tactics not able to identify previously unseen malicious URLs. Is AI the solution? #\n\n\ndesign thinking: put yourself into the shoes of your user at the start of a project\n\nKnow your user, understand their pain points and what they do\nNail the problem\nIdeas\nKnow the threat\npython - data, engr features, model/eval\n\ndid a lot of feature engineering on the urls, things like getting whois data, domain expiry dates, location info, etc etc.\nthey used fast.ai for deep learning, sklearn for randomforests and tensorflow for word embeddings\nused F1 score to evaluate models\n\ntakeaway: see the presentation again… it was a good explaination of a end to end process.\n\n\n\n\nThe Nature of Organisations, People, and how to Change Them #\n\n\nAurynn Shaw @aurynn, devops at Eiora\ntalk is about introducing change\nspoke about Contempt culture in the tech world\n\nwhat has pervasive hostility done in tech jobs?\n\nall new tech exists in a business context, and biz don’t care about technologies. Devs care about tech, not businesses.\n\nbiz cares about outcomes - can i deploy it, can i find ppl to work on it and the other things biz needs to care about\n\nexpressing opinions in contemptuous terms gets you sidelined and not trusted\n\nyou diss one tech, like another tech - makes you look biased\nwe have done this to ourselves.\n\nif they think its necessary and we don’t or vice versa its not them at fault there is a communications gap.\nand thus we end up in a process which doesn’t believe in engineers.\n\nwe should not be speaking only in a language of tech and treat our assumptions as better - we need to speak in the language of biz\n\nprocesses and status quos arise from past biz experiences\nthe cloud is where infrastructure as code become real - 13 yrs ago. Some companies are still having conversations about entering the cloud. Cause they don’t care about the tech and ease of the cloud, they care about risks and costs and regulations and so on.. the easy of spinning up services is a small thing.\nsomeone needs to communicate that\npower dynamics dictate change\nlistening is key to change\nnew processes need to make the lives of others betters and show them how its solving problems (and in the process you get to roll out the shiny new tech you care about)\nall technology is political - tech encodes the structures of the org which made it\n\nthings which make no sense often are because of the initial org constraints, and thats why they don’t make sense to a different org with different constraints\nunderstand the political ramifications of tech\n\ngo and talk to ppl. make conversations safe.\n\ngood example of Etsy\n\nlearn to speak business. Be empathetic to the entire org.\n\ntakeaways: Its not just them, its us. Solve business problems and communicate how your solution helps. Don’t spend too much time on we should have done it right in the first place. It just so happens that we adopt new tech as sometimes its better/faster/cheaper than the old.\n\n\n\n\nI will talk about challenges and wins that have come from introducing Python into a multilingual microservices kubernetes architecture with lots of legacy. #\n\n\nRichard Jones, dev at reecetech, a plumbing company\n\ntrys to make every service look similar, tools to enforce this\n\ncookiecutter\ntox\ngoal is full test coverage, pytest cause its best in class\nblack to format code\npycharm for all devs so ppl can help other ppl easily without editor shock and for ease of pair programming\npactman for contract testing of services\ndjango to deliver the services\n\ninconsistencies happen, so you have to check across teams. i.e one team switched tool cause they found it too hard, so now you have inconsistencies across projects\nresilience: microservices are prone to brief errors or tiny service interruptions. So use http retries for some errors\nmoniter services using grafana dashboards and kibana for monitering and searching logs\nget it running right first, then use tools to investigate performance, like silk for dango\n\nreducing number of sql queries was key to speed\n\nbatch interfaces is good, like allow consumer to get 100 price requests in one go, much faster than 100 different requests\ncaching really helped. @lru_cache(maxsize=1024) decorator built into python does the job.\n\ntakeaway: make things simpler by taking away choices by using automated tools. Test and monitor services.\n\n\n\n\nPython is being used to provide real-time environmental monitoring on the Melbourne Metro Tunnel project. Come along to see how open source Python tools from the web, IoT, cloud infrastructure and scientific domains are being used together to monitor environmental telemetry on a city-wide scale. #\n\n\nEvan Brumley, engr at WSP Digital\nworking on 7 construction sites for Melbourne Metro\none construction site next to hospitals, lab etc which can’t be disturbed - so they have a bunch of quantative requirements to meet and report on in real time.\nso, how to keep track of all the requirements?\nold school approach was to send grad engineers to site, collect readings, analyze in excel and file reports at the end of the month\nmodern approach: get sensors from a Vendor with a Saas platform, give them lots of money, download csvs, analyzie in excel and file reports at the end of the month.\n\nthey don’t respond to custom reports, don’t integrate with other vendors sensors\n\nsolution: build a new platform which could accept data from any device\nhint: try not to work with devices directly\n150-200 sensors, some sending data at a half second freq\nvalidate and store telemetry\nenvirnomental requirements don’t map directly to sensor data, so calculations needed to transform them - which are sometimes complex, and have to performed in real time\naccess to data - both internal as well as limited external access\nalerts and reporting\nthey had 4 months to build this out, fully self contained team of 2-3 devs + 1PM\nAWS Kinesis to store streaming data, S3 for resilient storage, then influxdb.\napi pollers packaged into docker containers and deployed via elastic beanstalk\n\nused pyftpdlib\n\nweb app is built on django+celery+react, powered by pandas and the scipy stack\n\npandas allows them to transform raw telemetrym live, on request - using upto 10K points in a dataframe. Pandas was a huge timesaver.\n\n\ntakeaway: dang. that is a lot of stuff.\n\n\n\n\nEarly-career comedians often have difficulties adding electronic props to their acts, due to the high cost of materials and fabrication skills required. This talk will recreate several props used in comedic performances, showing the code and components used. #\n\n\nAnthony Joseph & Debbie Zukerman\nused adafruit wearable devices, circuit playground, microbit, arduino\nqlab\n\ntakeaway: explore micropython and get a device to play with.\n\n\n\n\nEngineers tasked with planning new infrastructure constantly face the problem of having to look through too much information. This talk is about how we wanted to be lazy and wrote a bot to do it for us instead. #\n\n\nBen Chu, grad engr at WSP, a large engnr firm\nlots of stuff involved in railway planning\nEIA used to be done by junior staff, using datasets like noise receivers (hospitals etc), vegetation (what areas will the train cross)\n\nthese add up quickly to a lot of stuff\n\nso instead they are using jupyter notebook\n\nshapefiles store shapes like the railway line and vegetation and noise senstitive areas\nrun in [Papermill](https://github.com/nteract/papermill which outputs html, csv and shapefiles\n\nneed to check DA’s along the railway line which might impact on the line - there can be many thousands.\n\nused to be done manually, paid $70 per DA, slow, costly and infrequent updates\nthey need to know as soon as the DA comes in as it could pose a high risk to the railway design\n\nDABot automates this - it merges geocoded national address file and lot shape datasets, filters for lots on the buffer alighnment, which gives a list of addressses which they can use to search for DA’s matching those addresses.\n\nmajority of council websites are the same, so same scraper works on most of them\nNLP pipeline to clean text, then they form a document term matrix, used 200 most imp features\nML: went with XGBoost, got 85% Accuracy with a 95% recall\n\nmaximized recall as they didn’t want to miss high risk DA’s\n\n\nDABot allowed for frequent updates, saved time/money\nFuture: lots of improvements to do, from using word embeddings, deep learning,\n\nPostGIS database\nfully automated pipeline so they can send out automated weekly reports\n\n\ntakeaway: Impressive how a very simple NLP pipeline and application of XGBoost gave them such good usable results. They didn’t even use word embeddings! There is a lesson in this. Build simple, improve later.\n\n\n\n\nRuby has blocks. JavaScript has blocks. Swift has blocks. Python doesn’t have blocks.In this talk, we’ll look at why Python doesn’t have blocks, and recent programming techniques that have developed in languages that do have blocks. Then we’ll look at what we – or Python – can do about it! #\n\n\nChristopher Neugebauer, @chrisjrn, snr engr at AlphaSights and a director of PSF\nblocks was last discussed in 2005 for Python, and is unlikely to appear\nusing Kotlin at AlphaSights - Kotlin encourages passing blocks of code around\nPython isn’t really for functional programming, instead they have list comprehension\nBlocks: in python you can have a func (many lines) or a lambda (one line long)\n\nso you always need to define a func and pass that. In other languages you can easily pass a block of code\n\nContext Managers: when opening files, you had to manually close them. Easy to miss. Hence context managers.\n\nthe python syntax enforced correct behaviour\n\n\ntakeaway: things to thing about. Convention matters.\n\n\n\n\nWe’re building professional medical diagnostics equipment with micropython. This has come with minimal challenges, many positives and a few surprises! #\n\n\nAndrew Leech, Planet Innovation\nworking with Lumos to develop point of care diagnositcs tests (spinoff of PI)\nBuid a medical device:\n\nhazard and risk based development: at every stage minimise risks\n\nLumos Camera Reader runs micropython, connects to a phone via bluetooth\nSOuP: Software of unknown povidence\n\nfor medical devices, you need certified code, or blackbox it.\n\nuse jupytermicropython kernel to run code directly on the board. Makes dev very easy.\n\nuse ipywidgets to do live interactions.\n\nuse test driven development - the same unittest can run on the board, desktop and CI\nmedical devices aren’t that different from regular devices, they just need to blackbox all the uncertified bits.\n\ntakeaway: use the jupyter kernel for micropython for live dev on a board.\n\n\n\n\n@brandonrhodes, #\ngreat spkr, watch his technical talks as well\nturn me me me me you into you you you you me - works great in life for things from emails to conversations\n\ntakeaway: watch more talks by great talkers. Put the important stuff first, both in programming and emails.\n\n\n\n\n\n\nusing processing.py to build worlds and model weather using isca\ndeep deep rabbit whole of modelling and fitting worlds into the solar system\n\n\n\n\n\n@veronica_hanus\nthink about inline comments as documentation\n\nautogen docs from inline comments\n\ncomments can be magic\n\n\n\n\n\n@datanerdery\nlogging is good! built into the stdlib\n\nmultiple levels of details\n\n\n\n\n\n\n\nWhat if money isn’t the only way to create sustainable free and open source software projects? What if it turns out that sustainability is actually a multi-faceted concept that can’t truly be successful if people focus on only one of its many elements? #\n\n\nVM Brasseu @vmbrasseur\n\ntakeaway: code of conducts matter, look for, abide by and try to contribute to open source projects which have them.\n\n\n\n\nOne of the best things about Python is the vast ecosystem of packages available on the Python Package Index. Shipping your first Python package can be intimidating. This talk aims to remove the mystery of Python packaging and enable you to share your code with the Python community. #\n\n\nChris Wilcox, dev at Google @chriswilcox47\npypi: python package index makes python great - its easy to install and use packages\nmanual steps: (don’t do this)\n\nmake a packakge in the format pip expects along with a setup.py\nmake a venv and do a local install of your package and test it.\nupload to testpypi, and install from there\nif everything works, upload to pypi\n\nuse setup.cfg, specify all the metadata there.\n\nbunch of classes in classifiers you can assign your package to\nAlways add a licence.\nyou can add long_description = file:README.md which can be just the readme in github.\nyou can define things like packages required\n\nauthentication: pypi supports auth tokens, can make one per package and assign it permissions like upload package\nto start a new project\n\npypi has a sampleproject repo\ncan use cookiecutter too, or just build up a good basic setup and just copy paste and change for new projects -automation tools:\ntox is the most common\nnox is more flexible, configured with python scripts. can build docs as well.\n\nreally automate stuff with CI - once setup properly publishing a release to the git repo will build, test and deploy:\n\ncircleci/config.yml to define the build and deploy sequence.\ncircle ci checkouts out your repo into a docker image, installs the things required, runs tests, builds, and publishes the new version\n\n\ntakeaway: publish my first package. use nox and circleci to automate all the things.\n\n\n\n\nEntropy theory is usually thought of as something that applies to matter and energy, but it turns out that we can apply the same techniques of analysis to social media sites. Join me as we study the thermodynamic behaviour of users on Twitter, and learn how to analyse it better. #\n\n\nMars Geldard github, honours student at U of Tas\nawesome talk, see preso slides here.\nInteresting application of entropy theory to social media analysis.\n\ntakeaway: Look at other fields for ideas on how to tackle problems.\n\n\n\n\nI have invested huge amounts of time in achieving a simple goal – making the lighting in my home “smart”. It’s not ground breaking, nor is it practical or cost effective, but it sure was educational, uses a bunch of Python, and the result makes me (and my family) happy. #\n\n\nJim Mussared @jim_mussared github\nrenovated his house in 2015 wanted smart everything\nused zigbee enabled bulbs\ndigiXBee board runs micropython\n\ntakeaway: home automation is a deep deep rabbit hole to jump into. This guy should get a Medal for Bravery above and beyond the call of duty from Zigbee and Samsung smartthings.\n\n\n\n\nSurprisingly, we can approximate matter as a bunch of balls on springs and learn things about our bodies and the world. This talk will look at the different stages of molecular dynamics (MD) simulations and how Python is changing everything. #\n\n\nLily Wang, working on PhD on molecular dynamics at ANU\ndefine an atom as a ball (radiaus, mass) with everything else as springs then you can use molecular dynamics to model a system of atoms. To study proteins, viruses etc\nopenmm lib for molecular simulations, also MDanalysis\nplotly for interactive viz for the win!\nMD in the past: rubber balls stuck together\nnow: simulating viruses with millions of atoms using supercomputers\n\ncurrently sims top out at millions of atoms, so long way to go to simulate bigger structures\n\nOpenForceField: open science initiative\n\ntakeaway: molecular dynamics is hard. I liked how they simplified a very complex thing - proteins, viruses - to a simple system which they could model computationaly and hope it approximates the real thing.\n\n\n\n\nBuilding on Bret Victor’s famous ‘Inventing on Principle’ presentation, we look at writing Python where the code is instantly run and every line visualized after every single keystroke. There’s a future beyond the text-editor -&gt; console-run loop and this is a taste of it. #\n\n\nRobert Lechte @djrobstep web\npython demo: code on left, instant output on the right\nthings take a while a change. new forms of media like newspapers enabled by the printing press took ages to appear\nsame with computers\ndevs spend too much time on understanding code vs understanding the problem\nwatch:\n\nGary Bernhart “Whole new world” talk on reinventing the terminal\nBret Victor “Inventing on Principle”\n\nsegway example showing live reaction of a simulated segway to programming is awesome\n\ntakeaway: instant feedback is genius. I want a instant feedback python IDE. Actually strip the ide. Just a simple two pane interface, one showing code, other the results. Someone pls build a vscode or jupyter extension for this. I mean ppl are sending keystrokes to cloud AI’s to predict smart suggestions, just running the code must be easier than that!\n\n\n\n\nPython and serverless technologies are a great way to quickly scale a project, but what can you do when things get complicated? Here are some patterns to keep you sane. #\n\n\nMichael Kelly, web, github from versent\nlambda functions\n\nadv: cheaper, little config, encourages refactoring, easy learning curve\nlimitations: runtime limits\n\nuse aws step functions to build a state machine to connect different services together, trigger lambads and so on\n\nstate machines hold state unlike lambdas\n\nsee talk for project template example\nuse black for code linting.\n\ntakeaway: instead of making overly complex lambda functions, model the problem as a finite state machine, make simple lambdas as appropriate then make a state machine which embeds some of the logic and calls lambdas.\nThen you can make a state machine to watch over all your other state machines!\n\n\n\n\n\n\npototoes were far away from NZ. got there long time ago.\ngrowstuff is tinder for potaoes.\n\n\n\n\n\ncool new thing\nbasics of QM: instead of bits, you have qbits which have a vector instead of being a 0 or 1.\nyou can access quantum computers free online\nsee pennylane\n\n\n\n\n\ntalk about fear of removing old and semi dead bits of the stdlib\npep594 will break a lot of corporate code which doesn’t have replacements - this places the burden of maintainence from the corporations to python\nsome of the libs broken can be easily updated\nmake pythons development easier, kill the nasty old bits\n\n\n\n\n\nIts nice to hear talks in real life instead of on youtube. Focuses the attention.\nMoving forward, my biggest takeaway is to take an idea and just run with it. Many of the speakers had an itch - a home to automate with python, a paper to write or drones to fly and just did it. So take one of my project ideas and just do it, using circleci of course.\n\n\n\n\nEmbed youtube vidoes by {{&lt; youtube EqQj5os3Mfw &gt;}} or {{&lt; youtube id=\"w7Ft2ymGmfc\" autoplay=\"false\" &gt;}}\ncan’t use ' or - in anchor links for some reason. Chrome doesn’t jump to anchors! Bad chrome!"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#you-dont-always-need-numpy",
    "href": "posts/writing-about-code/PyConAu2019.html#you-dont-always-need-numpy",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "The numerical Python ecosystem and communities are mature and powerful, but sometimes we can be too quick to reach for the numerical hammer when simpler options exist. This talk will outline some areas where the numerical stack may not be the best starting point, and survey some alternatives. #\n\n\nSam Hames, software dev at QUT’s Digital Observatory.\nthe numerical python stack is very complicated, and its a differnt idiom to the other types of python\nnumpy is basically arrays and vectorized expressions on these arrays - but how to translate - for example using a dict to represent a bag of words {'the': 21, 'bag': 2} - its clear, there is a one to one mapping b/w word and its count - but using numpy arrays breaks that, you end up with two arrays like [21,2] and ['the', 'bag']\nwhen writing numerical python, remember that the rest of python exists\npythons built in data structures- lists, sets, dicts and tuples exist and are very very good - you can use them in many applications instead of numpy!\n\ndepending on the task, sometimes you need that numpy speed for certain ops\n\nlists - appends are fast, you can have dicts of lists\nsets - adding, deleting and checking membership are all fast\ntuples - work very well for things like metadata description, and you can have a set of tuples (you can’t have a set of lists)\ncombining python data structures is easier to understand\nthe stdlib has lots of things built in:\n\ncollections:\n\ndefaultdict can simplify code\nCounter - count all the things\n\nheapq\nbisect\n\nGenerators and streaming - you will run out of memory at some point\n\nexample pandas.read_csv is used a lot but it fits the whole file into memory - instead you can read a file line by line by using\n\n\n\nwith open('very_big_file.txt', 'r') as f:\n  for line in f:\n    results = (do_something(line) for line in f)\n\nmaybe don’t do everything in python, use a database?\n\nsqlite can do a lot of stuff, example compute and store a bag of words\n\nthere are lots of domain specific databases, both local and hosted, and they can be a great complement to a python stack\n\nthis way you only have to worry about a subset of the original python\n\nlook for task specific libs\n\ntakeaway: document all the numerical things, and don’t be afraid to mix and match. Whats the actual problem you’re solving - does it matter if its taking 1ms in numpy vs 5ms in python?"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#flying-by-the-seat-of-our-pants-our-journey-of-teaching-python-using-drones",
    "href": "posts/writing-about-code/PyConAu2019.html#flying-by-the-seat-of-our-pants-our-journey-of-teaching-python-using-drones",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "A few years ago we found ourselves teaching students everything from Python to Arduino. It was our plan to replace all the other languages with Python as we streamlined our course. Learn why we introduced drones, what we learned not to do and what we were surprised about. #\n\n\nKylie Mathers - teacher at Marymount catholic school in Qld\nthe school used a veritable kitchen sink of languages, wanted to switch to one language, preferrably open source, which was useful after students left school and worked with prototyping electronics (beeps and leds makes kids heads light up)\nsome considerations:\n\nblock based programming is too..ugh\nApple swift and playgrounds need ipads, those are expensive and lock you in\nthey had drones, wanted to use them, and guess what, you have libs like pyparrot to program drones in python\n\ndrumroll…. they choose python!\nthe talk covered introducing students to the big picture concept of drones, their issues and applications and things to think about\nso after ages, the students finally went hands on with python and drones\n\nused psuedo code, flowcharts to explain concepts\nthen real code, including functions like make_it_rock, circle etc etc\nseeing the drone do stuff in real life becuase of their code was very motivating\n\nhighly visible learning is great.\n\ntakeaway: theres a lot to research and think about before even starting to code and fly a drone. I guess that applies to programing in general - so much problems in python are best approached after a literature research for best practices.\nHighly visibile reactions are great, so try and use that with real projects as well."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#using-dash-by-plotly-for-interactive-visualisation-of-crime-data",
    "href": "posts/writing-about-code/PyConAu2019.html#using-dash-by-plotly-for-interactive-visualisation-of-crime-data",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Dash is a great Python framework for building data visualisation websites. In this talk I discuss the framework basics, explore a sample site and describe its use for an enterprise application to graph crime statistics. I finish with clear pros and cons of the Dash framework from our perspective. #\n\n\nLeo Broska github, software dev at ACIS, works with criminal data\nwith any numerical data, eventually you have to put it into a graph or dashboard\nDash is a python framework for building analytical websites, with no javascript required\ndash example apps:\nplotly does all the drawing and charts in dash, and react powers the interactivity layer for dash, flask actually delivers the webapp - every dash site is a flask app\n\nthere is not need to call the internal components of dash for simple things\n\nMake a simple dash app:\n\nimport your data, make a figure object which graphs/maps it using plotly express. use fig.update_traces to tweak the fig object.\nthen make a dash app, give it a layout - this defines what the app is serving, which is typically html or markdown text and the plot objects created earlier\nnow run the app!\n\nNote: use a server like gunicorn for production\ndash is great but some issues:\n\nslow, rendering is slow, underlying javascript code very big\nmulti-page sites awkward to code\n\n\ntakeaway: Just tried out plotly on my laptop, and it is a bit slow. Use it with caution as needed. It looks great for internal dashboard where you have beefy machines and fast connections to users, for personal use to dump on the internet a dash app is a hefty thing to host."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#building-designing-teaching-and-training-simulation-environments-for-machine-learning",
    "href": "posts/writing-about-code/PyConAu2019.html#building-designing-teaching-and-training-simulation-environments-for-machine-learning",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Imagine you’re building a fancy robot-driven warehouse. Your pick, place, and packing robots need to get around quickly, find the right item and put it to the right place without colliding with each other, shelves, or people. But you don’t have any robots yet, and you need to start. Try simulations! #\n\n\nParis Buttfield-Addison, Phd in CS, professional game dev and ml fan\nUnity isn’t open source but is free\nUnity Machine Learning Agents Toolkit (ML-Agents) - use python to connect to unity for using it as a training environment\nblog post explaining the talk is up at his website\nunity can be programmed using blocks, its very friendly, so don’t get scared\n\ntakeaway: Unity is easy to use, try it out with python!"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#forecasting-australias-2019-election-with-pymc3",
    "href": "posts/writing-about-code/PyConAu2019.html#forecasting-australias-2019-election-with-pymc3",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Can we predict the result of an Australian election before it occurs? How certain of the outcome can we be? My talk will use the 2019 Australian federal election as a case study to provide an entry-level introduction to the benefits of probabilistic forecasting and PyMC3. #\n\n\nMartin Burgess, github buckleysandnone\nnotebook for talk, includes slides and code\nprob forecasting tries to estimate te relative prob of all possible outcomes e.g weather forecasting\nthe future is uncertain, and prob forecasting gives us an estimate of how likely different outcomes are\nppl have different starting assumptions, probabilistic forecasting defines them up front\nPyMC3\n\ntakeaway: forecasts with probabilities are much more useful than just a plain number. With the last Australian elections, practically every channel had just the one number for wins - but if they had attaced a likeihood to it and shown the other likely scenarios it would have been a lot more useful.\nApply probabilistic forecasting to a future issue. Read this book and ThinkBayes."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#cudf-rapids-gpu-accelerated-dataframe-library",
    "href": "posts/writing-about-code/PyConAu2019.html#cudf-rapids-gpu-accelerated-dataframe-library",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "RAPIDS open-source software enables end-to-end data science and analytics pipelines to run entirely on GPUs. Key to RAPIDS is cuDF, a pandas-like Python data frame library with a high-performance CUDA C++ implementation. cuDF and RAPIDS enable large speedups for end-to-end data science using GPUs. #\n\n\nMark Harris, engr at rapids.\nmoving and transforming data is slow\ndata processing evolution for query, ETL and ML train steps:\n\nhadoop would read and write data to disk at every step\napache spark kept data in memory all the time\ntraditional gpu processing would read data into gp, do something then write to cpu. a lot of gpu speed gains were lost in the read write steps\napache arrow provides a common in memory data format so different tools can use the same format and save the data conversion steps\nrapids uses arrow to skip many of the data conversion steps, much faster\n\nrapids aims to accelerate existing python tools, like the pydata chain\nthe average data scientist spends 90% of their time in ETL\nenter cdDF: rapids dataframes to save the day by drastically speeding up ETL\n\nruns on libcuDF, a low level CUDA C++ lib which does all the work on the GPU\ncuDF provides a pandas like api, creates gpu dataframes from numpy arrays, pandas dataframes or pyarrow tables\nbridges python (a dynamic language) with C++ & Cuda (static languages)\ngpu accelerated i/o - 10x faster than pandas\n\nrapids speeds up workflows on a single pc with one gpu - for bigger data it works with dask for distributed computing\ncuML algorithms are largely compatible with sklearn, just much faster\n\n1 v100 gpu is 5-100x faster than 2x20 core cpu.\na regular desktop gpu is plenty fast too for a normal user\n\nRapids works in Google Colab, see getting started.\n\ntakeaway: should drastically speed up many of the bigger datasets I’ve tried in pandas, where a clean and transform pipeline would take many minutes. cuDF promises to at drastically speed up this process, making it easier to iterate faster.\nSo look into getting a Nvidia powered computer suitable for using with rapids, this would have drastically sped up some of my machine learning projects."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#understanding-gpus",
    "href": "posts/writing-about-code/PyConAu2019.html#understanding-gpus",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "With torch and tensorflow we have begun to rely on GPUs to speed up computations. Deep Learning or not, GPUs can provide massive computation speed ups but it’s not a panacea as NVIDIA would have you believe. Understanding how GPUs work can tell us where we should and shouldn’t use them. #\n\n\nVarun Nayyar, mathematician - linkedin\nwhat are GPU’s good for? graphics can be fundamentally reduced to matrix operations, DL is just matrix multiplication with a non linearity.\nCUDA: gpu’s are made up of streaming multiprocessors (SM’s) each with many cuda cores, say 64-128. They run thousands of threads simultanesouly vs 10s for a cpu.\nreal world cuda threads are divided into blocks of 32 threads called a warp, and threads in a warp run at the same time.\n\na rtx2080ti can run over 4K threads in one go\n\nGPU’s are generally memory bound, not compute bound.\nDeep learning: each forward pass is just a matrix multiply, as is backprop\nGradient Descent is a sequential algorithim - gpu compute hasn’t changed how it works, just makes it faster by 20-30x on a fully connected network.\nConvolutions are compute bound on a gpu, easy to parralize\nRNN’s are memory bound\ngpu’s work well with GradientBoosting\ngpu’s have variable perf depending on the algo, so its not straigtforward to put algorithms on the gpu. CPU and GPU implementations can differ.\ngpu sync is slow - stick with single gpu’s for personal use, you need a strong engineering team for multi-gpus.\nlocal compute is great - pays for itself soon over paying for cloud. don’t skimp - go for a rtx2080ti\n\ntakeaways: very impressive talk. Worth rewatching if doing something with GPU’s. For my next ML project consider getting my own gpu box running linux and look at libraries like rapids."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#not-a-long-time-ago-in-a-galaxy-not-very-far-away-an-astronomer-and-a-computer-scientist-walk-into-a-bar",
    "href": "posts/writing-about-code/PyConAu2019.html#not-a-long-time-ago-in-a-galaxy-not-very-far-away-an-astronomer-and-a-computer-scientist-walk-into-a-bar",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Python is one of the most popular programming languages in astronomy. In this talk, I will tell a story about how Python helped me to develop a software tool for galaxy modelling, and tackle the scientific and technical challenges that arise in the Big Data era of astronomy. #\n\n\nGeorgios Bekiaris, astrocoder, makes software for astronomy\nwrote gbkfit - tool for galaxy kinematic modelling\n\ninitially in c++, rewrote twice, finally rewrote in python\n\ngalaxy kinematics refers to the motions of stars, clouds of gas\ndoppler effect: approaching: higher freq, receding: lower freq\nkinematics combines spectroscopy with doppler effects\nsee talk…\ncommonly used libs: Astropy, scipy and matplotlib.\nnumpy code ran as fast or faster as c++ on a single cpu. numba makes it easy to speed up python code by applying a numba decorator to a function. Also supports gpus.\n\ntakeaway: Astronomers are using python for everything, and python can be fast with the right tools."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#learn-to-control-your-brain-brain-computer-interfacing-with-python",
    "href": "posts/writing-about-code/PyConAu2019.html#learn-to-control-your-brain-brain-computer-interfacing-with-python",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Neurofeedback is a brain-computer interface where a person’s own brain waves are audio/visually presented back in real-time after they’ve been recorded and filtered within a few milliseconds. We present methods to allow people to see their own brainwaves with python. #\n\n\nJohan van der Meer, neuroscientist at the QIMR Berghofer Medical Research Institute in Australia. github\nbasics: grab signals from the brainusing EEG, do real time analysis, send a feedback signal\nbrain has many neurons, when a neuron fires a electrical field is generated which is pretty tiny - but when many neurons fire together it makes a big field which you can measure from even outside the brain\nsignal changes with action - exampls eyes open/close.\neeg resolution is quite poor/limited\nEEG signals can have rythyms, so thus you can do frequency analysis\nmne - open source python lib for neurophysical data\nfor better signals - you have to measure from inside the brain - see neuralink.\neeg is non-invasive, portable and cheap\nlabstreaminglayer is a good library to stream realtime data from a measuring device.\nso we have devices, we have libararies to connect to them and python to glue it all up.\nDecoding all this data: this is where the research is, figuring out what it means and using it do useful things, like control a wheelchair etc.\nneurofeedback learning: with some mental disorders ppl have irregular brain rythyms. Idea is that regularzing them with “training” will help improve disorders.\nEncoding: how much data can we transmit?\nopenbci looks really cool.\n\ntakeaway: lots of interesting work being done. Interesting thing is that the neural data is pretty straightforward, no wonder teenagers are building brain controlled prosthetic arms these days!\nExplore openbci further and hack my own neuro thingamajig."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#machine-learning-and-cyber-security-detecting-malicious-urls-in-the-haystack",
    "href": "posts/writing-about-code/PyConAu2019.html#machine-learning-and-cyber-security-detecting-malicious-urls-in-the-haystack",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Today, security teams are in an increasingly one-sided battle to defend against a myriad of cyber attacks. Web-based attacks are often devastating, with conventional blacklists and reputation-based defence tactics not able to identify previously unseen malicious URLs. Is AI the solution? #\n\n\ndesign thinking: put yourself into the shoes of your user at the start of a project\n\nKnow your user, understand their pain points and what they do\nNail the problem\nIdeas\nKnow the threat\npython - data, engr features, model/eval\n\ndid a lot of feature engineering on the urls, things like getting whois data, domain expiry dates, location info, etc etc.\nthey used fast.ai for deep learning, sklearn for randomforests and tensorflow for word embeddings\nused F1 score to evaluate models\n\ntakeaway: see the presentation again… it was a good explaination of a end to end process."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#creating-lasting-change",
    "href": "posts/writing-about-code/PyConAu2019.html#creating-lasting-change",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "The Nature of Organisations, People, and how to Change Them #\n\n\nAurynn Shaw @aurynn, devops at Eiora\ntalk is about introducing change\nspoke about Contempt culture in the tech world\n\nwhat has pervasive hostility done in tech jobs?\n\nall new tech exists in a business context, and biz don’t care about technologies. Devs care about tech, not businesses.\n\nbiz cares about outcomes - can i deploy it, can i find ppl to work on it and the other things biz needs to care about\n\nexpressing opinions in contemptuous terms gets you sidelined and not trusted\n\nyou diss one tech, like another tech - makes you look biased\nwe have done this to ourselves.\n\nif they think its necessary and we don’t or vice versa its not them at fault there is a communications gap.\nand thus we end up in a process which doesn’t believe in engineers.\n\nwe should not be speaking only in a language of tech and treat our assumptions as better - we need to speak in the language of biz\n\nprocesses and status quos arise from past biz experiences\nthe cloud is where infrastructure as code become real - 13 yrs ago. Some companies are still having conversations about entering the cloud. Cause they don’t care about the tech and ease of the cloud, they care about risks and costs and regulations and so on.. the easy of spinning up services is a small thing.\nsomeone needs to communicate that\npower dynamics dictate change\nlistening is key to change\nnew processes need to make the lives of others betters and show them how its solving problems (and in the process you get to roll out the shiny new tech you care about)\nall technology is political - tech encodes the structures of the org which made it\n\nthings which make no sense often are because of the initial org constraints, and thats why they don’t make sense to a different org with different constraints\nunderstand the political ramifications of tech\n\ngo and talk to ppl. make conversations safe.\n\ngood example of Etsy\n\nlearn to speak business. Be empathetic to the entire org.\n\ntakeaways: Its not just them, its us. Solve business problems and communicate how your solution helps. Don’t spend too much time on we should have done it right in the first place. It just so happens that we adopt new tech as sometimes its better/faster/cheaper than the old."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#lessons-learned-building-python-microservices",
    "href": "posts/writing-about-code/PyConAu2019.html#lessons-learned-building-python-microservices",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "I will talk about challenges and wins that have come from introducing Python into a multilingual microservices kubernetes architecture with lots of legacy. #\n\n\nRichard Jones, dev at reecetech, a plumbing company\n\ntrys to make every service look similar, tools to enforce this\n\ncookiecutter\ntox\ngoal is full test coverage, pytest cause its best in class\nblack to format code\npycharm for all devs so ppl can help other ppl easily without editor shock and for ease of pair programming\npactman for contract testing of services\ndjango to deliver the services\n\ninconsistencies happen, so you have to check across teams. i.e one team switched tool cause they found it too hard, so now you have inconsistencies across projects\nresilience: microservices are prone to brief errors or tiny service interruptions. So use http retries for some errors\nmoniter services using grafana dashboards and kibana for monitering and searching logs\nget it running right first, then use tools to investigate performance, like silk for dango\n\nreducing number of sql queries was key to speed\n\nbatch interfaces is good, like allow consumer to get 100 price requests in one go, much faster than 100 different requests\ncaching really helped. @lru_cache(maxsize=1024) decorator built into python does the job.\n\ntakeaway: make things simpler by taking away choices by using automated tools. Test and monitor services."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#tunnel-snakes-rule-bringing-the-many-worlds-of-python-together-to-monitor-melbournes-biggest-infrastructure-project.",
    "href": "posts/writing-about-code/PyConAu2019.html#tunnel-snakes-rule-bringing-the-many-worlds-of-python-together-to-monitor-melbournes-biggest-infrastructure-project.",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Python is being used to provide real-time environmental monitoring on the Melbourne Metro Tunnel project. Come along to see how open source Python tools from the web, IoT, cloud infrastructure and scientific domains are being used together to monitor environmental telemetry on a city-wide scale. #\n\n\nEvan Brumley, engr at WSP Digital\nworking on 7 construction sites for Melbourne Metro\none construction site next to hospitals, lab etc which can’t be disturbed - so they have a bunch of quantative requirements to meet and report on in real time.\nso, how to keep track of all the requirements?\nold school approach was to send grad engineers to site, collect readings, analyze in excel and file reports at the end of the month\nmodern approach: get sensors from a Vendor with a Saas platform, give them lots of money, download csvs, analyzie in excel and file reports at the end of the month.\n\nthey don’t respond to custom reports, don’t integrate with other vendors sensors\n\nsolution: build a new platform which could accept data from any device\nhint: try not to work with devices directly\n150-200 sensors, some sending data at a half second freq\nvalidate and store telemetry\nenvirnomental requirements don’t map directly to sensor data, so calculations needed to transform them - which are sometimes complex, and have to performed in real time\naccess to data - both internal as well as limited external access\nalerts and reporting\nthey had 4 months to build this out, fully self contained team of 2-3 devs + 1PM\nAWS Kinesis to store streaming data, S3 for resilient storage, then influxdb.\napi pollers packaged into docker containers and deployed via elastic beanstalk\n\nused pyftpdlib\n\nweb app is built on django+celery+react, powered by pandas and the scipy stack\n\npandas allows them to transform raw telemetrym live, on request - using upto 10K points in a dataframe. Pandas was a huge timesaver.\n\n\ntakeaway: dang. that is a lot of stuff."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#using-python-programmed-microcontrollers-to-build-comedy-props",
    "href": "posts/writing-about-code/PyConAu2019.html#using-python-programmed-microcontrollers-to-build-comedy-props",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Early-career comedians often have difficulties adding electronic props to their acts, due to the high cost of materials and fabrication skills required. This talk will recreate several props used in comedic performances, showing the code and components used. #\n\n\nAnthony Joseph & Debbie Zukerman\nused adafruit wearable devices, circuit playground, microbit, arduino\nqlab\n\ntakeaway: explore micropython and get a device to play with."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#python-applications-in-infrastructure-planning-and-civil-engineering",
    "href": "posts/writing-about-code/PyConAu2019.html#python-applications-in-infrastructure-planning-and-civil-engineering",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Engineers tasked with planning new infrastructure constantly face the problem of having to look through too much information. This talk is about how we wanted to be lazy and wrote a bot to do it for us instead. #\n\n\nBen Chu, grad engr at WSP, a large engnr firm\nlots of stuff involved in railway planning\nEIA used to be done by junior staff, using datasets like noise receivers (hospitals etc), vegetation (what areas will the train cross)\n\nthese add up quickly to a lot of stuff\n\nso instead they are using jupyter notebook\n\nshapefiles store shapes like the railway line and vegetation and noise senstitive areas\nrun in [Papermill](https://github.com/nteract/papermill which outputs html, csv and shapefiles\n\nneed to check DA’s along the railway line which might impact on the line - there can be many thousands.\n\nused to be done manually, paid $70 per DA, slow, costly and infrequent updates\nthey need to know as soon as the DA comes in as it could pose a high risk to the railway design\n\nDABot automates this - it merges geocoded national address file and lot shape datasets, filters for lots on the buffer alighnment, which gives a list of addressses which they can use to search for DA’s matching those addresses.\n\nmajority of council websites are the same, so same scraper works on most of them\nNLP pipeline to clean text, then they form a document term matrix, used 200 most imp features\nML: went with XGBoost, got 85% Accuracy with a 95% recall\n\nmaximized recall as they didn’t want to miss high risk DA’s\n\n\nDABot allowed for frequent updates, saved time/money\nFuture: lots of improvements to do, from using word embeddings, deep learning,\n\nPostGIS database\nfully automated pipeline so they can send out automated weekly reports\n\n\ntakeaway: Impressive how a very simple NLP pipeline and application of XGBoost gave them such good usable results. They didn’t even use word embeddings! There is a lesson in this. Build simple, improve later."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#fantastic-blocks-and-where-to-hide-them",
    "href": "posts/writing-about-code/PyConAu2019.html#fantastic-blocks-and-where-to-hide-them",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Ruby has blocks. JavaScript has blocks. Swift has blocks. Python doesn’t have blocks.In this talk, we’ll look at why Python doesn’t have blocks, and recent programming techniques that have developed in languages that do have blocks. Then we’ll look at what we – or Python – can do about it! #\n\n\nChristopher Neugebauer, @chrisjrn, snr engr at AlphaSights and a director of PSF\nblocks was last discussed in 2005 for Python, and is unlikely to appear\nusing Kotlin at AlphaSights - Kotlin encourages passing blocks of code around\nPython isn’t really for functional programming, instead they have list comprehension\nBlocks: in python you can have a func (many lines) or a lambda (one line long)\n\nso you always need to define a func and pass that. In other languages you can easily pass a block of code\n\nContext Managers: when opening files, you had to manually close them. Easy to miss. Hence context managers.\n\nthe python syntax enforced correct behaviour\n\n\ntakeaway: things to thing about. Convention matters."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#profiling-pathogens-with-micro-python",
    "href": "posts/writing-about-code/PyConAu2019.html#profiling-pathogens-with-micro-python",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "We’re building professional medical diagnostics equipment with micropython. This has come with minimal challenges, many positives and a few surprises! #\n\n\nAndrew Leech, Planet Innovation\nworking with Lumos to develop point of care diagnositcs tests (spinoff of PI)\nBuid a medical device:\n\nhazard and risk based development: at every stage minimise risks\n\nLumos Camera Reader runs micropython, connects to a phone via bluetooth\nSOuP: Software of unknown povidence\n\nfor medical devices, you need certified code, or blackbox it.\n\nuse jupytermicropython kernel to run code directly on the board. Makes dev very easy.\n\nuse ipywidgets to do live interactions.\n\nuse test driven development - the same unittest can run on the board, desktop and CI\nmedical devices aren’t that different from regular devices, they just need to blackbox all the uncertified bits.\n\ntakeaway: use the jupyter kernel for micropython for live dev on a board."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#the-antipodes",
    "href": "posts/writing-about-code/PyConAu2019.html#the-antipodes",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "@brandonrhodes, #\ngreat spkr, watch his technical talks as well\nturn me me me me you into you you you you me - works great in life for things from emails to conversations\n\ntakeaway: watch more talks by great talkers. Put the important stuff first, both in programming and emails."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#saturday-lightning-talks",
    "href": "posts/writing-about-code/PyConAu2019.html#saturday-lightning-talks",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "using processing.py to build worlds and model weather using isca\ndeep deep rabbit whole of modelling and fitting worlds into the solar system\n\n\n\n\n\n@veronica_hanus\nthink about inline comments as documentation\n\nautogen docs from inline comments\n\ncomments can be magic\n\n\n\n\n\n@datanerdery\nlogging is good! built into the stdlib\n\nmultiple levels of details"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#the-real-costs-of-open-source-sustainability",
    "href": "posts/writing-about-code/PyConAu2019.html#the-real-costs-of-open-source-sustainability",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "What if money isn’t the only way to create sustainable free and open source software projects? What if it turns out that sustainability is actually a multi-faceted concept that can’t truly be successful if people focus on only one of its many elements? #\n\n\nVM Brasseu @vmbrasseur\n\ntakeaway: code of conducts matter, look for, abide by and try to contribute to open source projects which have them."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#shipping-your-first-python-package-and-automating-future-publishing",
    "href": "posts/writing-about-code/PyConAu2019.html#shipping-your-first-python-package-and-automating-future-publishing",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "One of the best things about Python is the vast ecosystem of packages available on the Python Package Index. Shipping your first Python package can be intimidating. This talk aims to remove the mystery of Python packaging and enable you to share your code with the Python community. #\n\n\nChris Wilcox, dev at Google @chriswilcox47\npypi: python package index makes python great - its easy to install and use packages\nmanual steps: (don’t do this)\n\nmake a packakge in the format pip expects along with a setup.py\nmake a venv and do a local install of your package and test it.\nupload to testpypi, and install from there\nif everything works, upload to pypi\n\nuse setup.cfg, specify all the metadata there.\n\nbunch of classes in classifiers you can assign your package to\nAlways add a licence.\nyou can add long_description = file:README.md which can be just the readme in github.\nyou can define things like packages required\n\nauthentication: pypi supports auth tokens, can make one per package and assign it permissions like upload package\nto start a new project\n\npypi has a sampleproject repo\ncan use cookiecutter too, or just build up a good basic setup and just copy paste and change for new projects -automation tools:\ntox is the most common\nnox is more flexible, configured with python scripts. can build docs as well.\n\nreally automate stuff with CI - once setup properly publishing a release to the git repo will build, test and deploy:\n\ncircleci/config.yml to define the build and deploy sequence.\ncircle ci checkouts out your repo into a docker image, installs the things required, runs tests, builds, and publishes the new version\n\n\ntakeaway: publish my first package. use nox and circleci to automate all the things."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#insights-into-social-media-data-using-entropy-theory",
    "href": "posts/writing-about-code/PyConAu2019.html#insights-into-social-media-data-using-entropy-theory",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Entropy theory is usually thought of as something that applies to matter and energy, but it turns out that we can apply the same techniques of analysis to social media sites. Join me as we study the thermodynamic behaviour of users on Twitter, and learn how to analyse it better. #\n\n\nMars Geldard github, honours student at U of Tas\nawesome talk, see preso slides here.\nInteresting application of entropy theory to social media analysis.\n\ntakeaway: Look at other fields for ideas on how to tackle problems."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#its-dark-and-my-lights-arent-working-an-asyncio-success-story",
    "href": "posts/writing-about-code/PyConAu2019.html#its-dark-and-my-lights-arent-working-an-asyncio-success-story",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "I have invested huge amounts of time in achieving a simple goal – making the lighting in my home “smart”. It’s not ground breaking, nor is it practical or cost effective, but it sure was educational, uses a bunch of Python, and the result makes me (and my family) happy. #\n\n\nJim Mussared @jim_mussared github\nrenovated his house in 2015 wanted smart everything\nused zigbee enabled bulbs\ndigiXBee board runs micropython\n\ntakeaway: home automation is a deep deep rabbit hole to jump into. This guy should get a Medal for Bravery above and beyond the call of duty from Zigbee and Samsung smartthings."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#the-universe-as-balls-and-springs-molecular-dynamics-in-python",
    "href": "posts/writing-about-code/PyConAu2019.html#the-universe-as-balls-and-springs-molecular-dynamics-in-python",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Surprisingly, we can approximate matter as a bunch of balls on springs and learn things about our bodies and the world. This talk will look at the different stages of molecular dynamics (MD) simulations and how Python is changing everything. #\n\n\nLily Wang, working on PhD on molecular dynamics at ANU\ndefine an atom as a ball (radiaus, mass) with everything else as springs then you can use molecular dynamics to model a system of atoms. To study proteins, viruses etc\nopenmm lib for molecular simulations, also MDanalysis\nplotly for interactive viz for the win!\nMD in the past: rubber balls stuck together\nnow: simulating viruses with millions of atoms using supercomputers\n\ncurrently sims top out at millions of atoms, so long way to go to simulate bigger structures\n\nOpenForceField: open science initiative\n\ntakeaway: molecular dynamics is hard. I liked how they simplified a very complex thing - proteins, viruses - to a simple system which they could model computationaly and hope it approximates the real thing."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#instant-feedback-instant-debugging-python-coding",
    "href": "posts/writing-about-code/PyConAu2019.html#instant-feedback-instant-debugging-python-coding",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Building on Bret Victor’s famous ‘Inventing on Principle’ presentation, we look at writing Python where the code is instantly run and every line visualized after every single keystroke. There’s a future beyond the text-editor -&gt; console-run loop and this is a taste of it. #\n\n\nRobert Lechte @djrobstep web\npython demo: code on left, instant output on the right\nthings take a while a change. new forms of media like newspapers enabled by the printing press took ages to appear\nsame with computers\ndevs spend too much time on understanding code vs understanding the problem\nwatch:\n\nGary Bernhart “Whole new world” talk on reinventing the terminal\nBret Victor “Inventing on Principle”\n\nsegway example showing live reaction of a simulated segway to programming is awesome\n\ntakeaway: instant feedback is genius. I want a instant feedback python IDE. Actually strip the ide. Just a simple two pane interface, one showing code, other the results. Someone pls build a vscode or jupyter extension for this. I mean ppl are sending keystrokes to cloud AI’s to predict smart suggestions, just running the code must be easier than that!"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#orchestrating-complex-not-complicated-tasks-using-aws-serverless-and-python",
    "href": "posts/writing-about-code/PyConAu2019.html#orchestrating-complex-not-complicated-tasks-using-aws-serverless-and-python",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Python and serverless technologies are a great way to quickly scale a project, but what can you do when things get complicated? Here are some patterns to keep you sane. #\n\n\nMichael Kelly, web, github from versent\nlambda functions\n\nadv: cheaper, little config, encourages refactoring, easy learning curve\nlimitations: runtime limits\n\nuse aws step functions to build a state machine to connect different services together, trigger lambads and so on\n\nstate machines hold state unlike lambdas\n\nsee talk for project template example\nuse black for code linting.\n\ntakeaway: instead of making overly complex lambda functions, model the problem as a finite state machine, make simple lambdas as appropriate then make a state machine which embeds some of the logic and calls lambdas.\nThen you can make a state machine to watch over all your other state machines!"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#sunday-lightning-talks",
    "href": "posts/writing-about-code/PyConAu2019.html#sunday-lightning-talks",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "pototoes were far away from NZ. got there long time ago.\ngrowstuff is tinder for potaoes.\n\n\n\n\n\ncool new thing\nbasics of QM: instead of bits, you have qbits which have a vector instead of being a 0 or 1.\nyou can access quantum computers free online\nsee pennylane\n\n\n\n\n\ntalk about fear of removing old and semi dead bits of the stdlib\npep594 will break a lot of corporate code which doesn’t have replacements - this places the burden of maintainence from the corporations to python\nsome of the libs broken can be easily updated\nmake pythons development easier, kill the nasty old bits"
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#mega-takeaway",
    "href": "posts/writing-about-code/PyConAu2019.html#mega-takeaway",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Its nice to hear talks in real life instead of on youtube. Focuses the attention.\nMoving forward, my biggest takeaway is to take an idea and just run with it. Many of the speakers had an itch - a home to automate with python, a paper to write or drones to fly and just did it. So take one of my project ideas and just do it, using circleci of course."
  },
  {
    "objectID": "posts/writing-about-code/PyConAu2019.html#misc-notes",
    "href": "posts/writing-about-code/PyConAu2019.html#misc-notes",
    "title": "PyCon Australia 2019",
    "section": "",
    "text": "Embed youtube vidoes by {{&lt; youtube EqQj5os3Mfw &gt;}} or {{&lt; youtube id=\"w7Ft2ymGmfc\" autoplay=\"false\" &gt;}}\ncan’t use ' or - in anchor links for some reason. Chrome doesn’t jump to anchors! Bad chrome!"
  },
  {
    "objectID": "posts/writing-about-code/sml-meetup.html",
    "href": "posts/writing-about-code/sml-meetup.html",
    "title": "SML 2018-08: Deep Reinforcement Learning",
    "section": "",
    "text": "SML 2018-08: Deep Reinforcement Learning\nSML’s 2018-08 meetup.\n\nlaunching StarAI Deep Reinforcement Learning Course\n\n7 week course to take you from RL near-zero to hero\n10-15hrs study a week, meet 6-8pm once a week\n\n\nthe talks:\n\n\nAlasdair Hamilton: Reinforcement learning\n\nThe future of Artificial Intelligence is, at least partly, rooted in Reinforcement Learning. During our presentation, the team from Remi AI will take attendees on a journey through their experiences in Reinforcement Learning. Special attention will be paid to the areas in which the team have been able to apply Reinforcement Learning in a commercial setting and the four mammalian methods of learning, and how they inspire us. Remi AI has applied RL to budget and bid management, to dynamic pricing and web design, to large scale management of power requirements, inventory management, predictive maintenance. The talk will conclude with a discussion on future applications, as well as a roadmap for those looking to start out in the space, then Q&A.\n\n\nfounder of Remi AI, one of Sydney’s only RL companies.\nRemi AI was founded 5 yrs ago, works on applied AI for companies/biz/ngo’s.\nRL is a form of AI that includes motivating the agent toward some goal.\ncurrent RL agents are incredibly dumb - even though they excel at certain tasks.\nWhy RL:\n\nits been a very long time since humans invented tools - RL is the framework which enables an artificial agent to invent more tools to solve a problem\nRL is the framework which humans use internally to learn\nRL covers a lot of domains, from engineering to pyschology\nRL is modelled around the dopamine reward delivering system in animals/humans\n\nsome real world RL applications: stunt flight, traffic optimisation, Go (see Deepmind), dynamic pricing\nthe RL agent can change its environment with its own actions. there is no supervisor, ony a reward mechanism.\nunlike supervised methods, feedbck is often delayed, so time is extremely important. e.g in a traffic optimsation problem traffic jams can arise a long time after a decision.\nDemoed of a RL agent learning breakout, a remote control helicopter and space invadors.\nreward hacking - designers can set wrong or low rewards and punishments - e.g if the punishment for crashing is to high, the best stratgey is to never take off.\nRL in robotics is changing the field - what took decades to learn incrementally, RL systems are learning insanely quickly.\nrewarding an agent: a reward is a scalar feedback signal - we dictate through rewards how well the agent is doing at step t. the best systems combine prediction and reward.\ngames are a very useful way to researh RL, as you have clear scores which can be tied to rewards and punishments.\nRL is a lot harder in the real world in terms of choosing rewards and punishement.\nimitating human reward systems can be both useful and incredibly dangerous.\nthere is quite a range of RL algos.\nRL needs a huge amount of training time - agents initially know nothing so there is a lot of trial and error - ideally in simulation since doing this in the real world is very expensive\na lot of RL is building a simulator to train an agent, then taking it to the real world.\ntodo: research transfer learning in RL.\nresearch happening in hierarchical learning - break up things so learn individual goals which build to the final goal.\nSome real world applications of RL:\n\na few cities have implemented deep RL algos in traffic optimisation. RL is also useful for devising plans on how to handle breakdowns, like a road out of commision.\nenergy reduction - Google’s data center\nsupply chain optimisation - warehousing, logistic.\n\nRL models are simulation agnostic, but transition from simulation to the real world can be extremely troublesome. So:\n\ncontinually review simulators.\nwhere are the reward predictions going wrong?\n\n\nq & a\n\nHow do you reward differnt/multiple objectives?\n\nfor a wide range of objectives, scale up rewards at different levels - example reward individual contract signings and also bigger level revenue goals.\n\nis RL limited by the complexity of the simulation?\n\nsome things can be simulated by very simple things like graphs written in python - sims don’t have to be complex and visual.\n\narbitary rewards can cause problems - whats a good fix?\n\nits a hot area - ppl are figuring out best utility functions for rewards.\n\n\n\n\nAlex Long: Deep Reinforcement Learning: Zero to PPO in 20 minutes\n\nDeep Reinforcement Learning (Deep-RL) is the combination of traditional RL algorithms with the high-dimensional function approximation methods of deep learning. This combination allows Deep-RL to eclipse human performance on systems of previously intractable state-spaces and high branching factors, such as the game of GO, Atari arcade games, and heads up limit poker. In this talk I will focus on the intuition behind Deep-RL, how it compares (and differs) to other machine learning methods, as well as discuss some potential commercial applications.\n\n\ngoal is to wow the audience with how general Proximal Policy Optimization (PPO) is.\nOpenAI used PPO to learn DOTA and beat professionals, exact same algo can be used to learn/do totally something else.\ncontext: where does RL fit?\ngeneral RL: I’m in situation X and my overall goal is y. What do I need to do?\nRL is very general, fits with and takes bits of all parts of AI.\nsee ben recht’s blog on RL.\na way to think of any move choosing machine is as a tuneable black box which looks at the state of the world and spits out a probablity for all possible moves.\nRL is adjusting this move choosing machine so that the likelihood of good moves is higher and bad moves lower. Thats it. Of course, you need to do this many times over.\none move choosing machines can be a NN, which are nice since they let us generalize inputs we haven’t seen before by learning the underlying data distribution.\nthe simplest way of recording moves is just making a table of moves and a score, but this doesn’t help us with moves we haven’t seen. A naive answer is to pick the closest neighbours, or interpolate linearly. Neural networks allow us to approximate the underlying distribution, which allows us to reason about states we haven’t seen before.\nhow do we train our NN to approximate a game globally when we are using the NN itself to collect the data we are using for training?\n\nbe extremely careful about how we explore the space\nbe extremely carefeul about how we update our NN\n\nPolicy gradients gives us a safe way to update the NN, but have lots of issues\nProximal Policy Optimization uses a surrogate loss.\nkey diff: using the ratio of probabilites b/w old and new policies and limit the amount this can be updated. This bounds policy updates and introduces stability.\nthe engineering behind RL algos is impresisve and important - the algorithims can be straightforward but see all the engineering/computing resources OpenAI is throwing at their RL algos playing computer games like DOTA.\n\nq & a\n\nproblem of local minima?\n\nin a multi-dimensional world, the local minima generally disappear, as there are so many directions to move. It does exist, but in a different way from SL.\nthere is a problem of getting stuck in a bad policy space so never learning ‘good’ moves.\n\nwhat about overfitting?\n\nwe can’t overfit - there is no test set. We’re doing RL in an environment where we want it to win the mostest.\n\nhow many policies do u keep track of?\n\nonly two - current and one step ahead\n\nis there a q value in PPO?\n\nin policy gradient methods you don’t have a q\n\ncatostrophic policy updates - how often do u have to abondon?\n\nvanilla policy gradients, have to restart all the time, then tune hyperparameters.\nPPO almost never, it generally figures things out.\n\nhow efficient is RL?\n\nRL uses huge amounts of compute but is progressing very fast, in orders of magnitude. At the moment its prohibitvely expensive for normal ppl to train things like DOTA bots but it keeps getting computationaly cheaper as algos improve."
  },
  {
    "objectID": "posts/recipes/omelette.html",
    "href": "posts/recipes/omelette.html",
    "title": "Desi Omelette",
    "section": "",
    "text": "I love cheese, but it has no place in a true desi omelette."
  },
  {
    "objectID": "posts/recipes/omelette.html#ingredients",
    "href": "posts/recipes/omelette.html#ingredients",
    "title": "Desi Omelette",
    "section": "Ingredients",
    "text": "Ingredients\n\n2 desi eggs (organic, free range)\ntomato, diced\ncoriander, diced\nonion, diced\nchilli, diced"
  },
  {
    "objectID": "posts/recipes/omelette.html#spice",
    "href": "posts/recipes/omelette.html#spice",
    "title": "Desi Omelette",
    "section": "Spice",
    "text": "Spice\n\n1/4 tsp desi chilli (or thai)\n1/4 tsp salt\npinch of tumeric/haldi (optional)"
  },
  {
    "objectID": "posts/recipes/omelette.html#directions",
    "href": "posts/recipes/omelette.html#directions",
    "title": "Desi Omelette",
    "section": "Directions",
    "text": "Directions\n\nbreak eggs into a bowl, whisk, add all the veggies as you see fit\nadd spices\nwhisk a bit\nfry half the mixture on a flat pan, flip and don’t fold over\n\nServe with toast or paratha.\nNote: If you diced a whole onion/coriander/tomato, that will do for 2-3 serves. Obviously you need at least a chilli per serve.\nNote: this recipe is as good as it gets, don’t add no grama masala or anything."
  },
  {
    "objectID": "posts/recipes/omelette.html#variations",
    "href": "posts/recipes/omelette.html#variations",
    "title": "Desi Omelette",
    "section": "Variations",
    "text": "Variations\n\nhttps://www.theguardian.com/food/2020/feb/29/meera-sodha-vegan-tarka-dal-recipe-maham-anjum"
  },
  {
    "objectID": "posts/recipes/aloo-keema.html",
    "href": "posts/recipes/aloo-keema.html",
    "title": "Aloo Keema",
    "section": "",
    "text": "Recipe in progress in an attempt to recreate a vegan version of homestyle aloo keema."
  },
  {
    "objectID": "posts/recipes/aloo-keema.html#ingredients",
    "href": "posts/recipes/aloo-keema.html#ingredients",
    "title": "Aloo Keema",
    "section": "Ingredients",
    "text": "Ingredients\n\n350 gms vegan mince, pea protein based\n2 tomatoes diced\n1 onion sliced and cut a bit more\n1-2 green chilis, chopped\n1 tsp garlic crushed\n1 tsp ginger minced\n1/2 tsp tamarind paste\ncoriander, half bunch\n1-2 potatoes, cubed small and boiled for 5 minutes\n1 tsp zeera (cumin seeds) - make this 1/2 tsp I think\n\n\nSpices\nSpices vary a lot in different home style recipes. So long as you have chilli you can’t really go wrong. Vegan mince needs a lot of spice to cover up its lack of flavour compared to actual meat.\n\n1 tsp salt\n1/4 tsp haldi (tumeric)\n1/4 tsp cumin powder\n1/4 tsp coriander powder\n1/2 tsp chilli powder\n1/2 tsp garam masala (optional)\nground black pepper, just a bit"
  },
  {
    "objectID": "posts/recipes/aloo-keema.html#steps",
    "href": "posts/recipes/aloo-keema.html#steps",
    "title": "Aloo Keema",
    "section": "Steps",
    "text": "Steps\n\nheat oil to medium, fry onions for 5-6 minutes, lower heat\nadd in cumin seeds, fry for 1/2 minute\nAdd chilli, fry 1-2 minutes\nAdd garlic, ginger and tamarind paste, fry for a 1 minute\nadd spices, stir\nAdd frozen mince, it takes a few minutes to settle down\nadd potatoes and fry 2-3 minutes\nAdd tomotoes, stir through for 2 minutes\nadd 1/2 cup water then put a lid on, lower heat all the way. 5-8 minutes\nremove lid, stir for 2-3 minutes to get rid of remaining water\ncheck if potatoes soft enough to eat, and if so, add coriander, stir through and servce\n\nOptional: Add chopped fresh ginger and chilli as garninsh, serve with lemon or lime wedges.\nThis recipe feeds 3ish ppl. Makes good cold sandwhiches the next day,"
  },
  {
    "objectID": "posts/recipes/aloo-keema.html#notes",
    "href": "posts/recipes/aloo-keema.html#notes",
    "title": "Aloo Keema",
    "section": "Notes:",
    "text": "Notes:\n\ntry with 100-200g frozen peas\ngaram masala yeah or nay?\ndon’t use soy or tvp mince, they have a strong taste"
  },
  {
    "objectID": "posts/recipes/paneer-karhai.html",
    "href": "posts/recipes/paneer-karhai.html",
    "title": "Paneer Karhai",
    "section": "",
    "text": "Home made recipe from FDT. This is a basic desi karhai receipe which can be used as the base for anything."
  },
  {
    "objectID": "posts/recipes/paneer-karhai.html#ingredients",
    "href": "posts/recipes/paneer-karhai.html#ingredients",
    "title": "Paneer Karhai",
    "section": "ingredients",
    "text": "ingredients\n\n5 largish tomatoes\nbig piece of ginger - cut into long strips\n4 chillis cut into semi-large pieces (4 green or 2 green and 2 red)\nplain yoghurt\ncoriander, heaps of it (chopped)\noptional: paneer or tofu or vegetables (cooked separately and added at the end)\noptional: boil eggs, slice in half and add towards the end\n\n\nspices\n\nchilli power to taste - just a dash\ncoriander powder\nzeera\nsalt"
  },
  {
    "objectID": "posts/recipes/paneer-karhai.html#steps",
    "href": "posts/recipes/paneer-karhai.html#steps",
    "title": "Paneer Karhai",
    "section": "steps",
    "text": "steps\n\nuse a frying pan with a lid (or a wok) - though lid is not really needed\nheat oil\nadd tomotoes\nadd ginger and chillis right after\nadd spices right after\nstir then put a lid on for it just a bit\nremove lid & add 2 tablespoons of yoghurt\nfry without the lid until it turns back into tomatoe colour (the yoghurt will dissolve) - about ten minutes\nadd in the paneer or tofu or veggies here (stir fry the veggie or tofu first)\ncook about 1-2 minutes more\nsprinkle coriander and fry for 30 seconds\n\nPresto, done. Serve with nan, roti, chawal, whatever.,"
  },
  {
    "objectID": "posts/recipes/daal.html",
    "href": "posts/recipes/daal.html",
    "title": "Daal (lentils)",
    "section": "",
    "text": "A simple daal recipe.\n1 cup of daal is approx 3-4 ppl. (much closer to 4)."
  },
  {
    "objectID": "posts/recipes/daal.html#ingredients",
    "href": "posts/recipes/daal.html#ingredients",
    "title": "Daal (lentils)",
    "section": "Ingredients",
    "text": "Ingredients\n\n2 cups mixed dhal - 1 cup mung and 1 cup masoor\n2 tomatoes chopped\n1 onion, chopped fine\na decent amount of cumin seeds (more than a tsp, less than a tbsp)\nlots of garlic (3-5 cloves), or 2 tsp crushed garlic\na bit of ginger paste\ntwo fresh chillis (omit if kids)\n1 cup frozen peas, defrost for a minute in boiling water and drain\ncoriander, chopped"
  },
  {
    "objectID": "posts/recipes/daal.html#spice",
    "href": "posts/recipes/daal.html#spice",
    "title": "Daal (lentils)",
    "section": "Spice",
    "text": "Spice\n\n1 tsp chilli (use 1/2 tsp if kids)\n2 tsp salt (1 tsp per cup of daal)\npinch of tumeric/haldi\n1/4 tsp garam masala"
  },
  {
    "objectID": "posts/recipes/daal.html#directions",
    "href": "posts/recipes/daal.html#directions",
    "title": "Daal (lentils)",
    "section": "Directions",
    "text": "Directions\n\nrinse lentils a few times, then soak for at least 30 minutes\nput lentils on low heat\nadd onions, tomatoes, garlic and ginger and then all the spices and salt. mix well.\nslice a chilli down the middle and toss it into the dhal\nturn the heat up until close to boiling, then put it on low heat to cook."
  },
  {
    "objectID": "posts/recipes/daal.html#bhaagar",
    "href": "posts/recipes/daal.html#bhaagar",
    "title": "Daal (lentils)",
    "section": "bhaagar",
    "text": "bhaagar\nStart this when the dhal seems to be about 10 minutes from being cooked:\n\nTake a non stick saucepan, and a liberal amount of oil, the base should be coated.\nthrown in cumin seeds into hot oil\nafter 2-3 minutes add the peas, salt\n\nAdd this to the daal once it’s almost fully cooked, stir and leave for a few minutes.\nSprinkle coriander and serve with a side of cucumber."
  },
  {
    "objectID": "posts/recipes/daal.html#tips",
    "href": "posts/recipes/daal.html#tips",
    "title": "Daal (lentils)",
    "section": "Tips:",
    "text": "Tips:\n\nDon’t worry too much about excess water as the veggies soak up a bit once you stir them in at the end, and Pakistani dhal is supposed to be a bit watery.\nThis is for 2 cups of dhal, which should easily feed 4-5 ppl with rice.\nIf the people are big eaters use 3 cups of dhal, proportions don’t change much except for putting in a bit more spice mix"
  },
  {
    "objectID": "posts/recipes/daal.html#variations",
    "href": "posts/recipes/daal.html#variations",
    "title": "Daal (lentils)",
    "section": "Variations",
    "text": "Variations\n\nhttps://www.theguardian.com/food/2020/feb/29/meera-sodha-vegan-tarka-dal-recipe-maham-anjum"
  },
  {
    "objectID": "posts/recipes/khata-bhaigan.html",
    "href": "posts/recipes/khata-bhaigan.html",
    "title": "Khata Bhaigan",
    "section": "",
    "text": "Recipe from Lahore."
  },
  {
    "objectID": "posts/recipes/khata-bhaigan.html#ingredients",
    "href": "posts/recipes/khata-bhaigan.html#ingredients",
    "title": "Khata Bhaigan",
    "section": "Ingredients",
    "text": "Ingredients\n\n5-6 tomatoes\n3 onions\ncoriander\n1 kg bhaigan (eggplants)\n\n\nLemony spicey rub\nIt shouldn’t be too liquidy - Shan Achar Ghost masala - ½ packet per kg - 1 lemon or approx 3 tbsp lemon juice\n\n\nSpices\n\n1 tbsp achar ghost masala\n1 tsp red chilli\n1 tsp salt"
  },
  {
    "objectID": "posts/recipes/khata-bhaigan.html#steps",
    "href": "posts/recipes/khata-bhaigan.html#steps",
    "title": "Khata Bhaigan",
    "section": "Steps",
    "text": "Steps\n\nCut the bhaingain with 4 cuts and apply the spicey lemon.\nHeat a lot of oil and add the bhaingain. Turn over after 5-10 minutes. Once both sides are soft take out and leave in a plate.\nBlend the onion and tomoatoes and add to saucepan\nAdd spices\nFry till oil seperates\nSprinkle half the coriander\nTurn down heat to low, then add the bhaigan one at a time, open it slightly and spoon in some masala. Put a lid and leave it for 5 minutes.\n\nSprinkle some coriander and serve."
  },
  {
    "objectID": "posts/recipes/biryani.html",
    "href": "posts/recipes/biryani.html",
    "title": "Paneer Beetroot Biryani",
    "section": "",
    "text": "A vegetable biryani. Cooked using a round cast iron enamelled casserole."
  },
  {
    "objectID": "posts/recipes/biryani.html#ingredients",
    "href": "posts/recipes/biryani.html#ingredients",
    "title": "Paneer Beetroot Biryani",
    "section": "Ingredients",
    "text": "Ingredients\n\n3 cups basmati rice (500-600g), add 1/2 cup if more ppl - parboiled basmati works great.\n400g sweet potato\n2 medium size beetroots (1.5 if large)\n5-6 tomatoes\nChickpea can 400g, drained and rinsed\n250g indian paneer, cut into cubes\n2 large onions, finely sliced\nCashews, small handful\n\n\nOil for pouring over veggies\nMix together well:\n\n6 tbsp oil\n1.5 tsp salt\n1 1/4 tsp chilli powder\n1 1/4 tsp ground cumin\n2 tsp garam masala\n1.5 lemon juice, freshly squeezed\n\n\n\nCoconut coriander sauce\nMix all this in a glass blender:\n\nCoconut milk 400g, make sure its not a cheap watered down one\nFresh coriander x 2, roughly chopped\n2-3 green finger chilli, or one thai birdseyes and 1 normal\n6 cloves garlic or ~4 tsp crushed garlica3s\n3cm ginger or ~2 tsp crushed ginger\n1 tsp salt\n1.5 lemon juice, freshly squeezed"
  },
  {
    "objectID": "posts/recipes/biryani.html#steps",
    "href": "posts/recipes/biryani.html#steps",
    "title": "Paneer Beetroot Biryani",
    "section": "Steps",
    "text": "Steps\n\nBake them veggies\n\npreheat oven to 200c\nwash sweet potatoes and beetroot, don’t peel, dice and add to a tray\nDice tomatoes and paneer and add to second tray\nMix the oil well and spoon over the veggies.\nPlace all the veggies in the oven, with paneer at the top, bake for 40 minutes (check and stir after 20 minutes)\n\nIf the cashews are roasted, no need to bake, just add them to the paneer baking tray for the last 5 minutes. If they are raw:\n\nget a small tray, mix in a tiny bit of the veggie oil and bake for 10-15m minutes.\n\n\n\nThe other stuff\nOnce the veggies are in the oven, start on this:\n\nwash rice in cold water and leave to soak 5-10 minutes\nput 3 tbsp oil in casserole and fry onions for 15-20 min until soft, brown and caramelized, stirring regularly. Remove to a bowl and put aside.\nWhile onions are cooking, place all the coconut sauce ingredients in a blender and whiz fine. Pour into a saucepan on medium heat and cook for 10 minutes, stirring every now and then\nDrain rice, cover with cold water and bring it to boil in a deep saucepan, then simmer for 10 minutes, or until rice is al dente. Drain well and leave aside. Don’t cook fully as it will cook a little bit more in the oven later.\n\n\n\nPut it all together\nNow to layer the biryani:\n\nput half the tomatoe/paneer/chickpea mix in the bottom of the biryani dish\nquarter rice and quarter onions, some of the cashews\nhalf the coconut sauce and half beetroot and sweet potatoes\nquarter rice and quarter onions, rest of the cashews\nrepeat\n\nSave a bit of the coconut sauce to put on top. Make sure cashews aren’t peeking out else they burn,\nNow cover the dish and bake for 30-35 minutes\nServe with cucumber + mint raita. (Note: add recipe for that)\n\nAnd thats all."
  },
  {
    "objectID": "posts/recipes/zuchinni-pasta.html",
    "href": "posts/recipes/zuchinni-pasta.html",
    "title": "Zucchini pasta",
    "section": "",
    "text": "My one and only pasta receipe."
  },
  {
    "objectID": "posts/recipes/zuchinni-pasta.html#ingredients",
    "href": "posts/recipes/zuchinni-pasta.html#ingredients",
    "title": "Zucchini pasta",
    "section": "Ingredients",
    "text": "Ingredients\n\n6-8 Zuchinnis, sliced into half moons\n4 tomatoes, rinsed and diced\ngarlic, lots of it, 6 cloves diced, ideally fresh\n1-2 chillis (optional)\nshallots (optional)\nPenne Pasta (or any other type)\nmint, chopped\nparmesan cheese, fancy as you want it to be, shredded or shaved\nsalt"
  },
  {
    "objectID": "posts/recipes/zuchinni-pasta.html#steps",
    "href": "posts/recipes/zuchinni-pasta.html#steps",
    "title": "Zucchini pasta",
    "section": "Steps",
    "text": "Steps\n\nheat a large frypan on medium with lots of oil and fry garlic and shallows for 1-2 minutes, add chili, fry 30 more seconds\nturn up heat and add the zuchhni, salt generously\nlet it be for 5 minutes, stir once and let it be again. Too much stirring breaks it up. Fry until looking slightly caramalized.\nboil pasta, drain and put into bowls\nadd zuchinni, tomatoes and mint\n\nServe with parmesan and a salt grinder. Fast, easy, delicious."
  },
  {
    "objectID": "posts/recipes/mung-sprouts-salad.html",
    "href": "posts/recipes/mung-sprouts-salad.html",
    "title": "Mung Beans Salad",
    "section": "",
    "text": "A very tasty salad, suitable for a side dish or an easy lunch."
  },
  {
    "objectID": "posts/recipes/mung-sprouts-salad.html#ingredients",
    "href": "posts/recipes/mung-sprouts-salad.html#ingredients",
    "title": "Mung Beans Salad",
    "section": "Ingredients",
    "text": "Ingredients\n\n200-250 grams baby tomatoes, rinsed and halved\n250 grams mung beans sprouts (rinse)\n1 cucumber, deseeded and sliced\n8-10 spring onions, top and tailed and sliced\ncoriander, finely chopped\n2 cm ginger, peeled and tiny diced\nradishes (a bunch or 8ish), finely sliced\n\n\nDressing\n\n1.5 tsp whole grain mustard (or dijon, or normal, not too strong)\n1.5 tsp dijon mustard (smooth and mild)\n1 tsp salt\n1 tsp sugar\n2 tbsp olive oil\n1/2 lemon\nground black pepper, tiny bit"
  },
  {
    "objectID": "posts/recipes/mung-sprouts-salad.html#directions",
    "href": "posts/recipes/mung-sprouts-salad.html#directions",
    "title": "Mung Beans Salad",
    "section": "Directions",
    "text": "Directions\nAdd half the ginger to the salad dressing and mix, chuck everything else in a salad bowl and throw on the dressing.\nServes about 5 as a side dish, or 2-3 as a lunch with a bit of bread.\nNote: Could dice a chilli and add it to the salad dressing for that extra kick."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html",
    "href": "posts/code/advent-of-code-2015/index.html",
    "title": "Advent of Code 2015",
    "section": "",
    "text": "A slow solution of Advent of Code 2015. The following is a write up of how to solve and the things I learned while doing so - look at the AOC reddit site for ninja level solutions.\nI am trying to\nFirst up, I’m importing all the libs I’ll use up here:\nCode\n# python essentials\nimport os\nimport re\nimport hashlib\nimport math\nfrom pathlib import Path\nfrom typing import List, NamedTuple\nfrom collections import defaultdict, namedtuple, Counter\n\n# useful external libs\n#import numpy as np\nimport pandas as pd\n\n# misc utils\nimport requests\n#from tqdm.notebook import trange, tqdm # progress bars for slow funcs\n#from functools import reduce \n\n# for plots, cause visuals\nimport matplotlib.pyplot as plt # goto python viz lib\n#import seaborn as sns # prettify matplotlib\nfrom IPython.display import display, Markdown\n\n# javascript plotting for interactive graphs\n#import altair as alt\n#import plotly.express as px\nSome helper functions to avoid rewriting the same code for all the problems:\nCode\ndef get_input(day:int=1, path=\"inputs\"):\n    try:\n        return (Path() / f\"{path}/{day}.txt\").read_text().strip()\n    except:\n        print(f\"Failed to load {day}.txt from `inputs` subdir.\")\n\ndef printmd(txt): display(Markdown(txt))"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "href": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "title": "Advent of Code 2015",
    "section": "Day 1: Not Quite Lisp",
    "text": "Day 1: Not Quite Lisp\n# We’re standing at a inifinite building, and following instructions:( is up, ) is down to find the right floor.\nThis is simple - minus the ups from the downs:\n\n\nCode\nin1 = get_input(1)\nin1.count(\"(\") - in1.count(\")\")\n\n\n138\n\n\nA list comprehension version for kicks:\n\n\nCode\nsum([1 if char == \"(\" else -1 for char in in1])\n\n\n138\n\n\nfor part 2, we need to find the first time the we enter the basement while following the instructions.\n\n\nCode\nfloor, ans = 0, None\nfloors = []\n\nfor i, mv in enumerate(inp1):\n    if mv == \"(\":\n        floor += 1\n    else:\n        floor -= 1\n    \n    floors.append(floor)\n    \n    if floor == -1 and not ans:\n        ans = i + 1\n        printmd(f\"First reached the basement at timestep: **{i + 1}**\")\n        #break # no need to continue climbing\n\nplt.title(\"Floor Tracker\"); plt.xlabel(\"Timestep\"); plt.ylabel(\"Floor\")\nplt.axvline(x=ans, label=\"Part 2\", alpha=0.35)\nplt.axhline(y=-1, label=\"Basement\", color=\"red\", alpha=0.35)\nplt.axhline(y=138, label=\"Final Floor\", color=\"orange\", alpha=0.35)\nplt.plot(range(len(floors)), floors, label=\"Position\")\nplt.legend(loc=\"lower right\");\n\n\nFirst reached the basement at timestep: 1771"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "href": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "title": "Advent of Code 2015",
    "section": "Day 2: I Was Told There Would Be No Math",
    "text": "Day 2: I Was Told There Would Be No Math\nHow much wrapping paper is needed to wrap a bunch of presents? We need 2*l*w + 2*w*h + 2*h*l paper, and the input is the l, w and h of each present.\nIn the bad old days of programming, this would be the perfect place to represent the data as a list or tuple in the form [3 ,3, 9] representing [l, w, h]. But now we can use namedtuples to make it easier to understand the data, and also it makes it easier to add more info, e.g type of paper used, cost etc:\n\n\nCode\nclass Present(NamedTuple):\n    l: int\n    w: int\n    h: int\n\ndata2 = [Present(*[int(x) for x in i.split(\"x\")]) for i in get_input(2).split(\"\\n\")]\ndata2[:4]\n\n\n[Present(l=29, w=13, h=26),\n Present(l=11, w=11, h=14),\n Present(l=27, w=2, h=5),\n Present(l=6, w=10, h=13)]\n\n\nNow to calcuate the area:\n\n\nCode\ndef get_present_area(p: Present) -&gt; int:\n    box_area = sum([2*p.l*p.w, 2*p.w*p.h, 2*p.h*p.l])\n    extra_paper = math.prod(sorted(p)[:2])\n    return box_area + extra_paper\n\nsum([get_present_area(x) for x in data2])\n\n\n1586300\n\n\nNow we need to calcuate the ribbon required, which is equal to the perimeter of the smallest face + cubic volume of the present\n\n\nCode\ndef ribbon(present: Present) -&gt; int:\n    \"\"\"takes in present, returns length of ribbon needed to wrap\"\"\"\n    l, w, h = sorted(present)\n    return 2*l + 2*w + l*w*h\n\nsum([ribbon(present) for present in data2])\n\n\n3737498"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "href": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "title": "Advent of Code 2015",
    "section": "Day 3: Perfectly Spherical Houses in a Vacuum",
    "text": "Day 3: Perfectly Spherical Houses in a Vacuum\n# Santa is delivering presents to houses, and his movements is 1 step at a time: north (^), south (v), east (&gt;), or west (&lt;)\n\n\nCode\ninp3 = get_input(3)\n\ndirs = {\"^\": (0,1), \"&gt;\": (1,0), \"v\": (0,-1), \"&lt;\": (-1, 0)}\n\ndef get_moves(data):\n    moves = [(0,0)]  # starting point\n\n    for mv in data:\n        x, y = moves[-1] # x,y of current pos\n        xx, yy = dirs[mv]\n        moves.append((x + xx, y + yy))\n    return moves\n\nmoves = get_moves(inp3)\nc = Counter(moves)\nprintmd(f\"Santa visited **{len(c)}** unique places.\")\n\nx, y = zip(*moves)\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"Santa visited {len(c)} unique places in {len(moves)} visits\")\nax.plot(x,y, alpha=0.7, label=\"Santa's Movements\"); ax.legend();\n\n\nSanta visited 2565 unique places.\n\n\n\n\n\nfor part 2, we have two santas! They move alternatingly, so we can say Santa_1 does all the odd moves and Santa_2 does all the even moves:\n\n\nCode\nsanta_1 = get_moves(inp3[::2])  # all the odd moves\nsanta_2 = get_moves(inp3[1::2]) # all the even moves\n\ntwo_santas = Counter(santa_1 + santa_2)\nprintmd(f\"The two santas visited **{len(two_santas)}** unique places.\")\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"the two santas visited {len(two_santas)} unique places\")\n\nfor name, santa in zip((\"Bob\", \"Alice\"), (santa_1, santa_2)):\n    x, y = zip(*santa)\n    ax.plot(x,y, alpha=0.8, label=f\"Santa_{name}\")\nax.legend();\n\n\nThe two santas visited 2639 unique places."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "href": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "title": "Advent of Code 2015",
    "section": "Day 4: The Ideal Stocking Stuffer",
    "text": "Day 4: The Ideal Stocking Stuffer\n#\n\n\nCode\ninp4 = \"bgvyzdsv\"\ntest4_1 = \"abcdef\" #609043\ntest4_2 = \"pqrstuv\" # 1048970\n\ndef make_hash(txt):\n    return hashlib.md5(txt.encode(\"utf\")).hexdigest()\n\n\n\nFalse\n\n\n\n\nCode\ndef day_4_1(inp=inp4, s=\"None\", target=\"00000\", i=0) -&gt; int:\n    while not s.startswith(target):\n        i += 1\n        txt = inp + str(i)\n        s = make_hash(txt)\n    printmd(f\"_{inp}_ target at position **{i:,}** ({s})\")\n    return i\n\nassert day_4_1(\"abcdef\") == 609043    # tests are always a good idea\nassert day_4_1(\"pqrstuv\") == 1048970    \n\nday_4_1()\n\n\nabcdef target at position 609,043 (000001dbbfa3a5c83a2d506429c7b00e)\n\n\npqrstuv target at position 1,048,970 (000006136ef2ff3b291c85725f17325c)\n\n\nbgvyzdsv target at position 254,575 (000004b30d481662b9cb0c105f6549b2)\n\n\n254575\n\n\nPart two just changes the target sring to have one more zero so thanks to making part one a function this is easy:\n\n\nCode\nday_4_1(target=\"000000\")\n\n\nbgvyzdsv target at position 1,038,736 (000000b1b64bf5eb55aad89986126953)\n\n\n1038736"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "href": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "title": "Advent of Code 2015",
    "section": "Day 5: Doesn’t He Have Intern-Elves For This?",
    "text": "Day 5: Doesn’t He Have Intern-Elves For This?\n# We have a list of strings, and Santa has the following rules to figure out which ones are nice:\n\nat least three vowels (aeiou only), like aei, xazegov, or aeiouaeiouaeiou.\nat least one letter that appears twice in a row, like xx, abcdde (dd), or aabbccdd (aa, bb, cc, or dd).\ndoes not contain the strings ab, cd, pq, or xy, even if they are part of one of the other requirements.\n\n\n\nCode\nvowels = \"aeiou\"                        # need vowels\nbad_strings = [\"ab\", \"cd\", \"pq\", \"xy\"]  # don't want these\nregex = re.compile(r\"([a-zA-Z])\\1{1,}\") # search for 2+ letters in a row\n\ntest4 = [\"ugknbfddgicrmopn\", \"aaa\", \"jchzalrnumimnmhp\", \n        \"haegwjzuvuyypxyu\", \"dvszwmarrgswjxmb\"]\n\ndef is_nice_string(txt):\n    vowel_count = len([char for char in txt if char in vowels]) &gt;= 3\n    two_chars = len(re.findall(regex, txt)) &gt; 0\n    no_bad_str = True if (sum([s3d in txt for s in bad_strings]) == 0) else False\n    \n    return vowel_count and two_chars and no_bad_str\n\n[is_nice_string(t) for t in test4] #== [False, False, True, True, True]\n\n\n[True, True, False, False, False]\n\n\n\n\nCode\ninp5 = get_input(5).split(\"\\n\")\nprint(\"Number of nice strings: \", sum([is_nice_string(t) for t in inp5]))\n\n\nNumber of nice strings:  258\n\n\nfor part two, the rules have changed, a nice string has these properties:\n\nIt contains a pair of any two letters that appears at least twice in the string without overlapping, like xyxy (xy) or aabcdefgaa (aa), but not like aaa (aa, but it overlaps).\nIt contains at least one letter which repeats with exactly one letter between them, like xyx, abcdefeghi (efe), or even aaa.\n\n\nNote: the rest remains to be done\n\n\n\nCode\nregex_2char = re.compile(r\"([a-zA-Z])\\1{1,}\")\nregex_3char = re.compile(r\"([a-zA-Z])\\1{2,}\")\n\nfor txt in [\"aa\", \"aba\", \"aaa\"]:\n    print(re.findall(regex_2char, txt))\n\n\n['a']\n[]\n['a']"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "href": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "title": "Advent of Code 2015",
    "section": "Day 6: Probably a Fire Hazard",
    "text": "Day 6: Probably a Fire Hazard\n#"
  },
  {
    "objectID": "posts/code/flood_fill.html",
    "href": "posts/code/flood_fill.html",
    "title": "Floodfill algorithim",
    "section": "",
    "text": "Flood Fill is a way to to visit every point in a bounded region. This makes it useful for many purposes. In this notebook I implement the “bucket fill” flood fill algo.\nFirst up generating a grid to flood fill:\nfill = np.random.randint(0, 2, size=(128,128), dtype=\"int\")\nfill\n\narray([[0, 1, 0, ..., 0, 1, 0],\n       [1, 1, 0, ..., 0, 0, 1],\n       [1, 1, 0, ..., 1, 0, 0],\n       ...,\n       [0, 1, 0, ..., 1, 1, 1],\n       [1, 1, 0, ..., 0, 1, 1],\n       [1, 0, 0, ..., 0, 1, 0]])\nEyeballing this grid visually:\nplt.imshow(fill);"
  },
  {
    "objectID": "posts/code/flood_fill.html#the-floodfill-algo",
    "href": "posts/code/flood_fill.html#the-floodfill-algo",
    "title": "Floodfill algorithim",
    "section": "the floodfill algo",
    "text": "the floodfill algo\nThe below function is a recursive implementation of flood fill - it will flood fill a a single region from 1 val to another:\n\ndef flood_fill(x, y, old, new):\n    \"\"\"takes in a x,y position from where to flood fill, the old val to change from and the new val\n    to change too, and then does so on a to_fill array\"\"\"\n    \n    if fill[x][y] != old or fill[x][y] == new:\n        return\n    \n    fill[x][y] = new\n    \n    max_x = len(fill) - 1\n    max_y = len(fill) - 1\n    \n    if x &gt; 0: # go left\n        flood_fill(x-1, y, old, new)\n    \n    if x &lt; max_x: # go right\n        flood_fill(x+1, y, old, new)\n        \n    if y &gt; 0: # go down\n        flood_fill(x, y-1, old, new)\n    if y &lt; max_y: # go up\n        flood_fill(x, y+1, old, new)\n\nHere, I flood fill the fill, converting all the 1’s to 8’s.\n\nregion_count = 0\n\nimagelist = list()\nimagelist.append(fill)\n\nfor i in range(len(fill)):\n    for j in range(len(fill[0])):\n        if fill[i][j] == 1:\n            flood_fill(i, j, 1, 8)\n            region_count += 1\n            imagelist.append(fill)\n            if region_count % 100 == 0:\n                plt.imshow(imagelist[region_count])\n                plt.title(f\"Flood Fill by region\")\n                plt.show()"
  },
  {
    "objectID": "posts/blogs-i-like.html",
    "href": "posts/blogs-i-like.html",
    "title": "Data Science blogs",
    "section": "",
    "text": "Blogs on data science, AI, machine learning etc etc. which I liked.\nChris Albon’s notes on data science and ai\nThis is amazingly awesome. Essentially he has put online all his programming notes, broken down into bite sized chunks on how to do stuff, all organized neatly and accessibly.\nThis is the blog I want to be. He has a somewhat ugly way to transform ipython notebooks into markdown files which hugo processes into html - its way ugly really but I’m going to try it out cause jupyter notebooks are awesome.\nADVENTURES IN DEEP LEARNING\nThis guy is documenting his deep learning journey from zero to hero. Though thats a bit misleading as he already was a computer scientist."
  },
  {
    "objectID": "posts/reading/three-body-problem.html",
    "href": "posts/reading/three-body-problem.html",
    "title": "The Three Body Problem",
    "section": "",
    "text": "So many ideas. It’s my favourite first contact book. While some of the ideas might be over the top, they keep coming. Whats really impressive is that this is just book 1 of a trilogy and the ideas keep going.\nThis is a seminal work in science fiction, the kind which only comes once a decade (if so). I still think about the ideas and philosophies in the book\nThank god the internet exists as this book is best read alongside learning more about some of the things mentioned."
  },
  {
    "objectID": "posts/reading/ghachar-ghochar.html",
    "href": "posts/reading/ghachar-ghochar.html",
    "title": "Ghachar Ghochar",
    "section": "",
    "text": "A family goes from poverty to money - and it tears them apart. A hard look inside one family’s financial journey.\nWhat’s really interesting is how the author shows us that poor families are a community, within themselves, their neighbours and the people they work with, but rich families are individual unto to themselves.\nGoogle Books"
  },
  {
    "objectID": "posts/reading/stats-done-wrong-book.html",
    "href": "posts/reading/stats-done-wrong-book.html",
    "title": "Statistics Done Wrong: The Woefully Complete Guide",
    "section": "",
    "text": "This is a great statistics primer - Alex Reinhart explains commonly used statistical techniques in use by not just showing how ppl make mistakes, but by explaining them in simple english which actually makes sense, unlike most “real” statistics books.\ntodo: write as a jupyter notebook showing some of the lessons learned?"
  },
  {
    "objectID": "posts/reading/the_uninhabitable_earth.html",
    "href": "posts/reading/the_uninhabitable_earth.html",
    "title": "The Uninhabitable Earth",
    "section": "",
    "text": "It’s a awesome political, economical and yes, climate overview of the planet and where its headed. Too much of future looking literature really holds its tongue. This is a no holds barred overview.\nA book extract:\nhttps://www.theguardian.com/environment/2019/feb/02/the-devastation-of-human-life-is-in-view-what-a-burning-world-tells-us-about-climate-change-global-warming\nThe author writes for nymag and has a bunch of articles here."
  },
  {
    "objectID": "posts/reading/grokking-deep-learning-book.html",
    "href": "posts/reading/grokking-deep-learning-book.html",
    "title": "Grokking Deep Learning by Andrew Trask",
    "section": "",
    "text": "Andrew Trask is an AI superstar, having come up with the idea for Generative Adverserial Networks (GANS) - one of the most exicting new AI techniques today - in a bar.\nIn this book he comes down to earth and explains neural networks from scratch to mere mortals. The book is still being written, but whats there so far is a really good easy to understand introduction to neural networks.\nWhile frameworks like tensorflow and pytorch are essential for actually doing useful and practical AI stuff, they abstract away a lot of the inner workings of how data is transformed as it passes through the many layers of a deep learning network.\nGrokking Deep Learning has you build a NN from scratch, by hand, propagating neurons down the hill through the snow, first backwards then forwards. Its a great excercise to learn how Neural Networks work internally.\n\nThis book will teach you the fundamentals of Deep Learning from an intuitive perspective, so that you can understand how machines learn using Deep Learning. This book is not focused on learning a framework such as Torch, TensorFlow, or Keras. Instead, it is focused on teaching you the Deep Learning methods behind well known frameworks. Everything will be built from scratch using only Python and numpy (a matrix library). In this way, you will understand every detail that goes into training a neural network, not just how to use a code library. You should consider this book a pre-requisite to mastering one of the major frameworks."
  },
  {
    "objectID": "posts/reading/too_migrant_too_muslim_too_loud.html",
    "href": "posts/reading/too_migrant_too_muslim_too_loud.html",
    "title": "Too Migrant, Too Muslim, Too Loud",
    "section": "",
    "text": "The book to read to get a handle of where Australia is today, and how politics works at the state and federal level.\n\nToo Migrant, Too Muslim, Too Loud is a no-holds-barred memoir and manifesto from outspoken senator, trouble¬maker and multicultural icon Mehreen Faruqi. As the first Muslim woman in any Australian parliament, Mehreen has a unique and crucial perspective on our politics and democracy. It is a tale of a political outsider fighting for her right and the rights of others like her to be let inside on their terms.\n\nOn offsets:\n\nAll this is done under the false promise of the greatest con job in environmental regulation: biodiversity offsetting, which is no more than an accounting trick to justify destruction of nature.\n\n\nThere is a real risk of ‘climate colonialism’ locking away from communities in the global South land which is purchased by the North as carbon offsets. Poor countries cannot simply become ‘offsets’ to allow business as usual for climate culprits.\n\n\nIt is becoming increasingly clear that Western nations that overconsume massively disproportionate quantities of natural resources and have a footprint many times larger than those in the global South don’t intend to change their ways even as they tackle climate change. Complex socio-political dimensions of environmental problems remain unaddressed, as do power structures like racism, sexism, neoliberalism and capitalism. These are uncomfortable truths for the climate movement. Their focus remains, by and large, on technological solutions. Rapid transformation to renewable energy is the mantra. Fossil-fuel corporations are the enemy. I don’t disagree with that—but I want more."
  },
  {
    "objectID": "posts/reading/freedom_at_midnight.html",
    "href": "posts/reading/freedom_at_midnight.html",
    "title": "Freedom at Midnight",
    "section": "",
    "text": "Oh goody, yet another book written through colonial tinted glasses.\nIt’s a well written, easy reading book so I can see why it’s so popular, and if it was labeled fictional, I’d give it four stars, for fictional it is, speaking of a world where the British Raj and it’s leaders brought civilization to the masses, but the masses turned the wise Brits away even though they were led by that holiest of holy cows, Lord Mountbatten - and this turning away caused mass bloodshed in the process. It’s almost a biblical story, and no wonder so many people still think fondly of empire, they probably read books like this one.\nThe target audience for the book seems to be people who want to be able to understand just enough of the British Raj to absolve the Raj of any guilt and blame Jinnah and others for much of the ills of partition.\nThe authors struggle with the very basic idea of why some brown people wanted independence, especially when the British were so benevolent and wise, and give up and just talk about it like it was just something which was happening, no hard feelings really, except against Jinnah.\nThe book ignores practically all Indian writings, and even famous British writers like Adam Smith or Florence Nightingale, who were harping on about the British needlessly killing millions in famines every few years in British India. Famines, bigger than the holocaust - skip that, lets concentrate and talk about Mountbatten’s shiny medals and his big big parties! And oh, look, Mountbatten has a Rolls-Royce! And he’s the grandson of some queen or the other!\nSo on one side we have Mountbatten, working hard, inviting a few brown men to luncheons every now and then, working so hard, with hardly any help, just a few thousand servants, not much at all, and on the other we have those spoiled little boys, Gandhi and Jinnah, needlessly talking about freedom and what not. It was enough to put Mountbatten of his tea, but poor little Mountbatten suffered through it all, why one year he met Jinnah twice! And after each visit he had to go recover in the hill stations of Simla because Jinnah was such an unpleasant little man, asking uncomfortable questions. Forget the questions, did you know Jinnah was a stiff man who had this very uncomfortable stare?\nWhat were those uncomfortable questions? If you only read this book you won’t know, for the authors were obviously very aware that Mountbatten descendants themselves would be reading this book, and they didn’t want to make them uncomfortable with annoying little questions.\nSome reviews point out that this book is well researched - I’m sure it is, but only in that section of the British Imperial Archives which has been scrubbed of voices which are in any way critical of British rule, or attempt to look at it honestly.\nLittle things like India having to bear the staggering high military cost of Empire don’t exist in the authors fictionalized world. Heck, the authors go all the other way, and say that the British lost money during the Raj, and it was literally out of the goodness of their white hearts that the British ruled India.\nHistory is a story - and the problem with this book is not that it’s a story - the problem is that it’s a glib view which completely omits and washes British hands of what they did during their occupation and departure from India."
  }
]