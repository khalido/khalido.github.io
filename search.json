[
  {
    "objectID": "quote-clock/index.html",
    "href": "quote-clock/index.html",
    "title": "Literary Quote Clock",
    "section": "",
    "text": "A raspberry pi project, inspired by tjaap. This python notebook is looking at the basics - how to get the data, parse it, display a quote, then moves on to create a repo and put the code onto the actual hardware.\nThe primary source for quotes is the guardian’s user provided list. This list is incomplete, and the guardian put up a bigger list in 2014 as a gsheet but thats no longer avaiable, so I found a more complete quotes list on github.\nFirst up, making a list of times to check for missing times:\n# a list containiner all 1,440 times\ntimes = [f\"{hour:02}:{minute:02}\" for hour in range(24) for minute in range(60)]\nassert len(times) == 60 * 24  # check for all 1,440 clock times\nprint(times[55:65])  # check it looks good\n\n['00:55', '00:56', '00:57', '00:58', '00:59', '01:00', '01:01', '01:02', '01:03', '01:04']"
  },
  {
    "objectID": "quote-clock/index.html#the-literary-quotes",
    "href": "quote-clock/index.html#the-literary-quotes",
    "title": "Literary Quote Clock",
    "section": "The literary quotes",
    "text": "The literary quotes\nI initial looked at the list of quotes on the guardian website from 2011, but that was really incomplete, so after some internet kungfu I moved on to a better list of quotes maintained here.\nThat project already implements a literary clock, but ignoring that to make my own, here goes:\n\nurl = \"https://raw.githubusercontent.com/JohannesNE/literature-clock/master/litclock_annotated.csv\"\n\ndf = pd.read_csv(\n    url, sep=\"|\", names=[\"time\", \"annot\", \"quote\", \"title\", \"author\", \"nsfw\"]\n)\n\ndf.sample(4)\n\n\n\n\n\n\n\n\ntime\nannot\nquote\ntitle\nauthor\nnsfw\n\n\n\n\n2854\n22:00\nten\nBy ten, Quoyle was drunk. The crowd was enormo...\nThe Shipping News\nE. Annie Proulx\nunknown\n\n\n210\n01:30\n1:30 A.M.\nAround 1:30 A.M. the door opened and I thought...\nMicroserfs\nDouglas Coupland\nunknown\n\n\n1596\n12:00\nNoon\nThe Birds begun at Four o'clock&lt;br/&gt;Their peri...\nThe Birds Begun at Four o'clock\nEmily Dickinson\nunknown\n\n\n1878\n14:31\nthirty-one minutes past two\nIgor checked the time again. He didn't really;...\nThe Vampire's Treaty\nMatt Shaw\nsfw\n\n\n\n\n\n\n\nSome basic data checks:\n\nmissing_times = [time for time in times if time not in df.time.values]\nprint(f\"{len(missing_times)} time values missing: {missing_times}\")\n\nsingle_quote = sum(df.groupby(\"time\").count().title == 1)\nmultiple_quotes = sum(df.groupby(\"time\").count().title &gt; 1)\n\nprint(\n    f\"\\n{single_quote} times have exactly 1 quote, {multiple_quotes} times have more than 1 quote.\"\n)\n\nax = (\n    df[[\"time\", \"title\"]]\n    .groupby(\"time\")\n    .count()[df.groupby(\"time\").count().title &gt; 6]\n    .plot.bar(title=\"Times with quotes &gt; 6\", figsize=(8, 2))\n)\n\n21 time values missing: ['02:49', '03:24', '06:03', '06:07', '06:18', '06:47', '07:41', '08:21', '10:28', '11:16', '11:46', '12:31', '13:19', '13:36', '14:23', '15:31', '17:46', '18:14', '18:44', '21:06', '21:14']\n\n886 times have exactly 1 quote, 533 times have more than 1 quote.\n\n\n\n\n\nSo we now have a dataframe full of quotes. I’ll deal with the 21 missing quotes later.\nThe gthub repo produced a folder full of json files, one for each time stamp, so I could just re-use that, but whats the fun of that.\n\nSpliting up quotes\nIdeally, we want the current time in the quote highlighted. The csv file has a col annot which is the time part of the quote string. Initially I tried to use regex to split the string, but since the time is in so many different formats it has to be done manually, hence the annot col.\nThe time can contain special characters like [] so re.escape fixes that. I had to use re.split instead of str.split here as only re.split has a ignorecase feature.\n\ndef split_quote(row):\n    \"\"\"adds a prefix and suffix to the quote.\"\"\"\n    prefix, suffix = re.split(\n        re.escape(row.annot), row.quote, maxsplit=1, flags=re.IGNORECASE\n    )\n    row[\"prefix\"] = prefix\n    row[\"suffix\"] = suffix\n    return row\n\n\ndf = df.apply(split_quote, axis=1)\ndf.sample(3)\n\n\n\n\n\n\n\n\ntime\nannot\nquote\ntitle\nauthor\nnsfw\nprefix\nsuffix\n\n\n\n\n2453\n19:00\nseven o’clock\n“Yes, seven o’clock. I will bring dessert.” Sm...\nThe Catacombs\nJeremy Bates\nunknown\n“Yes,\n. I will bring dessert.” Smiling in her sad-ha...\n\n\n612\n05:00\nfive in the morning\nI did Danièle’s math in my head. “If we start ...\nThe Catacombs\nJeremy Bates\nunknown\nI did Danièle’s math in my head. “If we start ...\n. Seven hours back, it won’t be noon until we ...\n\n\n685\n05:42\neighteen minutes to six\nAt eighteen minutes to six, they blew the wall...\nA Gentleman's Game\nGreg Rucka\nsfw\nAt\n, they blew the wall, and even then, it was al...\n\n\n\n\n\n\n\nI can now display the quote with the time highlighted, an example below in markdown format:\n\nrow = df.iloc[100]\nf\"{row.prefix}**{row.annot}**{row.suffix}\"\n\n'It was **half-past twelve** when I returned to the Albany as a last desperate resort. The scene of my disaster was much as I had left it. The baccarat-counters still strewed the table, with the empty glasses and the loaded ash-trays. A window had been opened to let the smoke out, and was letting in the fog instead.'\n\n\n\n\nSaving quotes to disk\nFirst up, saving a single json object containing all the quotes, which is easy to use:\n\n# returns a string representing the json\nquotes_json = df.groupby(\"time\").agg(list).to_json(orient=\"index\")\n\n# writes the json to disk\ndf.groupby(\"time\").agg(list).to_json(\"quotes.json\", orient=\"index\", compression=None)\n\n# read the quotes from disk\nwith open(\"quotes.json\", \"r\") as f:\n    quotes = json.load(f)\n\nprint(f\"{len(quotes):,} keys which look like: {list(quotes.keys())[40:44]}\")\nquotes[\"01:40\"]\n\n1,419 keys which look like: ['00:40', '00:41', '00:42', '00:43']\n\n\n{'annot': ['one-forty am'],\n 'quote': ['March twelfth, one-forty am, she leaves a group of drinking buddies to catch a bus home. She never makes it.'],\n 'title': ['Bones to Ashes'],\n 'author': ['Kathy Reichs'],\n 'nsfw': ['unknown'],\n 'prefix': ['March twelfth, '],\n 'suffix': [', she leaves a group of drinking buddies to catch a bus home. She never makes it.']}\n\n\n\n\nSave each minute as a seperate file\nI’m using a raspberry pi pico w, it doesn’t have enough memory to contain the ~2k quotes. So I’m going to write a json file for every minute, and open and close each file as needed.\nI’m keeping only the columns needed to save on diskspace:\n\nfor key in df.time.unique():\n    df[[\"time\", \"annot\", \"prefix\", \"suffix\", \"title\", \"author\"]].query(\n        \"time==@key\"\n    ).groupby(\"time\").agg(list).to_json(f\"quotes/{key}.json\", orient=\"index\")\n\n# Checking the total size of the many quote files\nquotes_size = (\n    sum([os.path.getsize(f\"quotes/{f}\") for f in os.listdir(\"quotes\")]) / 10**3\n)\nf\"{len(os.listdir('quotes')):,} json files total {quotes_size:.2f}kb\"\n\n'1,419 json files total 931.76kb'\n\n\nI can compress these to save further on space, which I’ll get to later if needed. Its nice to have a human readable json string rather than a binary object on the pico for future poking around."
  },
  {
    "objectID": "quote-clock/index.html#get-current-time-and-display-quote",
    "href": "quote-clock/index.html#get-current-time-and-display-quote",
    "title": "Literary Quote Clock",
    "section": "Get current time and display quote",
    "text": "Get current time and display quote\nMicropython has a time module, which is a subset of the built in python time library. The pi’s clock needs to be synced on boot with a timeserver, leaving that for the future…\nRight now, I get the current time, than open the json file for that timestamp, select a random quote from it and return it.\n\ndef get_time():\n    \"\"\"returns current time in 00:00 format\"\"\"\n    time_now = time.localtime()\n    return f\"{time_now.tm_hour}:{time_now.tm_min}\"\n\n\ndef get_quote(time_str=None):\n    \"\"\"returns a dict containing a single quote for a given time\"\"\"\n    if time_str is None:\n        time_str = get_time()\n\n    # open json file\n    with open(f\"quotes/{time_str}.json\", \"r\") as f:\n        quote = json.load(f)[time_str]\n\n    i = random.randint(0, len(quote[\"title\"]) - 1)\n    return {key: quote[key][i] for key in quote.keys()}\n\n\nquote = get_quote()\nprint(f\"quote dict keys: {quote.keys()}\")\nprint(f\"\\n{quote['prefix']}**{quote['annot']}**{quote['suffix']}\")\nf\"{quote['title']} by {quote['author']}\"\n\nquote dict keys: dict_keys(['annot', 'prefix', 'suffix', 'title', 'author'])\n\nIt was now **eight minutes to eleven**, and he began to feel rather cross and impatient. There was nothing to do in the big, ugly, stately room into which he had been shown.\n\n\n'From Out of the Vasty Deep by Marie Belloc Lowndes'\n\n\nThat seems to be working well, so now I should be able to assemble the pieces and display this on a raspberry pico display."
  },
  {
    "objectID": "quote-clock/index.html#raspberry-pi-pico",
    "href": "quote-clock/index.html#raspberry-pi-pico",
    "title": "Literary Quote Clock",
    "section": "Raspberry Pi Pico",
    "text": "Raspberry Pi Pico\nAllright, now that I have the basics ready to go, here goes putting this on a pi pico. First up, we need a github repo."
  },
  {
    "objectID": "quote-clock/index.html#misc",
    "href": "quote-clock/index.html#misc",
    "title": "Literary Quote Clock",
    "section": "Misc",
    "text": "Misc\nThings not needed anymore, left in case they are useful in the future.\n\nGuardian quotes from 2011\nGuardian clock quotes from 2011, left below as a pandas exercise in fetching a table from a html page and some basic transformations to be able to use it.\n\nclock_url = \"https://www.theguardian.com/books/table/2011/apr/21/literary-clock\"\n\ndf_quotes = (\n    pd.read_html(clock_url)[0]  # get first table\n    .rename(\n        columns={\n            \"Time of quote\": \"time\",\n            \"Quote\": \"quote\",\n            \"Title of book\": \"title\",\n            \"Author\": \"author\",\n        }\n    )\n    .sort_values(by=\"time\")\n    .drop(columns=[\"Your username\"])  # no need for this col\n    .dropna(subset=[\"quote\"])  # drop blank rows at end\n)\n\n# fix some wonky time strings\ndf_quotes.time = df_quotes.time.str.replace(\".\", \":\")\n\nprint(f\"{df_quotes.shape} quotes, so missing a lot!\")\ndf_quotes.sample(2)\n\n(935, 4) quotes, so missing a lot!\n\n\n\n\n\n\n\n\n\ntime\nquote\ntitle\nauthor\n\n\n\n\n204\n07:10:00h\nA search in Bradshaw informed me that a train ...\nThe 39 Steps\nJohn Buchan\n\n\n409\n11:15:00h\nThe first time I saw them it was around eleven...\nWhere I'm Calling From\nRaymond Carver\n\n\n\n\n\n\n\nThere is a 2014 Guardlian list, but looks like that was a google sheet which they have now taken down."
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html",
    "href": "posts/algorithims/histogram_max_area.html",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "This is a popular coding interview question. It’s really simple with a simple histogram using pen and paper, but harder to think about in code, as it takes a bit of thinking through.\nDetailed problem description: https://leetcode.com/problems/largest-rectangle-in-histogram/\nA few test arrays with the largest area:\n\ntests = (\n    ([2, 1, 5, 6, 2, 3], 10),\n    ([6, 3, 1, 4, 12, 4], 12),\n    ([5, 6, 7, 4, 1], 16),\n    ([2, 1, 3, 4, 1], 6),\n)\n\nThe arrays are visualized below, which makes it much easier to think about the solution. I found it easier to do it on paper first, before coding.\n\n\nCode\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.8,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n\n\n\n\nLooking at the array makes it easy to see where the largest rectangle might be.\n\n\nWe can brute force this by generating the largest possible rectangle at every item in the array.\nFor each N in the arry:\n\nfind the left and right boundary of the largest possible rectangle\nthis gives us the width, exluding the width of N itself (adding 1 to ad back N)\nso now we have the width, and the height is just the value N in the array\n\nTo keep things simple, first up a helper function which returns the rectangle boundaries of a given point in an array:\n\ndef find_boundary(idx: int, arr: list[int]) -&gt; tuple[int, int]:\n    \"\"\"\n    Example:\n        idx 2 for [2, 1, 5, 6, 2, 3] returns (2, 3)\n    Returns:\n        (left, right)\n    \"\"\"\n\n    # find left boundary (can be itself)\n    left = 0\n    if idx == left:\n        pass  # deals with the left edge\n    else:\n        # march leftwards all the way to zero:\n        for j in range(idx - 1, -1, -1):\n            if arr[j] &lt; arr[idx]:\n                left = j + 1  # adding 1 to exlude boundary\n                break  # exit loop once the first boundary found\n\n    # find right boundary (can be itself)\n    right = len(arr) - 1  # deal with the right edge\n\n    if idx == right:\n        pass  # at right edge already\n    else:\n        # march rightwards\n        for j in range(idx, len(arr)):\n            if arr[j] &lt; arr[idx]:\n                right = j - 1  # subtracting 1 to exclude boundary\n                break\n\n    return left, right\n\nPhew! that should return the (left, right) boundaries for a given index and array. Using np.argmax should make this faster, but I wanted to see how it would look in pure python.\n\n# testing this for the first arrary in the tests\nfor arr, ans in tests:\n    print(\"array: \", arr)\n    for i in range(len(arr)):\n        print(f\"Index {i}: (val {arr[i]}) - Boundaries: {find_boundary(i, arr)}\")\n    break\n\narray:  [2, 1, 5, 6, 2, 3]\nIndex 0: (val 2) - Boundaries: (0, 0)\nIndex 1: (val 1) - Boundaries: (0, 5)\nIndex 2: (val 5) - Boundaries: (2, 3)\nIndex 3: (val 6) - Boundaries: (3, 3)\nIndex 4: (val 2) - Boundaries: (2, 5)\nIndex 5: (val 3) - Boundaries: (5, 5)\n\n\nI drew the boundaries by hand for the first array to test the algo, and checked that the left and right values at each index are correct.\nThe find_boundary func is working, so now its easy to get the area of the max rectangle.\n\n\nThe below function iterates through every item in the array, calculates the area of the largest rectangle at that point, and updates the max area function.\n\ndef max_area(arr: list[int], viz: bool = False) -&gt; int:\n    \"\"\"returns the area of the biggest rectangle,\n    and optionally returns its index\"\"\"\n    max_area = 0\n    max_idx = None\n\n    for i in range(len(arr)):\n        left, right = find_boundary(i, arr)\n        # adding 1 to width as when we calc (right - left) it excludes itself\n        width = 1 + right - left\n\n        new_area = arr[i] * width\n        if new_area &gt; max_area:\n            max_area = new_area\n            max_idx = i\n\n    if viz:\n        return max_idx, max_area\n    else:\n        return max_area\n\n\nfor arr, ans in tests:\n    assert max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nThat was pretty straight forward, though a bit verbose. Now an excercise in plotting this visually:\n\n\n\nUsing matplotlib to plot the largest rectangle - this helps explain why the answer better than a wall of text.\n\n\nCode\n# to use to show different rectangele with diff colors and fills\ncolors = list(mcolors.TABLEAU_COLORS.values())\nhatches = [\"/\", \"\\\\\", \"|\", \"-\", \"+\", \"x\", \"o\", \"O\", \".\", \"*\"]\n\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.6,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n    idx, area = max_area(arr, True)\n\n    for i in range(len(arr)):\n        if i == idx:  # got too messy plotting all the rects\n            left, right = find_boundary(i, arr)\n            width = 1 + right - left\n            ax.add_patch(\n                Rectangle(\n                    (left - 0.5, 0),\n                    width,\n                    arr[i],\n                    alpha=0.35,\n                    facecolor=\"purple\",\n                    ls=\"--\",\n                    lw=2,\n                    hatch=hatches[5],\n                )\n            )\n\n\n\n\n\nEvery time I use matplotlib I’m both horrified and inmpressed by what you can do it… basically anything but so much code…\n\n\n\n\nBy using a stack, we can find the largest rect in O(n) time, as we don’t go through the array multiple times in this solution.\nInstead of relooping through the array, we can use a stack to only go through the array once.\nThis algo:\n\nwhile the stack isn’t empty and the height of the current bar is &lt;= to the top bar in the stack\n\ncalc rect area, update if bigger\n\nrect height is the bar at the top of the array\n\npush current bar onto stack\n\nthe above loops end but leaves bars in the stack which only have bigger items to their right. Deal with this by:\n\npop the stack, the height is that item, the width is the distance b/w that item and the right end of the array\ncalc rect area, update max area\n\n\n\ndef histogram_max_area(arr):\n    stack = [-1]  # the stop or sentinel value\n    max_area = 0\n\n    for i in range(len(arr)):\n        while stack[-1] != -1 and arr[stack[-1]] &gt;= arr[i]:\n            current_height = arr[stack.pop()]\n            current_width = i - stack[-1] - 1\n            max_area = max(max_area, current_height * current_width)\n        stack.append(i)\n\n    # the remaining stack items rectangles extend all the way to the end\n    while stack[-1] != -1:\n        current_height = arr[stack.pop()]\n        current_width = len(arr) - stack[-1] - 1\n        max_area = max(max_area, current_height * current_width)\n    return max_area\n\n\nfor arr, ans in tests:\n    assert histogram_max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\n\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nEven though the stack based solution is better, faster and smaller its tougher to comprehend than the simpler one above."
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html#a-simple-solution",
    "href": "posts/algorithims/histogram_max_area.html#a-simple-solution",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "We can brute force this by generating the largest possible rectangle at every item in the array.\nFor each N in the arry:\n\nfind the left and right boundary of the largest possible rectangle\nthis gives us the width, exluding the width of N itself (adding 1 to ad back N)\nso now we have the width, and the height is just the value N in the array\n\nTo keep things simple, first up a helper function which returns the rectangle boundaries of a given point in an array:\n\ndef find_boundary(idx: int, arr: list[int]) -&gt; tuple[int, int]:\n    \"\"\"\n    Example:\n        idx 2 for [2, 1, 5, 6, 2, 3] returns (2, 3)\n    Returns:\n        (left, right)\n    \"\"\"\n\n    # find left boundary (can be itself)\n    left = 0\n    if idx == left:\n        pass  # deals with the left edge\n    else:\n        # march leftwards all the way to zero:\n        for j in range(idx - 1, -1, -1):\n            if arr[j] &lt; arr[idx]:\n                left = j + 1  # adding 1 to exlude boundary\n                break  # exit loop once the first boundary found\n\n    # find right boundary (can be itself)\n    right = len(arr) - 1  # deal with the right edge\n\n    if idx == right:\n        pass  # at right edge already\n    else:\n        # march rightwards\n        for j in range(idx, len(arr)):\n            if arr[j] &lt; arr[idx]:\n                right = j - 1  # subtracting 1 to exclude boundary\n                break\n\n    return left, right\n\nPhew! that should return the (left, right) boundaries for a given index and array. Using np.argmax should make this faster, but I wanted to see how it would look in pure python.\n\n# testing this for the first arrary in the tests\nfor arr, ans in tests:\n    print(\"array: \", arr)\n    for i in range(len(arr)):\n        print(f\"Index {i}: (val {arr[i]}) - Boundaries: {find_boundary(i, arr)}\")\n    break\n\narray:  [2, 1, 5, 6, 2, 3]\nIndex 0: (val 2) - Boundaries: (0, 0)\nIndex 1: (val 1) - Boundaries: (0, 5)\nIndex 2: (val 5) - Boundaries: (2, 3)\nIndex 3: (val 6) - Boundaries: (3, 3)\nIndex 4: (val 2) - Boundaries: (2, 5)\nIndex 5: (val 3) - Boundaries: (5, 5)\n\n\nI drew the boundaries by hand for the first array to test the algo, and checked that the left and right values at each index are correct.\nThe find_boundary func is working, so now its easy to get the area of the max rectangle.\n\n\nThe below function iterates through every item in the array, calculates the area of the largest rectangle at that point, and updates the max area function.\n\ndef max_area(arr: list[int], viz: bool = False) -&gt; int:\n    \"\"\"returns the area of the biggest rectangle,\n    and optionally returns its index\"\"\"\n    max_area = 0\n    max_idx = None\n\n    for i in range(len(arr)):\n        left, right = find_boundary(i, arr)\n        # adding 1 to width as when we calc (right - left) it excludes itself\n        width = 1 + right - left\n\n        new_area = arr[i] * width\n        if new_area &gt; max_area:\n            max_area = new_area\n            max_idx = i\n\n    if viz:\n        return max_idx, max_area\n    else:\n        return max_area\n\n\nfor arr, ans in tests:\n    assert max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nThat was pretty straight forward, though a bit verbose. Now an excercise in plotting this visually:\n\n\n\nUsing matplotlib to plot the largest rectangle - this helps explain why the answer better than a wall of text.\n\n\nCode\n# to use to show different rectangele with diff colors and fills\ncolors = list(mcolors.TABLEAU_COLORS.values())\nhatches = [\"/\", \"\\\\\", \"|\", \"-\", \"+\", \"x\", \"o\", \"O\", \".\", \"*\"]\n\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.6,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n    idx, area = max_area(arr, True)\n\n    for i in range(len(arr)):\n        if i == idx:  # got too messy plotting all the rects\n            left, right = find_boundary(i, arr)\n            width = 1 + right - left\n            ax.add_patch(\n                Rectangle(\n                    (left - 0.5, 0),\n                    width,\n                    arr[i],\n                    alpha=0.35,\n                    facecolor=\"purple\",\n                    ls=\"--\",\n                    lw=2,\n                    hatch=hatches[5],\n                )\n            )\n\n\n\n\n\nEvery time I use matplotlib I’m both horrified and inmpressed by what you can do it… basically anything but so much code…"
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html#a-faster-stack-based-solution",
    "href": "posts/algorithims/histogram_max_area.html#a-faster-stack-based-solution",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "By using a stack, we can find the largest rect in O(n) time, as we don’t go through the array multiple times in this solution.\nInstead of relooping through the array, we can use a stack to only go through the array once.\nThis algo:\n\nwhile the stack isn’t empty and the height of the current bar is &lt;= to the top bar in the stack\n\ncalc rect area, update if bigger\n\nrect height is the bar at the top of the array\n\npush current bar onto stack\n\nthe above loops end but leaves bars in the stack which only have bigger items to their right. Deal with this by:\n\npop the stack, the height is that item, the width is the distance b/w that item and the right end of the array\ncalc rect area, update max area\n\n\n\ndef histogram_max_area(arr):\n    stack = [-1]  # the stop or sentinel value\n    max_area = 0\n\n    for i in range(len(arr)):\n        while stack[-1] != -1 and arr[stack[-1]] &gt;= arr[i]:\n            current_height = arr[stack.pop()]\n            current_width = i - stack[-1] - 1\n            max_area = max(max_area, current_height * current_width)\n        stack.append(i)\n\n    # the remaining stack items rectangles extend all the way to the end\n    while stack[-1] != -1:\n        current_height = arr[stack.pop()]\n        current_width = len(arr) - stack[-1] - 1\n        max_area = max(max_area, current_height * current_width)\n    return max_area\n\n\nfor arr, ans in tests:\n    assert histogram_max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\n\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nEven though the stack based solution is better, faster and smaller its tougher to comprehend than the simpler one above."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html",
    "href": "posts/code/advent-of-code-2015/index.html",
    "title": "Advent of Code 2015",
    "section": "",
    "text": "A slow solution of Advent of Code 2015. The following is a write up of how to solve and the things I learned while doing so - look at the AOC reddit site for ninja level solutions.\nI am trying to\nFirst up, I’m importing all the libs I’ll use up here:\nCode\n# python essentials\nimport os\nimport re\nimport hashlib\nimport math\nfrom pathlib import Path\nfrom typing import List, NamedTuple\nfrom collections import defaultdict, namedtuple, Counter\n\n# useful external libs\n#import numpy as np\nimport pandas as pd\n\n# misc utils\nimport requests\n#from tqdm.notebook import trange, tqdm # progress bars for slow funcs\n#from functools import reduce \n\n# for plots, cause visuals\nimport matplotlib.pyplot as plt # goto python viz lib\n#import seaborn as sns # prettify matplotlib\nfrom IPython.display import display, Markdown\n\n# javascript plotting for interactive graphs\n#import altair as alt\n#import plotly.express as px\nSome helper functions to avoid rewriting the same code for all the problems:\nCode\ndef get_input(day:int=1, path=\"inputs\"):\n    try:\n        return (Path() / f\"{path}/{day}.txt\").read_text().strip()\n    except:\n        print(f\"Failed to load {day}.txt from `inputs` subdir.\")\n\ndef printmd(txt): display(Markdown(txt))"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "href": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "title": "Advent of Code 2015",
    "section": "Day 1: Not Quite Lisp",
    "text": "Day 1: Not Quite Lisp\n# We’re standing at a inifinite building, and following instructions:( is up, ) is down to find the right floor.\nThis is simple - minus the ups from the downs:\n\n\nCode\nin1 = get_input(1)\nin1.count(\"(\") - in1.count(\")\")\n\n\n138\n\n\nA list comprehension version for kicks:\n\n\nCode\nsum([1 if char == \"(\" else -1 for char in in1])\n\n\n138\n\n\nfor part 2, we need to find the first time the we enter the basement while following the instructions.\n\n\nCode\nfloor, ans = 0, None\nfloors = []\n\nfor i, mv in enumerate(inp1):\n    if mv == \"(\":\n        floor += 1\n    else:\n        floor -= 1\n    \n    floors.append(floor)\n    \n    if floor == -1 and not ans:\n        ans = i + 1\n        printmd(f\"First reached the basement at timestep: **{i + 1}**\")\n        #break # no need to continue climbing\n\nplt.title(\"Floor Tracker\"); plt.xlabel(\"Timestep\"); plt.ylabel(\"Floor\")\nplt.axvline(x=ans, label=\"Part 2\", alpha=0.35)\nplt.axhline(y=-1, label=\"Basement\", color=\"red\", alpha=0.35)\nplt.axhline(y=138, label=\"Final Floor\", color=\"orange\", alpha=0.35)\nplt.plot(range(len(floors)), floors, label=\"Position\")\nplt.legend(loc=\"lower right\");\n\n\nFirst reached the basement at timestep: 1771"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "href": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "title": "Advent of Code 2015",
    "section": "Day 2: I Was Told There Would Be No Math",
    "text": "Day 2: I Was Told There Would Be No Math\nHow much wrapping paper is needed to wrap a bunch of presents? We need 2*l*w + 2*w*h + 2*h*l paper, and the input is the l, w and h of each present.\nIn the bad old days of programming, this would be the perfect place to represent the data as a list or tuple in the form [3 ,3, 9] representing [l, w, h]. But now we can use namedtuples to make it easier to understand the data, and also it makes it easier to add more info, e.g type of paper used, cost etc:\n\n\nCode\nclass Present(NamedTuple):\n    l: int\n    w: int\n    h: int\n\ndata2 = [Present(*[int(x) for x in i.split(\"x\")]) for i in get_input(2).split(\"\\n\")]\ndata2[:4]\n\n\n[Present(l=29, w=13, h=26),\n Present(l=11, w=11, h=14),\n Present(l=27, w=2, h=5),\n Present(l=6, w=10, h=13)]\n\n\nNow to calcuate the area:\n\n\nCode\ndef get_present_area(p: Present) -&gt; int:\n    box_area = sum([2*p.l*p.w, 2*p.w*p.h, 2*p.h*p.l])\n    extra_paper = math.prod(sorted(p)[:2])\n    return box_area + extra_paper\n\nsum([get_present_area(x) for x in data2])\n\n\n1586300\n\n\nNow we need to calcuate the ribbon required, which is equal to the perimeter of the smallest face + cubic volume of the present\n\n\nCode\ndef ribbon(present: Present) -&gt; int:\n    \"\"\"takes in present, returns length of ribbon needed to wrap\"\"\"\n    l, w, h = sorted(present)\n    return 2*l + 2*w + l*w*h\n\nsum([ribbon(present) for present in data2])\n\n\n3737498"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "href": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "title": "Advent of Code 2015",
    "section": "Day 3: Perfectly Spherical Houses in a Vacuum",
    "text": "Day 3: Perfectly Spherical Houses in a Vacuum\n# Santa is delivering presents to houses, and his movements is 1 step at a time: north (^), south (v), east (&gt;), or west (&lt;)\n\n\nCode\ninp3 = get_input(3)\n\ndirs = {\"^\": (0,1), \"&gt;\": (1,0), \"v\": (0,-1), \"&lt;\": (-1, 0)}\n\ndef get_moves(data):\n    moves = [(0,0)]  # starting point\n\n    for mv in data:\n        x, y = moves[-1] # x,y of current pos\n        xx, yy = dirs[mv]\n        moves.append((x + xx, y + yy))\n    return moves\n\nmoves = get_moves(inp3)\nc = Counter(moves)\nprintmd(f\"Santa visited **{len(c)}** unique places.\")\n\nx, y = zip(*moves)\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"Santa visited {len(c)} unique places in {len(moves)} visits\")\nax.plot(x,y, alpha=0.7, label=\"Santa's Movements\"); ax.legend();\n\n\nSanta visited 2565 unique places.\n\n\n\n\n\nfor part 2, we have two santas! They move alternatingly, so we can say Santa_1 does all the odd moves and Santa_2 does all the even moves:\n\n\nCode\nsanta_1 = get_moves(inp3[::2])  # all the odd moves\nsanta_2 = get_moves(inp3[1::2]) # all the even moves\n\ntwo_santas = Counter(santa_1 + santa_2)\nprintmd(f\"The two santas visited **{len(two_santas)}** unique places.\")\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"the two santas visited {len(two_santas)} unique places\")\n\nfor name, santa in zip((\"Bob\", \"Alice\"), (santa_1, santa_2)):\n    x, y = zip(*santa)\n    ax.plot(x,y, alpha=0.8, label=f\"Santa_{name}\")\nax.legend();\n\n\nThe two santas visited 2639 unique places."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "href": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "title": "Advent of Code 2015",
    "section": "Day 4: The Ideal Stocking Stuffer",
    "text": "Day 4: The Ideal Stocking Stuffer\n#\n\n\nCode\ninp4 = \"bgvyzdsv\"\ntest4_1 = \"abcdef\" #609043\ntest4_2 = \"pqrstuv\" # 1048970\n\ndef make_hash(txt):\n    return hashlib.md5(txt.encode(\"utf\")).hexdigest()\n\n\n\nFalse\n\n\n\n\nCode\ndef day_4_1(inp=inp4, s=\"None\", target=\"00000\", i=0) -&gt; int:\n    while not s.startswith(target):\n        i += 1\n        txt = inp + str(i)\n        s = make_hash(txt)\n    printmd(f\"_{inp}_ target at position **{i:,}** ({s})\")\n    return i\n\nassert day_4_1(\"abcdef\") == 609043    # tests are always a good idea\nassert day_4_1(\"pqrstuv\") == 1048970    \n\nday_4_1()\n\n\nabcdef target at position 609,043 (000001dbbfa3a5c83a2d506429c7b00e)\n\n\npqrstuv target at position 1,048,970 (000006136ef2ff3b291c85725f17325c)\n\n\nbgvyzdsv target at position 254,575 (000004b30d481662b9cb0c105f6549b2)\n\n\n254575\n\n\nPart two just changes the target sring to have one more zero so thanks to making part one a function this is easy:\n\n\nCode\nday_4_1(target=\"000000\")\n\n\nbgvyzdsv target at position 1,038,736 (000000b1b64bf5eb55aad89986126953)\n\n\n1038736"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "href": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "title": "Advent of Code 2015",
    "section": "Day 5: Doesn’t He Have Intern-Elves For This?",
    "text": "Day 5: Doesn’t He Have Intern-Elves For This?\n# We have a list of strings, and Santa has the following rules to figure out which ones are nice:\n\nat least three vowels (aeiou only), like aei, xazegov, or aeiouaeiouaeiou.\nat least one letter that appears twice in a row, like xx, abcdde (dd), or aabbccdd (aa, bb, cc, or dd).\ndoes not contain the strings ab, cd, pq, or xy, even if they are part of one of the other requirements.\n\n\n\nCode\nvowels = \"aeiou\"                        # need vowels\nbad_strings = [\"ab\", \"cd\", \"pq\", \"xy\"]  # don't want these\nregex = re.compile(r\"([a-zA-Z])\\1{1,}\") # search for 2+ letters in a row\n\ntest4 = [\"ugknbfddgicrmopn\", \"aaa\", \"jchzalrnumimnmhp\", \n        \"haegwjzuvuyypxyu\", \"dvszwmarrgswjxmb\"]\n\ndef is_nice_string(txt):\n    vowel_count = len([char for char in txt if char in vowels]) &gt;= 3\n    two_chars = len(re.findall(regex, txt)) &gt; 0\n    no_bad_str = True if (sum([s3d in txt for s in bad_strings]) == 0) else False\n    \n    return vowel_count and two_chars and no_bad_str\n\n[is_nice_string(t) for t in test4] #== [False, False, True, True, True]\n\n\n[True, True, False, False, False]\n\n\n\n\nCode\ninp5 = get_input(5).split(\"\\n\")\nprint(\"Number of nice strings: \", sum([is_nice_string(t) for t in inp5]))\n\n\nNumber of nice strings:  258\n\n\nfor part two, the rules have changed, a nice string has these properties:\n\nIt contains a pair of any two letters that appears at least twice in the string without overlapping, like xyxy (xy) or aabcdefgaa (aa), but not like aaa (aa, but it overlaps).\nIt contains at least one letter which repeats with exactly one letter between them, like xyx, abcdefeghi (efe), or even aaa.\n\n\nNote: the rest remains to be done\n\n\n\nCode\nregex_2char = re.compile(r\"([a-zA-Z])\\1{1,}\")\nregex_3char = re.compile(r\"([a-zA-Z])\\1{2,}\")\n\nfor txt in [\"aa\", \"aba\", \"aaa\"]:\n    print(re.findall(regex_2char, txt))\n\n\n['a']\n[]\n['a']"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "href": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "title": "Advent of Code 2015",
    "section": "Day 6: Probably a Fire Hazard",
    "text": "Day 6: Probably a Fire Hazard\n#"
  },
  {
    "objectID": "posts/sql_101/index.html",
    "href": "posts/sql_101/index.html",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "This notebook sql and matplotlibs its way to:\n\n\n\nBefore the sql even starts, I want to use jupyter notebooks + sql. I’m using duckdb inside jupyter to look at a few sql datasets.\nFirst up, make a new env and install the necessary libraries if necessary by:\nmamba create -n sql101 python=3.11 jupyterlab duckdb duckdb-engine jupysql matplotlib openpyxl plotly\nmamba active sql101\n\n\nOnce all the packages have been installed, setup jupyter to use duckdb. You don’t need to use all the three config options below, just noting some useful ones for future reference.\nthe %config SqlMagic.autopandas in particular is useful - the sql query is returned as a pandas dataframe. This is useful as even though right now I have a small dataset, on a real project duckdb can talk to remote databases, or query parquet files on the web, do a query and only pull down the subset of data selected in the query.\nAfter that, the pandas dataframe is ideal for plotting and other pythonic stuff.\n\n# Import jupysql Jupyter extension to create SQL cells\n%load_ext sql\n\n%config SqlMagic.displaycon = False  # hides con info\n%config SqlMagic.autopandas = True   # output as df\n#%config SqlMagic.feedback = False   # \n\n%sql duckdb://\n\n\n\n\n\nI’m using the Economist’s big mac gdp source data. Pandas can directly download the csv file into a dataframe which duckdb can read, but trying to stick with most sql here, so I’m downloading the csv file to disk and reading it with duckdb directly.\nWhy the big mac index? To calculate the big mac index we need to use correlated subqueries, which is a pretty advanced topic!\nThe final output figure Figure 1 is somewhere below, this notebook works through getting there slowly:\n\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/master/source-data/big-mac-source-data-v2.csv\"\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/january-2023-update/source-data/big-mac-source-data-v2.csv\"\n\nfname = \"big-mac-source-data-v2.csv\"\n\nwith open(fname, \"wb\") as file:\n    file.write(requests.get(url).content)\n\nWhen using %%sql cells in jupyter, put python variables in brackets like so: {{var_name}}.\nThe read_csv_auto should auto-magically read and convert the csv to a sql table:\n\n%%sql\n-- make a table bigmac from the csv file\ncreate table if not exists bigmac as select * from read_csv_auto('{{fname}}');\n\nselect * from bigmac limit 3; -- comments can go here too!\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n-0.264266\n4822.738983\n\n\n\n\n\n\n\nSo now we have a table inside a sql database.\n\n\n\n\n\n\nNote\n\n\n\nsql convention is to use a lot of CAPITALS, but for fast typing and a lack of an sql formatter, I’m going lowercase. Ideally your sql writing thingamjig should have a formatter which does that for you.\n\n\n\n\n\nData always has some issues, so taking a look:\n\n\nA sql database can have many tables, so its useful to take a look at whats there:\n\n%sqlcmd tables\n\n\n\n\nName\n\n\n\n\nbigmac\n\n\n\n\n\nThis should describe the table:\n\n%sql describe bigmac;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\nThe describe bigmac code should spit out table info, seems to be some kind of bug, but moving on, we can get the gist using the information schema:\n\n%%sql \nSELECT COLUMN_NAME, DATA_TYPE\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_NAME = 'bigmac';\n\n\n\n\n\n\n\n\ncolumn_name\ndata_type\n\n\n\n\n0\nname\nVARCHAR\n\n\n1\niso_a3\nVARCHAR\n\n\n2\ncurrency_code\nVARCHAR\n\n\n3\nlocal_price\nDOUBLE\n\n\n4\ndollar_ex\nDOUBLE\n\n\n5\nGDP_dollar\nDOUBLE\n\n\n6\nGDP_local\nDOUBLE\n\n\n7\ndate\nDATE\n\n\n\n\n\n\n\nThe above is not that useful, a more informative look which counts the values is:\n\n%sqlcmd profile --table 'bigmac'\n\n\n \n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\ncount\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n\n\nunique\n73\n72\n56\n655\n1473\n1141\n1141\n37\n1827\n1835\n1790\n\n\ntop\nArgentina\nARG\nEUR\nnan\nnan\nnan\nnan\n2018-07-01\nnan\nnan\nnan\n\n\nfreq\n37\n37\n350\nnan\nnan\nnan\nnan\n72\nnan\nnan\nnan\n\n\nmean\nnan\nnan\nnan\n7559.2864\n2870.1936\n25020.0618\n2615423.9573\nnan\n3.4740\n-0.1340\n26351.7044\n\n\nstd\nnan\nnan\nnan\n154285.8636\n58921.0338\n21466.5984\n9471890.5225\nnan\n1.2289\n0.3013\n19541.3083\n\n\nmin\nnan\nnan\nnan\n1.05\n0.3008\n689.826\n3004.341\nnan\n0.64\n-0.7409\n0.0022\n\n\n25%\nnan\nnan\nnan\n4.4200\n1.0000\n6598.8800\n33698.7640\nnan\n2.5118\n-0.3530\n8516.5585\n\n\n50%\nnan\nnan\nnan\n14.7500\n5.3375\n18167.3440\n70506.5530\nnan\n3.4129\n-0.1755\n23327.7564\n\n\n75%\nnan\nnan\nnan\n85.0000\n32.1055\n41469.7730\n358689.5130\nnan\n4.3132\n0.0220\n40981.9351\n\n\nmax\nnan\nnan\nnan\n4000000.0\n1600500.0\n102576.68\n85420189.717\nnan\n8.3117\n1.5347\n98893.859\n\n\n\n \n\n\n\n\n\nSo we have 3 rows without a GDP_dollar, and at least one local_price and dollar_ex of 0, so dropping those rows. We also need GDP_local to be able to adjust the bigmac index, so dropping any nulls in that row too.\n\n%%sql \ndelete from bigmac where \nGDP_dollar is null or GDP_local is null or local_price&lt;=0 or dollar_ex&lt;=0;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\nrows = %sql select count(*) as N from bigmac;\nf\"The data set has a total of {rows.N.loc[0]} rows.\"\n\n'The data set has a total of 1918 rows.'\n\n\nIt’s a pretty clean and tidy dataset, we did loose a few rows, which in a real world case might bear more investigation, but moving on…\n\n%sql select count(distinct name) as Countries, min(date), max(date) from bigmac;\n\n\n\n\n\n\n\n\nCountries\nmin(date)\nmax(date)\n\n\n\n\n0\n73\n2000-04-01\n2022-07-01\n\n\n\n\n\n\n\nWe have data for 73 countries from April 2000 to July 2022.\n\n\n\n\nAdding some columns by calculating new ones in sql. This is where the sql starts.\n\n\nWe want to get the US dollar price for a big mac in every country, which is easy as our data contains the local_price and dollar exchange rate. So we add a new column: dollar_price = local_price / dollar_ex.\nIn sql you can’t just add a column, you first have to add it with a value type. In pandas you can do this in one step: df[\"dollar_price\"] = df.local_price / df.dollar_ex.\nSo here we add a new col dollar_price and calc its value:\n\n%%sql\nalter table bigmac \n    add column if not exists dollar_price DOUBLE;\nupdate bigmac \n    set dollar_price = local_price / dollar_ex;\n\nselect * from bigmac limit 3;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n-0.264266\n4822.738983\n\n\n\n\n\n\n\nThe line if not exists is optional, but useful in this context as it prevents errors if I rerun the notebook.\n\n\n\nNow we want to account for purchasing power parity by divinding the dollar price with the base currencies price. The Economist uses five base currencies: ('USD', 'EUR', 'GBP', 'JPY', 'CNY')\nFor simplicity’s sake, I’ll stick with just USD. This is a easy calc to do in python, in SQL its a bit messy…, so I am using Correlated Subqueries in SQL to do this.\nThe dollar_price on each row is divided by the dollar_price in USD. Since we have multiple dates, the query below matches on both the date and the country code.\nThe inner query returns the US dollar_price for each date, so every row in the table gets divided by the right dates USD price.\n\n\n\n\n\n\nNote\n\n\n\nThis only works because for each unique date, there is only one row for USA.\n\n\nFinally, we minus the number by -1, as we divide the US price by its own, so its always at 1. By subtracting -1, we set that to zero, which makes it wasy to see how the other countries are over or under that.\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS USD DOUBLE;\n\nupdate bigmac as b1\n    set USD = dollar_price / \n            (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA') - 1;\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nI did a few random checks and looks like the formula did use the right USD price to calculate the USD offset.\n\n\n\nBig Mac adjusted per capita GDP is the GDP in local currency divided by the exchange rate as determined by big macs (price in local currency divivded by price in US).\nThe formula is: GDP_Local / (local_price / dollar_price)\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS GDP_bigmac DOUBLE;\n\nupdate bigmac as b1\n    set GDP_bigmac = GDP_local / \n            (local_price / (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA'));\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nAnd presto, we have a gdp per big mac, which for most countries is significantly different from each other. A couple of plots to eyeball this:\n\n\nCode\ndf = %sql select * from bigmac;\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nax.set_title(\"GDP Bigmac vs dollar\", loc=\"left\", weight=\"bold\")\n\nfor name in [\"Pakistan\", \"India\"]:\n    d = df.query(\"name == @name\")\n    ax.plot(d.date, d.GDP_bigmac, alpha=0.8, label=f\"{name}\")\n    ax.fill_between(d.date, d.GDP_bigmac, d.GDP_dollar, alpha=0.12)\n    #ax.plot(d.date, d.GDP_dollar, alpha=0.6, ls=\"--\")\n\nax.legend();\n\n\n\n\n\n\n\nCode\ncountry = \"Pakistan\"\ndf = %sql select * from bigmac where name in ('{{country}}')\nfig = px.line(df, x=\"date\", y=[\"GDP_dollar\", \"GDP_bigmac\"], \n              title=f\"{country}: GDP local vs bigmac\",\n              markers=True)\nfig.show()\n\n\n\n                                                \n\n\nThis is interesting, you can see the indian dollar even though its close to the Pakistan one in GDP_dollar terms, the big mac index GDP says it buys a lot more.\n\n\n\nThis uses the big mac adjusted per capita GDP price calculated above to get a adjusted dollar price for a bigmac. This looks very similar to first big mac index so left this calc for a future date.\nSee Calculating the adjusted index for details.\n\n\n\n\nThere are a lot of countries in the index, which will make the graph a bit messy, so to add some groups to the counties (and do more sql!) I’m going to use the world banks income dataset, primary to add regions and income groups.\n\ndf_income = pd.read_excel(\n    \"https://datacatalogfiles.worldbank.org/ddh-published/0037712/DR0090755/CLASS.xlsx\"\n)\n\ndf_income = (\n    df_income.dropna(subset=[\"Income group\", \"Economy\"])\n    .drop(columns=\"Lending category\")\n    .rename(columns={\"Code\": \"iso_a3\", \"Income group\": \"income_group\"})\n)\n\n# adding in the missing eurozone\neuro_row = [\"Eurozone\", \"EUZ\", \"Europe\", \"High income\"]\ndf_income.loc[len(df_income)] = euro_row\n\ndf_income.tail(3)\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n215\nSouth Africa\nZAF\nSub-Saharan Africa\nUpper middle income\n\n\n216\nZambia\nZMB\nSub-Saharan Africa\nLower middle income\n\n\n217\nEurozone\nEUZ\nEurope\nHigh income\n\n\n\n\n\n\n\nNo need to do this step, but for more sql goodness, lets add this as a table in our database so we can practice joining two tables:\n\n%%sql\ncreate table income as select * from df_income;\nselect * from income limit 3;\n\nRuntimeError: (duckdb.CatalogException) Catalog Error: Table with name \"income\" already exists!\n[SQL: create table income as select * from df_income;]\n(Background on this error at: https://sqlalche.me/e/20/f405)\nIf you need help solving this issue, send us a message: https://ploomber.io/community\n\n\n\n\nHere we join these two tables into a dataframe for plotting.\nI am using a left join as I want to keep all data in the bigmac table, and just add country info.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t write sql like below, use capitals and linebreaks!\n\n\n\ndf_all = %sql select * from bigmac left join income on bigmac.iso_a3 = income.iso_a3 order by bigmac.date asc;\ndf_all.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\nArgentina\nARG\nLatin America & Caribbean\nUpper middle income\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n\n\n\n\n\n\n\nEverything looks good here, we now have a dataframe which combines the two tables and is ready to plot.\n\n\n\n\nFor using in a future something.\n\ndf_all.to_parquet(\"bigmac_index_data_ko.parquet\")\n\n\n\n\nThis is going to be ugly, but its good matplotlib practice. First up, for the economist plot we just want the latest date:\n\ndf = df_all.query(\"date == date.max()\").sort_values(\"USD\")\ndf.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n1899\nRomania\nROU\nRON\n11.0\n4.82175\n14667.089\n6.102120e+04\n2022-07-01\n2.281329\n-0.557023\n28569.017768\nRomania\nROU\nEurope & Central Asia\nHigh income\n\n\n1876\nIndonesia\nIDN\nIDR\n35000.0\n14977.50000\n4356.560\n6.233566e+07\n2022-07-01\n2.336839\n-0.546245\n9172.246719\nIndonesia\nIDN\nEast Asia & Pacific\nUpper middle income\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(5, 10))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n\nax.set_title(\n    \"Big Mac Index\", loc=\"left\", weight=\"bold\", ha=\"left\", fontsize=15, x=-0.42\n)\n\nax.text(\n    x=-0.2,\n    y=0.915,\n    s=\"Unadjusted index for 2022\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.8,\n)\n\n\n# plotting the actual data\n\n# assigning each income group a color\ncolor_list = list(mcolors.TABLEAU_COLORS.values())\ncolors = {country: color_list[i] for i, country in enumerate(df.income_group.unique())}\n\n# the actual plot\nax.barh(\n    y=df.name,\n    width=df.USD,\n    alpha=0.85,\n    height=0.78,\n    color=[colors[income] for income in df.income_group],\n    zorder=2,\n)\n\n# x axis things\nax.xaxis.tick_top()\nax.set_xlabel(\n    \"                 &lt;-- Undervalued              Overvalued --&gt; \",\n    labelpad=10,\n    fontsize=11,\n)\nax.xaxis.set_label_position(\"top\")\n\n# fix and label the y axis\n# ax.set_yticklabels(df.name, ha=\"left\")\nax.yaxis.set_tick_params(pad=2, labelsize=9, bottom=False)\nax.set_ylim(-1, df.shape[0])\n\n\n# legend for income groups\nhandles = [mpatches.Patch(color=colors[i]) for i in colors]\nlabels = [f\"{i}\" for i in colors]\nax.legend(handles, labels, fontsize=8)\nax.grid(which=\"major\", axis=\"x\", color=\"#758D99\", alpha=0.5, zorder=1)\n\n# Set source text\nax.text(\n    x=-0.2,\n    y=0.08,\n    s=\"\"\"Source: \"Big Mac Index\" via economist.com\"\"\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.7,\n);\n\n\n\n\n\nFigure 1: Big Mac Index\n\n\n\n\nphew! That is a long image, with too many countries, and its a bit ugly, so if this was going to be used somewhere, I’d filter the countries and produce a more sensible sized graph.\n\n\n\nI should have probably gone with plotly first for this, but here goes:\n\nfig = px.bar(\n    df, x=\"USD\", y=\"name\", width=600, height=800, text=\"name\", color=\"income_group\"\n)\nfig.update_layout(title_text=\"Big Mac Index\", yaxis_categoryorder=\"total ascending\")\nfig.show()\n\n\n                                                \n\n\n\n\nThis is a bit messy again… but here goes some plotly practice:\n\n\nCode\nnames = df_all.name.unique()\nshow_countries = [\"Pakistan\", \"India\"]\n\nfig = px.scatter(df_all, x=\"date\", y=\"USD\", color=\"name\", opacity=0.8,\n                 trendline=\"lowess\")\n\n# add a text label to the end of each scatter plot\nmax_date = df_all.date.max()\ndf = df_all.query(\"date == @max_date\")\n\nfor name in df.name:\n    fig.add_trace(go.Scatter(opacity=0.6,\n        x=df.query(\"name == @name\").date,\n        y=df.query(\"name == @name\").USD + 0.15,\n        mode=\"text\",\n        legendgroup=name,showlegend=False,\n        name=name,\n        text=name,\n        textposition=\"top center\",\n    ))\n\n# turn off plots\nfig.for_each_trace(lambda trace: trace.update(visible='legendonly')\n                   if trace.name not in show_countries else ())\n\n#fig.for_each_trace(lambda trace: trace.update(opacity=0.09)\n#                   if trace.name not in show_countries else ())\n\nfig.add_hline(y=0, opacity=0.5, visible=True, line_width=1)\n\nfig.update_layout(title=\"Big Mac Index with country selector\", template=\"simple_white\")\nfig.update_yaxes(tickformat=\".0%\", dtick=0.2, range=[-0.7, 0.7], \n                 showspikes=True, spikethickness=0.5)\nfig.update_xaxes(showspikes=True, spikethickness=0.5)\n\n\nfig.add_trace(go.Scatter(\n    x=[2022],\n    y=[0.01],\n    mode=\"text\",\n    name=\"US dollar\",\n    text=[\"US dollar\"],\n    textposition=\"top center\"\n))\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nnames = df_all.name.unique()\nshow_countries = [\"Pakistan\"]\n\ncolors = [\"red\" if usd &lt; 0 else \"#1f77b4\" for usd in df_all.USD]\nfig = px.scatter(df_all, x=\"date\", y=\"USD\", opacity=0.05).update_traces(\n    marker=dict(color=colors))\n\n# add a text label to the end of each scatter plot\nmax_date = df_all.date.max()\ndf = df_all.query(\"date == @max_date\")\n\nfor name in show_countries:\n    dff = df.query(\"name == @name\")\n\n    fig.add_traces(\n        px.line(df_all.query(\"name==@name\"), \n                   x=\"date\", y=\"USD\").data[:]\n    )\n    \n    fig.add_trace(go.Scatter(opacity=0.6,\n        x=dff.date,\n        y=dff.USD + 0.15,\n        mode=\"text\",\n        legendgroup=name,showlegend=False,\n        name=name,\n        text=name,\n        textposition=\"top center\",\n    ))\n\n\n# turn off plots\n#fig.for_each_trace(lambda trace: trace.update(visible='legendonly')\n#                   if trace.name not in show_countries else ())\n\n#fig.for_each_trace(lambda trace: trace.update(opacity=0.09)\n#                   if trace.name not in show_countries else ())\n\nfig.add_hline(y=0, opacity=0.5, visible=True, line_width=0.8)\n\nfig.update_layout(title=\"Big Mac Index highlighting a country\", template=\"simple_white\")\nfig.update_yaxes(tickformat=\".0%\", dtick=0.2, range=[-0.7, 0.7], \n                 showspikes=True, spikethickness=0.5)\nfig.update_xaxes(showspikes=True, spikethickness=0.5)\n\n\nfig.add_trace(go.Scatter(\n    x=[2022],\n    y=[0.01],\n    mode=\"text\",\n    name=\"US dollar\",\n    text=[\"US dollar\"],\n    textposition=\"top center\"\n))\n\nfig.show()\n\n\n\n                                                \n\n\nThe above graph has some issues, to be fixed later. Looks like this would be simpler in Altair instead of plotly.\n\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=1, cols=2)\n\nfig = px.choropleth(df, color=\"USD\", locations=\"iso_a3\", \n                    title=\"Big Mac Index\", hover_name=\"name\")\nfig.update_layout(height=600, width=800)\nfig.show()\n\n\n                                                \n\n\n\n\n\n\nAltair seems useful for this too… trying it out\n\n\nCode\nimport altair as alt\n\nhighlight = alt.selection_point(on='mouseover', fields=['name'], nearest=True)\n\nbase = alt.Chart(df_all).encode(\n    x='date:T',\n    y='USD:Q',\n    color='name:N',\n    tooltip=['name', 'date','USD']\n)\n\npoints = base.mark_circle().encode(\n    opacity=alt.value(0.01)\n).add_params(\n    highlight\n).properties(\n    width=600\n)\n\nlines = base.mark_line().encode(\n    size=alt.condition(~highlight, alt.value(0.2), alt.value(3))\n)\n\npoints + lines\n\n\n\n\n\n\n\n\n\nname = \"Pakistan\"\ndf = df_all.query(\"name == @name\")\n\nchart = alt.Chart(df).mark_area(\n    opacity=0.7, color=\"red\", line=True).encode(\n    x=\"date:T\",\n    y=alt.Y(\"USD:Q\")\n)\n\nchart\n\n\n\n\n\n\n\n\n\n\nCan play around a bit more with this data… but heaps enough for now. The economist has a great dashboard to this very simple dataset, so a good future dashboarding exercise is to make something which lookls like that."
  },
  {
    "objectID": "posts/sql_101/index.html#jupyter-duckdb-setup-for-sql",
    "href": "posts/sql_101/index.html#jupyter-duckdb-setup-for-sql",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Before the sql even starts, I want to use jupyter notebooks + sql. I’m using duckdb inside jupyter to look at a few sql datasets.\nFirst up, make a new env and install the necessary libraries if necessary by:\nmamba create -n sql101 python=3.11 jupyterlab duckdb duckdb-engine jupysql matplotlib openpyxl plotly\nmamba active sql101\n\n\nOnce all the packages have been installed, setup jupyter to use duckdb. You don’t need to use all the three config options below, just noting some useful ones for future reference.\nthe %config SqlMagic.autopandas in particular is useful - the sql query is returned as a pandas dataframe. This is useful as even though right now I have a small dataset, on a real project duckdb can talk to remote databases, or query parquet files on the web, do a query and only pull down the subset of data selected in the query.\nAfter that, the pandas dataframe is ideal for plotting and other pythonic stuff.\n\n# Import jupysql Jupyter extension to create SQL cells\n%load_ext sql\n\n%config SqlMagic.displaycon = False  # hides con info\n%config SqlMagic.autopandas = True   # output as df\n#%config SqlMagic.feedback = False   # \n\n%sql duckdb://"
  },
  {
    "objectID": "posts/sql_101/index.html#data---big-mac-index",
    "href": "posts/sql_101/index.html#data---big-mac-index",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "I’m using the Economist’s big mac gdp source data. Pandas can directly download the csv file into a dataframe which duckdb can read, but trying to stick with most sql here, so I’m downloading the csv file to disk and reading it with duckdb directly.\nWhy the big mac index? To calculate the big mac index we need to use correlated subqueries, which is a pretty advanced topic!\nThe final output figure Figure 1 is somewhere below, this notebook works through getting there slowly:\n\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/master/source-data/big-mac-source-data-v2.csv\"\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/january-2023-update/source-data/big-mac-source-data-v2.csv\"\n\nfname = \"big-mac-source-data-v2.csv\"\n\nwith open(fname, \"wb\") as file:\n    file.write(requests.get(url).content)\n\nWhen using %%sql cells in jupyter, put python variables in brackets like so: {{var_name}}.\nThe read_csv_auto should auto-magically read and convert the csv to a sql table:\n\n%%sql\n-- make a table bigmac from the csv file\ncreate table if not exists bigmac as select * from read_csv_auto('{{fname}}');\n\nselect * from bigmac limit 3; -- comments can go here too!\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n-0.264266\n4822.738983\n\n\n\n\n\n\n\nSo now we have a table inside a sql database.\n\n\n\n\n\n\nNote\n\n\n\nsql convention is to use a lot of CAPITALS, but for fast typing and a lack of an sql formatter, I’m going lowercase. Ideally your sql writing thingamjig should have a formatter which does that for you."
  },
  {
    "objectID": "posts/sql_101/index.html#eda-and-data-cleanup",
    "href": "posts/sql_101/index.html#eda-and-data-cleanup",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Data always has some issues, so taking a look:\n\n\nA sql database can have many tables, so its useful to take a look at whats there:\n\n%sqlcmd tables\n\n\n\n\nName\n\n\n\n\nbigmac\n\n\n\n\n\nThis should describe the table:\n\n%sql describe bigmac;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\nThe describe bigmac code should spit out table info, seems to be some kind of bug, but moving on, we can get the gist using the information schema:\n\n%%sql \nSELECT COLUMN_NAME, DATA_TYPE\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_NAME = 'bigmac';\n\n\n\n\n\n\n\n\ncolumn_name\ndata_type\n\n\n\n\n0\nname\nVARCHAR\n\n\n1\niso_a3\nVARCHAR\n\n\n2\ncurrency_code\nVARCHAR\n\n\n3\nlocal_price\nDOUBLE\n\n\n4\ndollar_ex\nDOUBLE\n\n\n5\nGDP_dollar\nDOUBLE\n\n\n6\nGDP_local\nDOUBLE\n\n\n7\ndate\nDATE\n\n\n\n\n\n\n\nThe above is not that useful, a more informative look which counts the values is:\n\n%sqlcmd profile --table 'bigmac'\n\n\n \n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\ncount\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n1918\n\n\nunique\n73\n72\n56\n655\n1473\n1141\n1141\n37\n1827\n1835\n1790\n\n\ntop\nArgentina\nARG\nEUR\nnan\nnan\nnan\nnan\n2018-07-01\nnan\nnan\nnan\n\n\nfreq\n37\n37\n350\nnan\nnan\nnan\nnan\n72\nnan\nnan\nnan\n\n\nmean\nnan\nnan\nnan\n7559.2864\n2870.1936\n25020.0618\n2615423.9573\nnan\n3.4740\n-0.1340\n26351.7044\n\n\nstd\nnan\nnan\nnan\n154285.8636\n58921.0338\n21466.5984\n9471890.5225\nnan\n1.2289\n0.3013\n19541.3083\n\n\nmin\nnan\nnan\nnan\n1.05\n0.3008\n689.826\n3004.341\nnan\n0.64\n-0.7409\n0.0022\n\n\n25%\nnan\nnan\nnan\n4.4200\n1.0000\n6598.8800\n33698.7640\nnan\n2.5118\n-0.3530\n8516.5585\n\n\n50%\nnan\nnan\nnan\n14.7500\n5.3375\n18167.3440\n70506.5530\nnan\n3.4129\n-0.1755\n23327.7564\n\n\n75%\nnan\nnan\nnan\n85.0000\n32.1055\n41469.7730\n358689.5130\nnan\n4.3132\n0.0220\n40981.9351\n\n\nmax\nnan\nnan\nnan\n4000000.0\n1600500.0\n102576.68\n85420189.717\nnan\n8.3117\n1.5347\n98893.859\n\n\n\n \n\n\n\n\n\nSo we have 3 rows without a GDP_dollar, and at least one local_price and dollar_ex of 0, so dropping those rows. We also need GDP_local to be able to adjust the bigmac index, so dropping any nulls in that row too.\n\n%%sql \ndelete from bigmac where \nGDP_dollar is null or GDP_local is null or local_price&lt;=0 or dollar_ex&lt;=0;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\nrows = %sql select count(*) as N from bigmac;\nf\"The data set has a total of {rows.N.loc[0]} rows.\"\n\n'The data set has a total of 1918 rows.'\n\n\nIt’s a pretty clean and tidy dataset, we did loose a few rows, which in a real world case might bear more investigation, but moving on…\n\n%sql select count(distinct name) as Countries, min(date), max(date) from bigmac;\n\n\n\n\n\n\n\n\nCountries\nmin(date)\nmax(date)\n\n\n\n\n0\n73\n2000-04-01\n2022-07-01\n\n\n\n\n\n\n\nWe have data for 73 countries from April 2000 to July 2022."
  },
  {
    "objectID": "posts/sql_101/index.html#calculations-in-sql",
    "href": "posts/sql_101/index.html#calculations-in-sql",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Adding some columns by calculating new ones in sql. This is where the sql starts.\n\n\nWe want to get the US dollar price for a big mac in every country, which is easy as our data contains the local_price and dollar exchange rate. So we add a new column: dollar_price = local_price / dollar_ex.\nIn sql you can’t just add a column, you first have to add it with a value type. In pandas you can do this in one step: df[\"dollar_price\"] = df.local_price / df.dollar_ex.\nSo here we add a new col dollar_price and calc its value:\n\n%%sql\nalter table bigmac \n    add column if not exists dollar_price DOUBLE;\nupdate bigmac \n    set dollar_price = local_price / dollar_ex;\n\nselect * from bigmac limit 3;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n-0.264266\n4822.738983\n\n\n\n\n\n\n\nThe line if not exists is optional, but useful in this context as it prevents errors if I rerun the notebook.\n\n\n\nNow we want to account for purchasing power parity by divinding the dollar price with the base currencies price. The Economist uses five base currencies: ('USD', 'EUR', 'GBP', 'JPY', 'CNY')\nFor simplicity’s sake, I’ll stick with just USD. This is a easy calc to do in python, in SQL its a bit messy…, so I am using Correlated Subqueries in SQL to do this.\nThe dollar_price on each row is divided by the dollar_price in USD. Since we have multiple dates, the query below matches on both the date and the country code.\nThe inner query returns the US dollar_price for each date, so every row in the table gets divided by the right dates USD price.\n\n\n\n\n\n\nNote\n\n\n\nThis only works because for each unique date, there is only one row for USA.\n\n\nFinally, we minus the number by -1, as we divide the US price by its own, so its always at 1. By subtracting -1, we set that to zero, which makes it wasy to see how the other countries are over or under that.\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS USD DOUBLE;\n\nupdate bigmac as b1\n    set USD = dollar_price / \n            (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA') - 1;\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nI did a few random checks and looks like the formula did use the right USD price to calculate the USD offset.\n\n\n\nBig Mac adjusted per capita GDP is the GDP in local currency divided by the exchange rate as determined by big macs (price in local currency divivded by price in US).\nThe formula is: GDP_Local / (local_price / dollar_price)\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS GDP_bigmac DOUBLE;\n\nupdate bigmac as b1\n    set GDP_bigmac = GDP_local / \n            (local_price / (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA'));\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nAnd presto, we have a gdp per big mac, which for most countries is significantly different from each other. A couple of plots to eyeball this:\n\n\nCode\ndf = %sql select * from bigmac;\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nax.set_title(\"GDP Bigmac vs dollar\", loc=\"left\", weight=\"bold\")\n\nfor name in [\"Pakistan\", \"India\"]:\n    d = df.query(\"name == @name\")\n    ax.plot(d.date, d.GDP_bigmac, alpha=0.8, label=f\"{name}\")\n    ax.fill_between(d.date, d.GDP_bigmac, d.GDP_dollar, alpha=0.12)\n    #ax.plot(d.date, d.GDP_dollar, alpha=0.6, ls=\"--\")\n\nax.legend();\n\n\n\n\n\n\n\nCode\ncountry = \"Pakistan\"\ndf = %sql select * from bigmac where name in ('{{country}}')\nfig = px.line(df, x=\"date\", y=[\"GDP_dollar\", \"GDP_bigmac\"], \n              title=f\"{country}: GDP local vs bigmac\",\n              markers=True)\nfig.show()\n\n\n\n                                                \n\n\nThis is interesting, you can see the indian dollar even though its close to the Pakistan one in GDP_dollar terms, the big mac index GDP says it buys a lot more.\n\n\n\nThis uses the big mac adjusted per capita GDP price calculated above to get a adjusted dollar price for a bigmac. This looks very similar to first big mac index so left this calc for a future date.\nSee Calculating the adjusted index for details."
  },
  {
    "objectID": "posts/sql_101/index.html#moar-data---adding-in-country-data",
    "href": "posts/sql_101/index.html#moar-data---adding-in-country-data",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "There are a lot of countries in the index, which will make the graph a bit messy, so to add some groups to the counties (and do more sql!) I’m going to use the world banks income dataset, primary to add regions and income groups.\n\ndf_income = pd.read_excel(\n    \"https://datacatalogfiles.worldbank.org/ddh-published/0037712/DR0090755/CLASS.xlsx\"\n)\n\ndf_income = (\n    df_income.dropna(subset=[\"Income group\", \"Economy\"])\n    .drop(columns=\"Lending category\")\n    .rename(columns={\"Code\": \"iso_a3\", \"Income group\": \"income_group\"})\n)\n\n# adding in the missing eurozone\neuro_row = [\"Eurozone\", \"EUZ\", \"Europe\", \"High income\"]\ndf_income.loc[len(df_income)] = euro_row\n\ndf_income.tail(3)\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n215\nSouth Africa\nZAF\nSub-Saharan Africa\nUpper middle income\n\n\n216\nZambia\nZMB\nSub-Saharan Africa\nLower middle income\n\n\n217\nEurozone\nEUZ\nEurope\nHigh income\n\n\n\n\n\n\n\nNo need to do this step, but for more sql goodness, lets add this as a table in our database so we can practice joining two tables:\n\n%%sql\ncreate table income as select * from df_income;\nselect * from income limit 3;\n\nRuntimeError: (duckdb.CatalogException) Catalog Error: Table with name \"income\" already exists!\n[SQL: create table income as select * from df_income;]\n(Background on this error at: https://sqlalche.me/e/20/f405)\nIf you need help solving this issue, send us a message: https://ploomber.io/community\n\n\n\n\nHere we join these two tables into a dataframe for plotting.\nI am using a left join as I want to keep all data in the bigmac table, and just add country info.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t write sql like below, use capitals and linebreaks!\n\n\n\ndf_all = %sql select * from bigmac left join income on bigmac.iso_a3 = income.iso_a3 order by bigmac.date asc;\ndf_all.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\nArgentina\nARG\nLatin America & Caribbean\nUpper middle income\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n\n\n\n\n\n\n\nEverything looks good here, we now have a dataframe which combines the two tables and is ready to plot."
  },
  {
    "objectID": "posts/sql_101/index.html#save-data-to-disk",
    "href": "posts/sql_101/index.html#save-data-to-disk",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "For using in a future something.\n\ndf_all.to_parquet(\"bigmac_index_data_ko.parquet\")"
  },
  {
    "objectID": "posts/sql_101/index.html#big-mac-index-chart-using-matplotlib",
    "href": "posts/sql_101/index.html#big-mac-index-chart-using-matplotlib",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "This is going to be ugly, but its good matplotlib practice. First up, for the economist plot we just want the latest date:\n\ndf = df_all.query(\"date == date.max()\").sort_values(\"USD\")\ndf.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n1899\nRomania\nROU\nRON\n11.0\n4.82175\n14667.089\n6.102120e+04\n2022-07-01\n2.281329\n-0.557023\n28569.017768\nRomania\nROU\nEurope & Central Asia\nHigh income\n\n\n1876\nIndonesia\nIDN\nIDR\n35000.0\n14977.50000\n4356.560\n6.233566e+07\n2022-07-01\n2.336839\n-0.546245\n9172.246719\nIndonesia\nIDN\nEast Asia & Pacific\nUpper middle income\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(5, 10))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n\nax.set_title(\n    \"Big Mac Index\", loc=\"left\", weight=\"bold\", ha=\"left\", fontsize=15, x=-0.42\n)\n\nax.text(\n    x=-0.2,\n    y=0.915,\n    s=\"Unadjusted index for 2022\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.8,\n)\n\n\n# plotting the actual data\n\n# assigning each income group a color\ncolor_list = list(mcolors.TABLEAU_COLORS.values())\ncolors = {country: color_list[i] for i, country in enumerate(df.income_group.unique())}\n\n# the actual plot\nax.barh(\n    y=df.name,\n    width=df.USD,\n    alpha=0.85,\n    height=0.78,\n    color=[colors[income] for income in df.income_group],\n    zorder=2,\n)\n\n# x axis things\nax.xaxis.tick_top()\nax.set_xlabel(\n    \"                 &lt;-- Undervalued              Overvalued --&gt; \",\n    labelpad=10,\n    fontsize=11,\n)\nax.xaxis.set_label_position(\"top\")\n\n# fix and label the y axis\n# ax.set_yticklabels(df.name, ha=\"left\")\nax.yaxis.set_tick_params(pad=2, labelsize=9, bottom=False)\nax.set_ylim(-1, df.shape[0])\n\n\n# legend for income groups\nhandles = [mpatches.Patch(color=colors[i]) for i in colors]\nlabels = [f\"{i}\" for i in colors]\nax.legend(handles, labels, fontsize=8)\nax.grid(which=\"major\", axis=\"x\", color=\"#758D99\", alpha=0.5, zorder=1)\n\n# Set source text\nax.text(\n    x=-0.2,\n    y=0.08,\n    s=\"\"\"Source: \"Big Mac Index\" via economist.com\"\"\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.7,\n);\n\n\n\n\n\nFigure 1: Big Mac Index\n\n\n\n\nphew! That is a long image, with too many countries, and its a bit ugly, so if this was going to be used somewhere, I’d filter the countries and produce a more sensible sized graph."
  },
  {
    "objectID": "posts/sql_101/index.html#big-mac-index-using-plotly",
    "href": "posts/sql_101/index.html#big-mac-index-using-plotly",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "I should have probably gone with plotly first for this, but here goes:\n\nfig = px.bar(\n    df, x=\"USD\", y=\"name\", width=600, height=800, text=\"name\", color=\"income_group\"\n)\nfig.update_layout(title_text=\"Big Mac Index\", yaxis_categoryorder=\"total ascending\")\nfig.show()\n\n\n                                                \n\n\n\n\nThis is a bit messy again… but here goes some plotly practice:\n\n\nCode\nnames = df_all.name.unique()\nshow_countries = [\"Pakistan\", \"India\"]\n\nfig = px.scatter(df_all, x=\"date\", y=\"USD\", color=\"name\", opacity=0.8,\n                 trendline=\"lowess\")\n\n# add a text label to the end of each scatter plot\nmax_date = df_all.date.max()\ndf = df_all.query(\"date == @max_date\")\n\nfor name in df.name:\n    fig.add_trace(go.Scatter(opacity=0.6,\n        x=df.query(\"name == @name\").date,\n        y=df.query(\"name == @name\").USD + 0.15,\n        mode=\"text\",\n        legendgroup=name,showlegend=False,\n        name=name,\n        text=name,\n        textposition=\"top center\",\n    ))\n\n# turn off plots\nfig.for_each_trace(lambda trace: trace.update(visible='legendonly')\n                   if trace.name not in show_countries else ())\n\n#fig.for_each_trace(lambda trace: trace.update(opacity=0.09)\n#                   if trace.name not in show_countries else ())\n\nfig.add_hline(y=0, opacity=0.5, visible=True, line_width=1)\n\nfig.update_layout(title=\"Big Mac Index with country selector\", template=\"simple_white\")\nfig.update_yaxes(tickformat=\".0%\", dtick=0.2, range=[-0.7, 0.7], \n                 showspikes=True, spikethickness=0.5)\nfig.update_xaxes(showspikes=True, spikethickness=0.5)\n\n\nfig.add_trace(go.Scatter(\n    x=[2022],\n    y=[0.01],\n    mode=\"text\",\n    name=\"US dollar\",\n    text=[\"US dollar\"],\n    textposition=\"top center\"\n))\n\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nnames = df_all.name.unique()\nshow_countries = [\"Pakistan\"]\n\ncolors = [\"red\" if usd &lt; 0 else \"#1f77b4\" for usd in df_all.USD]\nfig = px.scatter(df_all, x=\"date\", y=\"USD\", opacity=0.05).update_traces(\n    marker=dict(color=colors))\n\n# add a text label to the end of each scatter plot\nmax_date = df_all.date.max()\ndf = df_all.query(\"date == @max_date\")\n\nfor name in show_countries:\n    dff = df.query(\"name == @name\")\n\n    fig.add_traces(\n        px.line(df_all.query(\"name==@name\"), \n                   x=\"date\", y=\"USD\").data[:]\n    )\n    \n    fig.add_trace(go.Scatter(opacity=0.6,\n        x=dff.date,\n        y=dff.USD + 0.15,\n        mode=\"text\",\n        legendgroup=name,showlegend=False,\n        name=name,\n        text=name,\n        textposition=\"top center\",\n    ))\n\n\n# turn off plots\n#fig.for_each_trace(lambda trace: trace.update(visible='legendonly')\n#                   if trace.name not in show_countries else ())\n\n#fig.for_each_trace(lambda trace: trace.update(opacity=0.09)\n#                   if trace.name not in show_countries else ())\n\nfig.add_hline(y=0, opacity=0.5, visible=True, line_width=0.8)\n\nfig.update_layout(title=\"Big Mac Index highlighting a country\", template=\"simple_white\")\nfig.update_yaxes(tickformat=\".0%\", dtick=0.2, range=[-0.7, 0.7], \n                 showspikes=True, spikethickness=0.5)\nfig.update_xaxes(showspikes=True, spikethickness=0.5)\n\n\nfig.add_trace(go.Scatter(\n    x=[2022],\n    y=[0.01],\n    mode=\"text\",\n    name=\"US dollar\",\n    text=[\"US dollar\"],\n    textposition=\"top center\"\n))\n\nfig.show()\n\n\n\n                                                \n\n\nThe above graph has some issues, to be fixed later. Looks like this would be simpler in Altair instead of plotly.\n\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=1, cols=2)\n\nfig = px.choropleth(df, color=\"USD\", locations=\"iso_a3\", \n                    title=\"Big Mac Index\", hover_name=\"name\")\nfig.update_layout(height=600, width=800)\nfig.show()"
  },
  {
    "objectID": "posts/sql_101/index.html#altair",
    "href": "posts/sql_101/index.html#altair",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Altair seems useful for this too… trying it out\n\n\nCode\nimport altair as alt\n\nhighlight = alt.selection_point(on='mouseover', fields=['name'], nearest=True)\n\nbase = alt.Chart(df_all).encode(\n    x='date:T',\n    y='USD:Q',\n    color='name:N',\n    tooltip=['name', 'date','USD']\n)\n\npoints = base.mark_circle().encode(\n    opacity=alt.value(0.01)\n).add_params(\n    highlight\n).properties(\n    width=600\n)\n\nlines = base.mark_line().encode(\n    size=alt.condition(~highlight, alt.value(0.2), alt.value(3))\n)\n\npoints + lines\n\n\n\n\n\n\n\n\n\nname = \"Pakistan\"\ndf = df_all.query(\"name == @name\")\n\nchart = alt.Chart(df).mark_area(\n    opacity=0.7, color=\"red\", line=True).encode(\n    x=\"date:T\",\n    y=alt.Y(\"USD:Q\")\n)\n\nchart"
  },
  {
    "objectID": "posts/sql_101/index.html#the-end",
    "href": "posts/sql_101/index.html#the-end",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Can play around a bit more with this data… but heaps enough for now. The economist has a great dashboard to this very simple dataset, so a good future dashboarding exercise is to make something which lookls like that."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KO",
    "section": "",
    "text": "SQL 101 with duckdb and jupysql\n\n\n\n\n\n\n\npython\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nThe largest rectangle in a histogram\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nQuarto Jupyter notebook test\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nWindows 10/11 Setup\n\n\n\n\n\n\n\nwindows\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2015\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\na verbose solve of AOC 2015\n\n\n\n\n\n\nAug 10, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nFloodfill algorithim\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2018\n\n\nKO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/windows.html",
    "href": "posts/windows.html",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC.\n\n\n\ninstall scoop to easily install programs by running this in powershell:\nSet-ExecutionPolicy RemoteSigned -Scope CurrentUser # Optional: Needed to run a remote script the first time\nirm get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable.\n\n\n\n\nFollow the real wsl instructions, my notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nInstall the latest linux kernel update package.\nNow install debian from the windows store.\nAnd presto, you have a VM running inside a linux container, and its easy to have multiple ones setup for different things."
  },
  {
    "objectID": "posts/windows.html#factory-reset",
    "href": "posts/windows.html#factory-reset",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC."
  },
  {
    "objectID": "posts/windows.html#setup",
    "href": "posts/windows.html#setup",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "install scoop to easily install programs by running this in powershell:\nSet-ExecutionPolicy RemoteSigned -Scope CurrentUser # Optional: Needed to run a remote script the first time\nirm get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable."
  },
  {
    "objectID": "posts/windows.html#wsl-2",
    "href": "posts/windows.html#wsl-2",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Follow the real wsl instructions, my notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nInstall the latest linux kernel update package.\nNow install debian from the windows store.\nAnd presto, you have a VM running inside a linux container, and its easy to have multiple ones setup for different things."
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html",
    "href": "posts/code/quarto_notebook_test.html",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#polar-axis",
    "href": "posts/code/quarto_notebook_test.html#polar-axis",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "href": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "title": "Quarto Jupyter notebook test",
    "section": "Lets try some seaborn…",
    "text": "Lets try some seaborn…\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#testing",
    "href": "posts/code/quarto_notebook_test.html#testing",
    "title": "Quarto Jupyter notebook test",
    "section": "testing…",
    "text": "testing…\nwhy isn’t quarto creating a _freeze directory? What stops it?"
  },
  {
    "objectID": "posts/algorithims/flood_fill.html",
    "href": "posts/algorithims/flood_fill.html",
    "title": "Floodfill algorithim",
    "section": "",
    "text": "Flood Fill is a way to to visit every point in a bounded region. This makes it useful for many purposes. In this notebook I implement the “bucket fill” flood fill algo.\nFirst up generating a grid to flood fill:\nfill = np.random.randint(0, 2, size=(128,128), dtype=\"int\")\nfill\n\narray([[0, 1, 0, ..., 0, 1, 0],\n       [1, 1, 0, ..., 0, 0, 1],\n       [1, 1, 0, ..., 1, 0, 0],\n       ...,\n       [0, 1, 0, ..., 1, 1, 1],\n       [1, 1, 0, ..., 0, 1, 1],\n       [1, 0, 0, ..., 0, 1, 0]])\nEyeballing this grid visually:\nplt.imshow(fill);"
  },
  {
    "objectID": "posts/algorithims/flood_fill.html#the-floodfill-algo",
    "href": "posts/algorithims/flood_fill.html#the-floodfill-algo",
    "title": "Floodfill algorithim",
    "section": "the floodfill algo",
    "text": "the floodfill algo\nThe below function is a recursive implementation of flood fill - it will flood fill a a single region from 1 val to another:\n\ndef flood_fill(x, y, old, new):\n    \"\"\"takes in a x,y position from where to flood fill, the old val to change from and the new val\n    to change too, and then does so on a to_fill array\"\"\"\n    \n    if fill[x][y] != old or fill[x][y] == new:\n        return\n    \n    fill[x][y] = new\n    \n    max_x = len(fill) - 1\n    max_y = len(fill) - 1\n    \n    if x &gt; 0: # go left\n        flood_fill(x-1, y, old, new)\n    \n    if x &lt; max_x: # go right\n        flood_fill(x+1, y, old, new)\n        \n    if y &gt; 0: # go down\n        flood_fill(x, y-1, old, new)\n    if y &lt; max_y: # go up\n        flood_fill(x, y+1, old, new)\n\nHere, I flood fill the fill, converting all the 1’s to 8’s.\n\nregion_count = 0\n\nimagelist = list()\nimagelist.append(fill)\n\nfor i in range(len(fill)):\n    for j in range(len(fill[0])):\n        if fill[i][j] == 1:\n            flood_fill(i, j, 1, 8)\n            region_count += 1\n            imagelist.append(fill)\n            if region_count % 100 == 0:\n                plt.imshow(imagelist[region_count])\n                plt.title(f\"Flood Fill by region\")\n                plt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Trying out Quarto as a code blog."
  },
  {
    "objectID": "about.html#a-future-about",
    "href": "about.html#a-future-about",
    "title": "About",
    "section": "",
    "text": "Trying out Quarto as a code blog."
  }
]