[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Trying out Quarto as a code blog."
  },
  {
    "objectID": "about.html#a-future-about",
    "href": "about.html#a-future-about",
    "title": "About",
    "section": "",
    "text": "Trying out Quarto as a code blog."
  },
  {
    "objectID": "posts/sql_101/index.html",
    "href": "posts/sql_101/index.html",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Before the sql even starts, I want to use jupyter notebooks + sql. I’m using duckdb inside jupyter to look at a few sql datasets.\nFirst up, make a new env and install the necessary libraries if necessary by:\nmamba create -n sql101 python=3.11 jupyterlab duckdb duckdb-engine jupysql matplotlib openpyxl plotly\nmamba active sql101\n\n\nOnce all the packages have been installed, setup jupyter to use duckdb. You don’t need to use all the three config options below, just noting some useful ones for future reference.\nthe %config SqlMagic.autopandas in particular is useful - the sql query is returned as a pandas dataframe. This is useful as even though right now I have a small dataset, on a real project duckdb can talk to remote databases, or query parquet files on the web, do a query and only pull down the subset of data selected in the query.\nAfter that, the pandas dataframe is ideal for plotting and other pythonic stuff.\n\n# Import jupysql Jupyter extension to create SQL cells\n%load_ext sql\n\n%config SqlMagic.displaycon = False  # hides con info\n%config SqlMagic.autopandas = True   # output as df\n#%config SqlMagic.feedback = False   # \n\n%sql duckdb://\n\n\n\n\n\nI’m using the Economist’s big mac gdp source data. Pandas can directly download the csv file into a dataframe which duckdb can read, but trying to stick with most sql here, so I’m downloading the csv file to disk and reading it with duckdb directly.\nWhy the big mac index? To calculate the big mac index we need to use correlated subqueries, which is a pretty advanced topic!\nThe final output figure Figure 1 is somewhere below, this notebook works through getting there slowly:\n\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/master/source-data/big-mac-source-data-v2.csv\"\n\nfname = \"big-mac-source-data-v2.csv\"\n\nwith open(fname, \"wb\") as file:\n    file.write(requests.get(url).content)\n\nWhen using %%sql cells in jupyter, put python variables in brackets like so: {{var_name}}.\nThe read_csv_auto should auto-magically read and convert the csv to a sql table:\n\n%%sql\n-- make a table bigmac from the csv file\ncreate table if not exists bigmac as select * from read_csv_auto('{{fname}}');\n\nselect * from bigmac limit 3; -- comments can go here too!\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n\n\n\n\n\n\n\nSo now we have a table inside a sql database.\n\n\n\n\n\n\nNote\n\n\n\nsql convention is to use a lot of CAPITALS, but for fast typing and a lack of an sql formatter, I’m going lowercase. Ideally your sql writing thingamjig should have a formatter which does that for you.\n\n\n\n\n\nData always has some issues, so taking a look:\n\n\nA sql database can have many tables, so its useful to take a look at whats there:\n\n%sqlcmd tables\n\n\n\n\nName\n\n\n\n\nbigmac\n\n\n\n\n\nThis should describe the table:\n\n%sql describe bigmac;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\nThe describe bigmac code should spit out table info, seems to be some kind of bug, but moving on, we can get the gist using the information schema:\n\n%%sql \nSELECT COLUMN_NAME, DATA_TYPE\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_NAME = 'bigmac';\n\n\n\n\n\n\n\n\ncolumn_name\ndata_type\n\n\n\n\n0\nname\nVARCHAR\n\n\n1\niso_a3\nVARCHAR\n\n\n2\ncurrency_code\nVARCHAR\n\n\n3\nlocal_price\nDOUBLE\n\n\n4\ndollar_ex\nDOUBLE\n\n\n5\nGDP_dollar\nDOUBLE\n\n\n6\nGDP_local\nDOUBLE\n\n\n7\ndate\nDATE\n\n\n\n\n\n\n\nThe above is not that useful, a more informative look which counts the values is:\n\n%sqlcmd profile --table 'bigmac'\n\n\n \n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\n\n\n\n\ncount\n1946\n1946\n1946\n1946\n1946\n1943\n1918\n1946\n\n\nunique\n74\n73\n58\n669\n1497\n1157\n1141\n37\n\n\ntop\nArgentina\nARG\nEUR\nnan\nnan\nnan\nnan\n2021-07-01\n\n\nfreq\n37\n37\n351\nnan\nnan\nnan\nnan\n73\n\n\nmean\nnan\nnan\nnan\n15816.0895\n4722.5632\n24790.7531\n2615423.9573\nnan\n\n\nstd\nnan\nnan\nnan\n393903.7657\n100597.3791\n21425.9054\n9471890.5225\nnan\n\n\nmin\nnan\nnan\nnan\n0.0\n0.0\n689.826\n3004.341\nnan\n\n\n25%\nnan\nnan\nnan\n4.4500\n1.0035\n6577.2870\n33698.7640\nnan\n\n\n50%\nnan\nnan\nnan\n15.0000\n5.4650\n17815.1920\n70506.5530\nnan\n\n\n75%\nnan\nnan\nnan\n87.0000\n32.8750\n41409.6309\n358689.5130\nnan\n\n\nmax\nnan\nnan\nnan\n16020000.0\n3613989.071\n102576.68\n85420189.717\nnan\n\n\n\n \n\n\n\n\n\nSo we have 3 rows without a GDP_dollar, and at least one local_price and dollar_ex of 0, so dropping those rows. We also need GDP_local to be able to adjust the bigmac index, so dropping any nulls in that row too.\n\n%%sql \ndelete from bigmac where \nGDP_dollar is null or GDP_local is null or local_price&lt;=0 or dollar_ex&lt;=0;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\nrows = %sql select count(*) as N from bigmac;\nf\"The data set has a total of {rows.N.loc[0]} rows.\"\n\n'The data set has a total of 1918 rows.'\n\n\nIt’s a pretty clean and tidy dataset, we did loose a few rows, which in a real world case might bear more investigation, but moving on…\n\n%sql select count(distinct name) as Countries, min(date), max(date) from bigmac;\n\n\n\n\n\n\n\n\nCountries\nmin(date)\nmax(date)\n\n\n\n\n0\n73\n2000-04-01\n2022-07-01\n\n\n\n\n\n\n\nWe have data for 73 countries from April 2000 to July 2022.\n\n\n\n\nAdding some columns by calculating new ones in sql. This is where the sql starts.\n\n\nWe want to get the US dollar price for a big mac in every country, which is easy as our data contains the local_price and dollar exchange rate. So we add a new column: dollar_price = local_price / dollar_ex.\nIn sql you can’t just add a column, you first have to add it with a value type. In pandas you can do this in one step: df[\"dollar_price\"] = df.local_price / df.dollar_ex.\nSo here we add a new col dollar_price and calc its value:\n\n%%sql\nalter table bigmac \n    add column if not exists dollar_price DOUBLE;\nupdate bigmac \n    set dollar_price = local_price / dollar_ex;\n\nselect * from bigmac limit 3;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n\n\n\n\n\n\n\nThe line if not exists is optional, but useful in this context as it prevents errors if I rerun the notebook.\n\n\n\nNow we want to account for purchasing power parity by divinding the dollar price with the base currencies price. The Economist uses five base currencies: ('USD', 'EUR', 'GBP', 'JPY', 'CNY')\nFor simplicity’s sake, I’ll stick with just USD. This is a easy calc to do in python, in SQL its a bit messy…, so I am using Correlated Subqueries in SQL to do this.\nThe dollar_price on each row is divided by the dollar_price in USD. Since we have multiple dates, the query below matches on both the date and the country code.\nThe inner query returns the US dollar_price for each date, so every row in the table gets divided by the right dates USD price.\n\n\n\n\n\n\nNote\n\n\n\nThis only works because for each unique date, there is only one row for USA.\n\n\nFinally, we minus the number by -1, as we divide the US price by its own, so its always at 1. By subtracting -1, we set that to zero, which makes it wasy to see how the other countries are over or under that.\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS USD DOUBLE;\n\nupdate bigmac as b1\n    set USD = dollar_price / \n            (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA') - 1;\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n\n\n\n\n\n\n\nI did a few random checks and looks like the formula did use the right USD price to calculate the USD offset.\n\n\n\nBig Mac adjusted per capita GDP is the GDP in local currency divided by the exchange rate as determined by big macs (price in local currency divivded by price in US).\nThe formula is: GDP_Local / (local_price / dollar_price)\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS GDP_bigmac DOUBLE;\n\nupdate bigmac as b1\n    set GDP_bigmac = GDP_local / \n            (local_price / (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA'));\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nAnd presto, we have a gdp per big mac, which for most countries is significantly different from each other. A couple of plots to eyeball this:\n\n\nCode\ncountry = \"Pakistan\"\ndf = %sql select * from bigmac where name in ('{{country}}')\nfig = px.line(df, x=\"date\", y=[\"GDP_dollar\", \"GDP_bigmac\"], \n              title=f\"{country}: GDP local vs bigmac\",\n              markers=True)\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\ndf = %sql select * from bigmac;\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nax.set_title(\"GDP Bigmac vs dollar\", loc=\"left\", weight=\"bold\")\n\nfor name in [\"Pakistan\", \"India\"]:\n    d = df.query(\"name == @name\")\n    ax.plot(d.date, d.GDP_bigmac, alpha=0.8, label=f\"{name}\")\n    ax.fill_between(d.date, d.GDP_bigmac, d.GDP_dollar, alpha=0.12)\n    #ax.plot(d.date, d.GDP_dollar, alpha=0.6, ls=\"--\")\n\nax.legend();\n\n\n\n\n\nThis is interesting, you can see the indian dollar even though its close to the Pakistan one in GDP_dollar terms, the big mac index GDP says it buys a lot more.\n\n\n\nThis uses the big mac adjusted per capita GDP price calculated above to get a adjusted dollar price for a bigmac. This looks very similar to first big mac index so left this calc for a future date.\nSee Calculating the adjusted index for details.\n\n\n\n\nThere are a lot of countries in the index, which will make the graph a bit messy, so to add some groups to the counties (and do more sql!) I’m going to use the world banks income dataset, primary to add regions and income groups.\n\ndf_income = pd.read_excel(\n    \"https://datacatalogfiles.worldbank.org/ddh-published/0037712/DR0090755/CLASS.xlsx\"\n)\n\ndf_income = (\n    df_income.dropna(subset=[\"Income group\", \"Economy\"])\n    .drop(columns=\"Lending category\")\n    .rename(columns={\"Code\": \"iso_a3\", \"Income group\": \"income_group\"})\n)\n\n# adding in the missing eurozone\neuro_row = [\"Eurozone\", \"EUZ\", \"Europe\", \"High income\"]\ndf_income.loc[len(df_income)] = euro_row\n\ndf_income.tail(3)\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n215\nSouth Africa\nZAF\nSub-Saharan Africa\nUpper middle income\n\n\n216\nZambia\nZMB\nSub-Saharan Africa\nLower middle income\n\n\n217\nEurozone\nEUZ\nEurope\nHigh income\n\n\n\n\n\n\n\nNo need to do this step, but for more sql goodness, lets add this as a table in our database so we can practice joining two tables:\n\n%%sql\ncreate table income as select * from df_income;\nselect * from income limit 3;\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n0\nAruba\nABW\nLatin America & Caribbean\nHigh income\n\n\n1\nAfghanistan\nAFG\nSouth Asia\nLow income\n\n\n2\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n\n\n\n\n\n\n\n\n\nHere we join these two tables into a dataframe for plotting.\nI am using a left join as I want to keep all data in the bigmac table, and just add country info.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t write sql like below, use capitals and linebreaks!\n\n\n\ndf_all = %sql select * from bigmac left join income on bigmac.iso_a3 = income.iso_a3 order by bigmac.date asc;\ndf_all.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\nArgentina\nARG\nLatin America & Caribbean\nUpper middle income\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n\n\n\n\n\n\n\nEverything looks good here, we now have a dataframe which combines the two tables and is ready to plot.\n\n\n\n\nThis is going to be ugly, but its good matplotlib practice. First up, for the economist plot we just want the latest date:\n\ndf = df_all.query(\"date == date.max()\").sort_values(\"USD\")\ndf.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n1899\nRomania\nROU\nRON\n11.0\n4.82175\n14667.089\n6.102120e+04\n2022-07-01\n2.281329\n-0.557023\n28569.017768\nRomania\nROU\nEurope & Central Asia\nHigh income\n\n\n1876\nIndonesia\nIDN\nIDR\n35000.0\n14977.50000\n4356.560\n6.233566e+07\n2022-07-01\n2.336839\n-0.546245\n9172.246719\nIndonesia\nIDN\nEast Asia & Pacific\nUpper middle income\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(5, 10))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n\nax.set_title(\n    \"Big Mac Index\", loc=\"left\", weight=\"bold\", ha=\"left\", fontsize=15, x=-0.42\n)\n\nax.text(\n    x=-0.2,\n    y=0.915,\n    s=\"Unadjusted index for 2022\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.8,\n)\n\n\n# plotting the actual data\n\n# assigning each income group a color\ncolor_list = list(mcolors.TABLEAU_COLORS.values())\ncolors = {country: color_list[i] for i, country in enumerate(df.income_group.unique())}\n\n# the actual plot\nax.barh(\n    y=df.name,\n    width=df.USD,\n    alpha=0.85,\n    height=0.78,\n    color=[colors[income] for income in df.income_group],\n    zorder=2,\n)\n\n# x axis things\nax.xaxis.tick_top()\nax.set_xlabel(\n    \"                 &lt;-- Undervalued              Overvalued --&gt; \",\n    labelpad=10,\n    fontsize=11,\n)\nax.xaxis.set_label_position(\"top\")\n\n# fix and label the y axis\n# ax.set_yticklabels(df.name, ha=\"left\")\nax.yaxis.set_tick_params(pad=2, labelsize=9, bottom=False)\nax.set_ylim(-1, df.shape[0])\n\n\n# legend for income groups\nhandles = [mpatches.Patch(color=colors[i]) for i in colors]\nlabels = [f\"{i}\" for i in colors]\nax.legend(handles, labels, fontsize=8)\nax.grid(which=\"major\", axis=\"x\", color=\"#758D99\", alpha=0.5, zorder=1)\n\n# Set source text\nax.text(\n    x=-0.2,\n    y=0.08,\n    s=\"\"\"Source: \"Big Mac Index\" via economist.com\"\"\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.7,\n)\n\n\n\nText(-0.2, 0.08, 'Source: \"Big Mac Index\" via economist.com')\n(a) Big Mac Index\n\n\n\n\n\n\n(b)\n\n\n\nFigure 1: ?(caption)\n\n\nphew! That is a long image, with too many countries, and its a bit ugly, so if this was going to be used somewhere, I’d filter the countries and produce a more sensible sized graph.\n\n\n\nI should have probably gone with plotly first for this, but here goes:\n\nfig = px.bar(\n    df, x=\"USD\", y=\"name\", width=600, height=800, text=\"name\", color=\"income_group\"\n)\nfig.update_layout(title_text=\"Big Mac Index\", yaxis_categoryorder=\"total ascending\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\nCan play around a bit more with this data… but heaps enough for now."
  },
  {
    "objectID": "posts/sql_101/index.html#jupyter-duckdb-setup-for-sql",
    "href": "posts/sql_101/index.html#jupyter-duckdb-setup-for-sql",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Before the sql even starts, I want to use jupyter notebooks + sql. I’m using duckdb inside jupyter to look at a few sql datasets.\nFirst up, make a new env and install the necessary libraries if necessary by:\nmamba create -n sql101 python=3.11 jupyterlab duckdb duckdb-engine jupysql matplotlib openpyxl plotly\nmamba active sql101\n\n\nOnce all the packages have been installed, setup jupyter to use duckdb. You don’t need to use all the three config options below, just noting some useful ones for future reference.\nthe %config SqlMagic.autopandas in particular is useful - the sql query is returned as a pandas dataframe. This is useful as even though right now I have a small dataset, on a real project duckdb can talk to remote databases, or query parquet files on the web, do a query and only pull down the subset of data selected in the query.\nAfter that, the pandas dataframe is ideal for plotting and other pythonic stuff.\n\n# Import jupysql Jupyter extension to create SQL cells\n%load_ext sql\n\n%config SqlMagic.displaycon = False  # hides con info\n%config SqlMagic.autopandas = True   # output as df\n#%config SqlMagic.feedback = False   # \n\n%sql duckdb://"
  },
  {
    "objectID": "posts/sql_101/index.html#data",
    "href": "posts/sql_101/index.html#data",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "I’m using the Economist’s big mac gdp source data. Pandas can directly download the csv file into a dataframe which duckdb can read, but trying to stick with most sql here, so I’m downloading the csv file to disk and reading it with duckdb directly.\nWhy the big mac index? To calculate the big mac index we need to use correlated subqueries, which is a pretty advanced topic!\nThe final output figure Figure 1 is somewhere below, this notebook works through getting there slowly:\n\nurl = \"https://github.com/TheEconomist/big-mac-data/raw/master/source-data/big-mac-source-data-v2.csv\"\n\nfname = \"big-mac-source-data-v2.csv\"\n\nwith open(fname, \"wb\") as file:\n    file.write(requests.get(url).content)\n\nWhen using %%sql cells in jupyter, put python variables in brackets like so: {{var_name}}.\nThe read_csv_auto should auto-magically read and convert the csv to a sql table:\n\n%%sql\n-- make a table bigmac from the csv file\ncreate table if not exists bigmac as select * from read_csv_auto('{{fname}}');\n\nselect * from bigmac limit 3; -- comments can go here too!\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n\n\n\n\n\n\n\nSo now we have a table inside a sql database.\n\n\n\n\n\n\nNote\n\n\n\nsql convention is to use a lot of CAPITALS, but for fast typing and a lack of an sql formatter, I’m going lowercase. Ideally your sql writing thingamjig should have a formatter which does that for you."
  },
  {
    "objectID": "posts/sql_101/index.html#eda-and-data-cleanup",
    "href": "posts/sql_101/index.html#eda-and-data-cleanup",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Data always has some issues, so taking a look:\n\n\nA sql database can have many tables, so its useful to take a look at whats there:\n\n%sqlcmd tables\n\n\n\n\nName\n\n\n\n\nbigmac\n\n\n\n\n\nThis should describe the table:\n\n%sql describe bigmac;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\nThe describe bigmac code should spit out table info, seems to be some kind of bug, but moving on, we can get the gist using the information schema:\n\n%%sql \nSELECT COLUMN_NAME, DATA_TYPE\nFROM INFORMATION_SCHEMA.COLUMNS\nWHERE TABLE_NAME = 'bigmac';\n\n\n\n\n\n\n\n\ncolumn_name\ndata_type\n\n\n\n\n0\nname\nVARCHAR\n\n\n1\niso_a3\nVARCHAR\n\n\n2\ncurrency_code\nVARCHAR\n\n\n3\nlocal_price\nDOUBLE\n\n\n4\ndollar_ex\nDOUBLE\n\n\n5\nGDP_dollar\nDOUBLE\n\n\n6\nGDP_local\nDOUBLE\n\n\n7\ndate\nDATE\n\n\n\n\n\n\n\nThe above is not that useful, a more informative look which counts the values is:\n\n%sqlcmd profile --table 'bigmac'\n\n\n \n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\n\n\n\n\ncount\n1946\n1946\n1946\n1946\n1946\n1943\n1918\n1946\n\n\nunique\n74\n73\n58\n669\n1497\n1157\n1141\n37\n\n\ntop\nArgentina\nARG\nEUR\nnan\nnan\nnan\nnan\n2021-07-01\n\n\nfreq\n37\n37\n351\nnan\nnan\nnan\nnan\n73\n\n\nmean\nnan\nnan\nnan\n15816.0895\n4722.5632\n24790.7531\n2615423.9573\nnan\n\n\nstd\nnan\nnan\nnan\n393903.7657\n100597.3791\n21425.9054\n9471890.5225\nnan\n\n\nmin\nnan\nnan\nnan\n0.0\n0.0\n689.826\n3004.341\nnan\n\n\n25%\nnan\nnan\nnan\n4.4500\n1.0035\n6577.2870\n33698.7640\nnan\n\n\n50%\nnan\nnan\nnan\n15.0000\n5.4650\n17815.1920\n70506.5530\nnan\n\n\n75%\nnan\nnan\nnan\n87.0000\n32.8750\n41409.6309\n358689.5130\nnan\n\n\nmax\nnan\nnan\nnan\n16020000.0\n3613989.071\n102576.68\n85420189.717\nnan\n\n\n\n \n\n\n\n\n\nSo we have 3 rows without a GDP_dollar, and at least one local_price and dollar_ex of 0, so dropping those rows. We also need GDP_local to be able to adjust the bigmac index, so dropping any nulls in that row too.\n\n%%sql \ndelete from bigmac where \nGDP_dollar is null or GDP_local is null or local_price&lt;=0 or dollar_ex&lt;=0;\n\n\n\n\n\n\n\n\nSuccess\n\n\n\n\n\n\n\n\n\n\nrows = %sql select count(*) as N from bigmac;\nf\"The data set has a total of {rows.N.loc[0]} rows.\"\n\n'The data set has a total of 1918 rows.'\n\n\nIt’s a pretty clean and tidy dataset, we did loose a few rows, which in a real world case might bear more investigation, but moving on…\n\n%sql select count(distinct name) as Countries, min(date), max(date) from bigmac;\n\n\n\n\n\n\n\n\nCountries\nmin(date)\nmax(date)\n\n\n\n\n0\n73\n2000-04-01\n2022-07-01\n\n\n\n\n\n\n\nWe have data for 73 countries from April 2000 to July 2022."
  },
  {
    "objectID": "posts/sql_101/index.html#calculations-in-sql",
    "href": "posts/sql_101/index.html#calculations-in-sql",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Adding some columns by calculating new ones in sql. This is where the sql starts.\n\n\nWe want to get the US dollar price for a big mac in every country, which is easy as our data contains the local_price and dollar exchange rate. So we add a new column: dollar_price = local_price / dollar_ex.\nIn sql you can’t just add a column, you first have to add it with a value type. In pandas you can do this in one step: df[\"dollar_price\"] = df.local_price / df.dollar_ex.\nSo here we add a new col dollar_price and calc its value:\n\n%%sql\nalter table bigmac \n    add column if not exists dollar_price DOUBLE;\nupdate bigmac \n    set dollar_price = local_price / dollar_ex;\n\nselect * from bigmac limit 3;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n\n\n2\nBrazil\nBRA\nBRL\n2.95\n1.79\n3501.438\n6351.375\n2000-04-01\n1.648045\n\n\n\n\n\n\n\nThe line if not exists is optional, but useful in this context as it prevents errors if I rerun the notebook.\n\n\n\nNow we want to account for purchasing power parity by divinding the dollar price with the base currencies price. The Economist uses five base currencies: ('USD', 'EUR', 'GBP', 'JPY', 'CNY')\nFor simplicity’s sake, I’ll stick with just USD. This is a easy calc to do in python, in SQL its a bit messy…, so I am using Correlated Subqueries in SQL to do this.\nThe dollar_price on each row is divided by the dollar_price in USD. Since we have multiple dates, the query below matches on both the date and the country code.\nThe inner query returns the US dollar_price for each date, so every row in the table gets divided by the right dates USD price.\n\n\n\n\n\n\nNote\n\n\n\nThis only works because for each unique date, there is only one row for USA.\n\n\nFinally, we minus the number by -1, as we divide the US price by its own, so its always at 1. By subtracting -1, we set that to zero, which makes it wasy to see how the other countries are over or under that.\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS USD DOUBLE;\n\nupdate bigmac as b1\n    set USD = dollar_price / \n            (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA') - 1;\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n\n\n\n\n\n\n\nI did a few random checks and looks like the formula did use the right USD price to calculate the USD offset.\n\n\n\nBig Mac adjusted per capita GDP is the GDP in local currency divided by the exchange rate as determined by big macs (price in local currency divivded by price in US).\nThe formula is: GDP_Local / (local_price / dollar_price)\n\n%%sql\nALTER TABLE bigmac ADD COLUMN IF NOT EXISTS GDP_bigmac DOUBLE;\n\nupdate bigmac as b1\n    set GDP_bigmac = GDP_local / \n            (local_price / (select b2.dollar_price from bigmac as b2\n            where b2.date = b1.date\n            and b2.iso_a3 = 'USA'));\n\nselect * from bigmac order by date desc, name desc limit 6;\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\n\n\n\n\n0\nVietnam\nVNM\nVND\n69000.00\n23417.00000\n3724.543\n8.542019e+07\n2022-07-01\n2.946577\n-0.427849\n6375.564885\n\n\n1\nUruguay\nURY\nUYU\n255.00\n41.91000\n16756.344\n7.291942e+05\n2022-07-01\n6.084467\n0.181450\n14726.863618\n\n\n2\nUnited States\nUSA\nUSD\n5.15\n1.00000\n69231.400\n6.923140e+04\n2022-07-01\n5.150000\n0.000000\n69231.400000\n\n\n3\nUnited Arab Emirates\nARE\nAED\n18.00\n3.67305\n42883.686\n1.574903e+05\n2022-07-01\n4.900559\n-0.048435\n45059.735594\n\n\n4\nTurkey\nTUR\nTRY\n47.00\n17.56500\n9527.683\n8.448134e+04\n2022-07-01\n2.675776\n-0.480432\n9256.997784\n\n\n5\nThailand\nTHA\nTHB\n128.00\n36.61250\n7336.086\n2.313028e+05\n2022-07-01\n3.496074\n-0.321151\n9306.323554\n\n\n\n\n\n\n\nAnd presto, we have a gdp per big mac, which for most countries is significantly different from each other. A couple of plots to eyeball this:\n\n\nCode\ncountry = \"Pakistan\"\ndf = %sql select * from bigmac where name in ('{{country}}')\nfig = px.line(df, x=\"date\", y=[\"GDP_dollar\", \"GDP_bigmac\"], \n              title=f\"{country}: GDP local vs bigmac\",\n              markers=True)\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\ndf = %sql select * from bigmac;\n\nfig, ax = plt.subplots(figsize=(8, 4))\nax.spines[[\"top\", \"right\"]].set_visible(False)\nax.set_title(\"GDP Bigmac vs dollar\", loc=\"left\", weight=\"bold\")\n\nfor name in [\"Pakistan\", \"India\"]:\n    d = df.query(\"name == @name\")\n    ax.plot(d.date, d.GDP_bigmac, alpha=0.8, label=f\"{name}\")\n    ax.fill_between(d.date, d.GDP_bigmac, d.GDP_dollar, alpha=0.12)\n    #ax.plot(d.date, d.GDP_dollar, alpha=0.6, ls=\"--\")\n\nax.legend();\n\n\n\n\n\nThis is interesting, you can see the indian dollar even though its close to the Pakistan one in GDP_dollar terms, the big mac index GDP says it buys a lot more.\n\n\n\nThis uses the big mac adjusted per capita GDP price calculated above to get a adjusted dollar price for a bigmac. This looks very similar to first big mac index so left this calc for a future date.\nSee Calculating the adjusted index for details."
  },
  {
    "objectID": "posts/sql_101/index.html#adding-country-data",
    "href": "posts/sql_101/index.html#adding-country-data",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "There are a lot of countries in the index, which will make the graph a bit messy, so to add some groups to the counties (and do more sql!) I’m going to use the world banks income dataset, primary to add regions and income groups.\n\ndf_income = pd.read_excel(\n    \"https://datacatalogfiles.worldbank.org/ddh-published/0037712/DR0090755/CLASS.xlsx\"\n)\n\ndf_income = (\n    df_income.dropna(subset=[\"Income group\", \"Economy\"])\n    .drop(columns=\"Lending category\")\n    .rename(columns={\"Code\": \"iso_a3\", \"Income group\": \"income_group\"})\n)\n\n# adding in the missing eurozone\neuro_row = [\"Eurozone\", \"EUZ\", \"Europe\", \"High income\"]\ndf_income.loc[len(df_income)] = euro_row\n\ndf_income.tail(3)\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n215\nSouth Africa\nZAF\nSub-Saharan Africa\nUpper middle income\n\n\n216\nZambia\nZMB\nSub-Saharan Africa\nLower middle income\n\n\n217\nEurozone\nEUZ\nEurope\nHigh income\n\n\n\n\n\n\n\nNo need to do this step, but for more sql goodness, lets add this as a table in our database so we can practice joining two tables:\n\n%%sql\ncreate table income as select * from df_income;\nselect * from income limit 3;\n\n\n\n\n\n\n\n\nEconomy\niso_a3\nRegion\nincome_group\n\n\n\n\n0\nAruba\nABW\nLatin America & Caribbean\nHigh income\n\n\n1\nAfghanistan\nAFG\nSouth Asia\nLow income\n\n\n2\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n\n\n\n\n\n\n\n\n\nHere we join these two tables into a dataframe for plotting.\nI am using a left join as I want to keep all data in the bigmac table, and just add country info.\n\n\n\n\n\n\nWarning\n\n\n\nDon’t write sql like below, use capitals and linebreaks!\n\n\n\ndf_all = %sql select * from bigmac left join income on bigmac.iso_a3 = income.iso_a3 order by bigmac.date asc;\ndf_all.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n0\nArgentina\nARG\nARS\n2.50\n1.00\n8709.072\n8709.072\n2000-04-01\n2.500000\n0.116071\n7803.328512\nArgentina\nARG\nLatin America & Caribbean\nUpper middle income\n\n\n1\nAustralia\nAUS\nAUD\n2.59\n1.68\n21746.809\n33698.764\n2000-04-01\n1.541667\n-0.311756\n29144.876973\nAustralia\nAUS\nEast Asia & Pacific\nHigh income\n\n\n\n\n\n\n\nEverything looks good here, we now have a dataframe which combines the two tables and is ready to plot."
  },
  {
    "objectID": "posts/sql_101/index.html#big-mac-index-chart-using-matplotlib",
    "href": "posts/sql_101/index.html#big-mac-index-chart-using-matplotlib",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "This is going to be ugly, but its good matplotlib practice. First up, for the economist plot we just want the latest date:\n\ndf = df_all.query(\"date == date.max()\").sort_values(\"USD\")\ndf.head(2)\n\n\n\n\n\n\n\n\nname\niso_a3\ncurrency_code\nlocal_price\ndollar_ex\nGDP_dollar\nGDP_local\ndate\ndollar_price\nUSD\nGDP_bigmac\nEconomy\niso_a3_2\nRegion\nincome_group\n\n\n\n\n1899\nRomania\nROU\nRON\n11.0\n4.82175\n14667.089\n6.102120e+04\n2022-07-01\n2.281329\n-0.557023\n28569.017768\nRomania\nROU\nEurope & Central Asia\nHigh income\n\n\n1876\nIndonesia\nIDN\nIDR\n35000.0\n14977.50000\n4356.560\n6.233566e+07\n2022-07-01\n2.336839\n-0.546245\n9172.246719\nIndonesia\nIDN\nEast Asia & Pacific\nUpper middle income\n\n\n\n\n\n\n\n\n\n\nCode\nfig, ax = plt.subplots(figsize=(5, 10))\nax.spines[[\"top\", \"right\", \"left\"]].set_visible(False)\n\nax.set_title(\n    \"Big Mac Index\", loc=\"left\", weight=\"bold\", ha=\"left\", fontsize=15, x=-0.42\n)\n\nax.text(\n    x=-0.2,\n    y=0.915,\n    s=\"Unadjusted index for 2022\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.8,\n)\n\n\n# plotting the actual data\n\n# assigning each income group a color\ncolor_list = list(mcolors.TABLEAU_COLORS.values())\ncolors = {country: color_list[i] for i, country in enumerate(df.income_group.unique())}\n\n# the actual plot\nax.barh(\n    y=df.name,\n    width=df.USD,\n    alpha=0.85,\n    height=0.78,\n    color=[colors[income] for income in df.income_group],\n    zorder=2,\n)\n\n# x axis things\nax.xaxis.tick_top()\nax.set_xlabel(\n    \"                 &lt;-- Undervalued              Overvalued --&gt; \",\n    labelpad=10,\n    fontsize=11,\n)\nax.xaxis.set_label_position(\"top\")\n\n# fix and label the y axis\n# ax.set_yticklabels(df.name, ha=\"left\")\nax.yaxis.set_tick_params(pad=2, labelsize=9, bottom=False)\nax.set_ylim(-1, df.shape[0])\n\n\n# legend for income groups\nhandles = [mpatches.Patch(color=colors[i]) for i in colors]\nlabels = [f\"{i}\" for i in colors]\nax.legend(handles, labels, fontsize=8)\nax.grid(which=\"major\", axis=\"x\", color=\"#758D99\", alpha=0.5, zorder=1)\n\n# Set source text\nax.text(\n    x=-0.2,\n    y=0.08,\n    s=\"\"\"Source: \"Big Mac Index\" via economist.com\"\"\",\n    transform=fig.transFigure,\n    ha=\"left\",\n    fontsize=9,\n    alpha=0.7,\n)\n\n\n\nText(-0.2, 0.08, 'Source: \"Big Mac Index\" via economist.com')\n(a) Big Mac Index\n\n\n\n\n\n\n(b)\n\n\n\nFigure 1: ?(caption)\n\n\nphew! That is a long image, with too many countries, and its a bit ugly, so if this was going to be used somewhere, I’d filter the countries and produce a more sensible sized graph."
  },
  {
    "objectID": "posts/sql_101/index.html#big-mac-index-using-plotly",
    "href": "posts/sql_101/index.html#big-mac-index-using-plotly",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "I should have probably gone with plotly first for this, but here goes:\n\nfig = px.bar(\n    df, x=\"USD\", y=\"name\", width=600, height=800, text=\"name\", color=\"income_group\"\n)\nfig.update_layout(title_text=\"Big Mac Index\", yaxis_categoryorder=\"total ascending\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "posts/sql_101/index.html#the-end",
    "href": "posts/sql_101/index.html#the-end",
    "title": "SQL 101 with duckdb and jupysql",
    "section": "",
    "text": "Can play around a bit more with this data… but heaps enough for now."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html",
    "href": "posts/code/advent-of-code-2015/index.html",
    "title": "Advent of Code 2015",
    "section": "",
    "text": "A slow solution of Advent of Code 2015. The following is a write up of how to solve and the things I learned while doing so - look at the AOC reddit site for ninja level solutions.\nI am trying to\nFirst up, I’m importing all the libs I’ll use up here:\nCode\n# python essentials\nimport os\nimport re\nimport hashlib\nimport math\nfrom pathlib import Path\nfrom typing import List, NamedTuple\nfrom collections import defaultdict, namedtuple, Counter\n\n# useful external libs\n#import numpy as np\nimport pandas as pd\n\n# misc utils\nimport requests\n#from tqdm.notebook import trange, tqdm # progress bars for slow funcs\n#from functools import reduce \n\n# for plots, cause visuals\nimport matplotlib.pyplot as plt # goto python viz lib\n#import seaborn as sns # prettify matplotlib\nfrom IPython.display import display, Markdown\n\n# javascript plotting for interactive graphs\n#import altair as alt\n#import plotly.express as px\nSome helper functions to avoid rewriting the same code for all the problems:\nCode\ndef get_input(day:int=1, path=\"inputs\"):\n    try:\n        return (Path() / f\"{path}/{day}.txt\").read_text().strip()\n    except:\n        print(f\"Failed to load {day}.txt from `inputs` subdir.\")\n\ndef printmd(txt): display(Markdown(txt))"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "href": "posts/code/advent-of-code-2015/index.html#day-1-not-quite-lisp",
    "title": "Advent of Code 2015",
    "section": "Day 1: Not Quite Lisp",
    "text": "Day 1: Not Quite Lisp\n# We’re standing at a inifinite building, and following instructions:( is up, ) is down to find the right floor.\nThis is simple - minus the ups from the downs:\n\n\nCode\nin1 = get_input(1)\nin1.count(\"(\") - in1.count(\")\")\n\n\n138\n\n\nA list comprehension version for kicks:\n\n\nCode\nsum([1 if char == \"(\" else -1 for char in in1])\n\n\n138\n\n\nfor part 2, we need to find the first time the we enter the basement while following the instructions.\n\n\nCode\nfloor, ans = 0, None\nfloors = []\n\nfor i, mv in enumerate(inp1):\n    if mv == \"(\":\n        floor += 1\n    else:\n        floor -= 1\n    \n    floors.append(floor)\n    \n    if floor == -1 and not ans:\n        ans = i + 1\n        printmd(f\"First reached the basement at timestep: **{i + 1}**\")\n        #break # no need to continue climbing\n\nplt.title(\"Floor Tracker\"); plt.xlabel(\"Timestep\"); plt.ylabel(\"Floor\")\nplt.axvline(x=ans, label=\"Part 2\", alpha=0.35)\nplt.axhline(y=-1, label=\"Basement\", color=\"red\", alpha=0.35)\nplt.axhline(y=138, label=\"Final Floor\", color=\"orange\", alpha=0.35)\nplt.plot(range(len(floors)), floors, label=\"Position\")\nplt.legend(loc=\"lower right\");\n\n\nFirst reached the basement at timestep: 1771"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "href": "posts/code/advent-of-code-2015/index.html#day-2-i-was-told-there-would-be-no-math",
    "title": "Advent of Code 2015",
    "section": "Day 2: I Was Told There Would Be No Math",
    "text": "Day 2: I Was Told There Would Be No Math\nHow much wrapping paper is needed to wrap a bunch of presents? We need 2*l*w + 2*w*h + 2*h*l paper, and the input is the l, w and h of each present.\nIn the bad old days of programming, this would be the perfect place to represent the data as a list or tuple in the form [3 ,3, 9] representing [l, w, h]. But now we can use namedtuples to make it easier to understand the data, and also it makes it easier to add more info, e.g type of paper used, cost etc:\n\n\nCode\nclass Present(NamedTuple):\n    l: int\n    w: int\n    h: int\n\ndata2 = [Present(*[int(x) for x in i.split(\"x\")]) for i in get_input(2).split(\"\\n\")]\ndata2[:4]\n\n\n[Present(l=29, w=13, h=26),\n Present(l=11, w=11, h=14),\n Present(l=27, w=2, h=5),\n Present(l=6, w=10, h=13)]\n\n\nNow to calcuate the area:\n\n\nCode\ndef get_present_area(p: Present) -&gt; int:\n    box_area = sum([2*p.l*p.w, 2*p.w*p.h, 2*p.h*p.l])\n    extra_paper = math.prod(sorted(p)[:2])\n    return box_area + extra_paper\n\nsum([get_present_area(x) for x in data2])\n\n\n1586300\n\n\nNow we need to calcuate the ribbon required, which is equal to the perimeter of the smallest face + cubic volume of the present\n\n\nCode\ndef ribbon(present: Present) -&gt; int:\n    \"\"\"takes in present, returns length of ribbon needed to wrap\"\"\"\n    l, w, h = sorted(present)\n    return 2*l + 2*w + l*w*h\n\nsum([ribbon(present) for present in data2])\n\n\n3737498"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "href": "posts/code/advent-of-code-2015/index.html#day-3-perfectly-spherical-houses-in-a-vacuum",
    "title": "Advent of Code 2015",
    "section": "Day 3: Perfectly Spherical Houses in a Vacuum",
    "text": "Day 3: Perfectly Spherical Houses in a Vacuum\n# Santa is delivering presents to houses, and his movements is 1 step at a time: north (^), south (v), east (&gt;), or west (&lt;)\n\n\nCode\ninp3 = get_input(3)\n\ndirs = {\"^\": (0,1), \"&gt;\": (1,0), \"v\": (0,-1), \"&lt;\": (-1, 0)}\n\ndef get_moves(data):\n    moves = [(0,0)]  # starting point\n\n    for mv in data:\n        x, y = moves[-1] # x,y of current pos\n        xx, yy = dirs[mv]\n        moves.append((x + xx, y + yy))\n    return moves\n\nmoves = get_moves(inp3)\nc = Counter(moves)\nprintmd(f\"Santa visited **{len(c)}** unique places.\")\n\nx, y = zip(*moves)\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"Santa visited {len(c)} unique places in {len(moves)} visits\")\nax.plot(x,y, alpha=0.7, label=\"Santa's Movements\"); ax.legend();\n\n\nSanta visited 2565 unique places.\n\n\n\n\n\nfor part 2, we have two santas! They move alternatingly, so we can say Santa_1 does all the odd moves and Santa_2 does all the even moves:\n\n\nCode\nsanta_1 = get_moves(inp3[::2])  # all the odd moves\nsanta_2 = get_moves(inp3[1::2]) # all the even moves\n\ntwo_santas = Counter(santa_1 + santa_2)\nprintmd(f\"The two santas visited **{len(two_santas)}** unique places.\")\n\nf, ax = plt.subplots(figsize=(10,6))\nplt.title(f\"the two santas visited {len(two_santas)} unique places\")\n\nfor name, santa in zip((\"Bob\", \"Alice\"), (santa_1, santa_2)):\n    x, y = zip(*santa)\n    ax.plot(x,y, alpha=0.8, label=f\"Santa_{name}\")\nax.legend();\n\n\nThe two santas visited 2639 unique places."
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "href": "posts/code/advent-of-code-2015/index.html#day-4-the-ideal-stocking-stuffer",
    "title": "Advent of Code 2015",
    "section": "Day 4: The Ideal Stocking Stuffer",
    "text": "Day 4: The Ideal Stocking Stuffer\n#\n\n\nCode\ninp4 = \"bgvyzdsv\"\ntest4_1 = \"abcdef\" #609043\ntest4_2 = \"pqrstuv\" # 1048970\n\ndef make_hash(txt):\n    return hashlib.md5(txt.encode(\"utf\")).hexdigest()\n\n\n\nFalse\n\n\n\n\nCode\ndef day_4_1(inp=inp4, s=\"None\", target=\"00000\", i=0) -&gt; int:\n    while not s.startswith(target):\n        i += 1\n        txt = inp + str(i)\n        s = make_hash(txt)\n    printmd(f\"_{inp}_ target at position **{i:,}** ({s})\")\n    return i\n\nassert day_4_1(\"abcdef\") == 609043    # tests are always a good idea\nassert day_4_1(\"pqrstuv\") == 1048970    \n\nday_4_1()\n\n\nabcdef target at position 609,043 (000001dbbfa3a5c83a2d506429c7b00e)\n\n\npqrstuv target at position 1,048,970 (000006136ef2ff3b291c85725f17325c)\n\n\nbgvyzdsv target at position 254,575 (000004b30d481662b9cb0c105f6549b2)\n\n\n254575\n\n\nPart two just changes the target sring to have one more zero so thanks to making part one a function this is easy:\n\n\nCode\nday_4_1(target=\"000000\")\n\n\nbgvyzdsv target at position 1,038,736 (000000b1b64bf5eb55aad89986126953)\n\n\n1038736"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "href": "posts/code/advent-of-code-2015/index.html#day-5-doesnt-he-have-intern-elves-for-this",
    "title": "Advent of Code 2015",
    "section": "Day 5: Doesn’t He Have Intern-Elves For This?",
    "text": "Day 5: Doesn’t He Have Intern-Elves For This?\n# We have a list of strings, and Santa has the following rules to figure out which ones are nice:\n\nat least three vowels (aeiou only), like aei, xazegov, or aeiouaeiouaeiou.\nat least one letter that appears twice in a row, like xx, abcdde (dd), or aabbccdd (aa, bb, cc, or dd).\ndoes not contain the strings ab, cd, pq, or xy, even if they are part of one of the other requirements.\n\n\n\nCode\nvowels = \"aeiou\"                        # need vowels\nbad_strings = [\"ab\", \"cd\", \"pq\", \"xy\"]  # don't want these\nregex = re.compile(r\"([a-zA-Z])\\1{1,}\") # search for 2+ letters in a row\n\ntest4 = [\"ugknbfddgicrmopn\", \"aaa\", \"jchzalrnumimnmhp\", \n        \"haegwjzuvuyypxyu\", \"dvszwmarrgswjxmb\"]\n\ndef is_nice_string(txt):\n    vowel_count = len([char for char in txt if char in vowels]) &gt;= 3\n    two_chars = len(re.findall(regex, txt)) &gt; 0\n    no_bad_str = True if (sum([s3d in txt for s in bad_strings]) == 0) else False\n    \n    return vowel_count and two_chars and no_bad_str\n\n[is_nice_string(t) for t in test4] #== [False, False, True, True, True]\n\n\n[True, True, False, False, False]\n\n\n\n\nCode\ninp5 = get_input(5).split(\"\\n\")\nprint(\"Number of nice strings: \", sum([is_nice_string(t) for t in inp5]))\n\n\nNumber of nice strings:  258\n\n\nfor part two, the rules have changed, a nice string has these properties:\n\nIt contains a pair of any two letters that appears at least twice in the string without overlapping, like xyxy (xy) or aabcdefgaa (aa), but not like aaa (aa, but it overlaps).\nIt contains at least one letter which repeats with exactly one letter between them, like xyx, abcdefeghi (efe), or even aaa.\n\n\nNote: the rest remains to be done\n\n\n\nCode\nregex_2char = re.compile(r\"([a-zA-Z])\\1{1,}\")\nregex_3char = re.compile(r\"([a-zA-Z])\\1{2,}\")\n\nfor txt in [\"aa\", \"aba\", \"aaa\"]:\n    print(re.findall(regex_2char, txt))\n\n\n['a']\n[]\n['a']"
  },
  {
    "objectID": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "href": "posts/code/advent-of-code-2015/index.html#day-6-probably-a-fire-hazard",
    "title": "Advent of Code 2015",
    "section": "Day 6: Probably a Fire Hazard",
    "text": "Day 6: Probably a Fire Hazard\n#"
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html",
    "href": "posts/algorithims/histogram_max_area.html",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "This is a popular coding interview question. It’s really simple with a simple histogram using pen and paper, but harder to think about in code, as it takes a bit of thinking through.\nDetailed problem description: https://leetcode.com/problems/largest-rectangle-in-histogram/\nA few test arrays with the largest area:\n\ntests = (\n    ([2, 1, 5, 6, 2, 3], 10),\n    ([6, 3, 1, 4, 12, 4], 12),\n    ([5, 6, 7, 4, 1], 16),\n    ([2, 1, 3, 4, 1], 6),\n)\n\nThe arrays are visualized below, which makes it much easier to think about the solution. I found it easier to do it on paper first, before coding.\n\n\nCode\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.8,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n\n\n\n\nLooking at the array makes it easy to see where the largest rectangle might be.\n\n\nWe can brute force this by generating the largest possible rectangle at every item in the array.\nFor each N in the arry:\n\nfind the left and right boundary of the largest possible rectangle\nthis gives us the width, exluding the width of N itself (adding 1 to ad back N)\nso now we have the width, and the height is just the value N in the array\n\nTo keep things simple, first up a helper function which returns the rectangle boundaries of a given point in an array:\n\ndef find_boundary(idx: int, arr: list[int]) -&gt; tuple[int, int]:\n    \"\"\"\n    Example:\n        idx 2 for [2, 1, 5, 6, 2, 3] returns (2, 3)\n    Returns:\n        (left, right)\n    \"\"\"\n\n    # find left boundary (can be itself)\n    left = 0\n    if idx == left:\n        pass  # deals with the left edge\n    else:\n        # march leftwards all the way to zero:\n        for j in range(idx - 1, -1, -1):\n            if arr[j] &lt; arr[idx]:\n                left = j + 1  # adding 1 to exlude boundary\n                break  # exit loop once the first boundary found\n\n    # find right boundary (can be itself)\n    right = len(arr) - 1  # deal with the right edge\n\n    if idx == right:\n        pass  # at right edge already\n    else:\n        # march rightwards\n        for j in range(idx, len(arr)):\n            if arr[j] &lt; arr[idx]:\n                right = j - 1  # subtracting 1 to exclude boundary\n                break\n\n    return left, right\n\nPhew! that should return the (left, right) boundaries for a given index and array. Using np.argmax should make this faster, but I wanted to see how it would look in pure python.\n\n# testing this for the first arrary in the tests\nfor arr, ans in tests:\n    print(\"array: \", arr)\n    for i in range(len(arr)):\n        print(f\"Index {i}: (val {arr[i]}) - Boundaries: {find_boundary(i, arr)}\")\n    break\n\narray:  [2, 1, 5, 6, 2, 3]\nIndex 0: (val 2) - Boundaries: (0, 0)\nIndex 1: (val 1) - Boundaries: (0, 5)\nIndex 2: (val 5) - Boundaries: (2, 3)\nIndex 3: (val 6) - Boundaries: (3, 3)\nIndex 4: (val 2) - Boundaries: (2, 5)\nIndex 5: (val 3) - Boundaries: (5, 5)\n\n\nI drew the boundaries by hand for the first array to test the algo, and checked that the left and right values at each index are correct.\nThe find_boundary func is working, so now its easy to get the area of the max rectangle.\n\n\nThe below function iterates through every item in the array, calculates the area of the largest rectangle at that point, and updates the max area function.\n\ndef max_area(arr: list[int], viz: bool = False) -&gt; int:\n    \"\"\"returns the area of the biggest rectangle,\n    and optionally returns its index\"\"\"\n    max_area = 0\n    max_idx = None\n\n    for i in range(len(arr)):\n        left, right = find_boundary(i, arr)\n        # adding 1 to width as when we calc (right - left) it excludes itself\n        width = 1 + right - left\n\n        new_area = arr[i] * width\n        if new_area &gt; max_area:\n            max_area = new_area\n            max_idx = i\n\n    if viz:\n        return max_idx, max_area\n    else:\n        return max_area\n\n\nfor arr, ans in tests:\n    assert max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nThat was pretty straight forward, though a bit verbose. Now an excercise in plotting this visually:\n\n\n\nUsing matplotlib to plot the largest rectangle - this helps explain why the answer better than a wall of text.\n\n\nCode\n# to use to show different rectangele with diff colors and fills\ncolors = list(mcolors.TABLEAU_COLORS.values())\nhatches = [\"/\", \"\\\\\", \"|\", \"-\", \"+\", \"x\", \"o\", \"O\", \".\", \"*\"]\n\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.6,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n    idx, area = max_area(arr, True)\n\n    for i in range(len(arr)):\n        if i == idx:  # got too messy plotting all the rects\n            left, right = find_boundary(i, arr)\n            width = 1 + right - left\n            ax.add_patch(\n                Rectangle(\n                    (left - 0.5, 0),\n                    width,\n                    arr[i],\n                    alpha=0.35,\n                    facecolor=\"purple\",\n                    ls=\"--\",\n                    lw=2,\n                    hatch=hatches[5],\n                )\n            )\n\n\n\n\n\nEvery time I use matplotlib I’m both horrified and inmpressed by what you can do it… basically anything but so much code…\n\n\n\n\nBy using a stack, we can find the largest rect in O(n) time, as we don’t go through the array multiple times in this solution.\nInstead of relooping through the array, we can use a stack to only go through the array once.\nThis algo:\n\nwhile the stack isn’t empty and the height of the current bar is &lt;= to the top bar in the stack\n\ncalc rect area, update if bigger\n\nrect height is the bar at the top of the array\n\npush current bar onto stack\n\nthe above loops end but leaves bars in the stack which only have bigger items to their right. Deal with this by:\n\npop the stack, the height is that item, the width is the distance b/w that item and the right end of the array\ncalc rect area, update max area\n\n\n\ndef histogram_max_area(arr):\n    stack = [-1]  # the stop or sentinel value\n    max_area = 0\n\n    for i in range(len(arr)):\n        while stack[-1] != -1 and arr[stack[-1]] &gt;= arr[i]:\n            current_height = arr[stack.pop()]\n            current_width = i - stack[-1] - 1\n            max_area = max(max_area, current_height * current_width)\n        stack.append(i)\n\n    # the remaining stack items rectangles extend all the way to the end\n    while stack[-1] != -1:\n        current_height = arr[stack.pop()]\n        current_width = len(arr) - stack[-1] - 1\n        max_area = max(max_area, current_height * current_width)\n    return max_area\n\n\nfor arr, ans in tests:\n    assert histogram_max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\n\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nEven though the stack based solution is better, faster and smaller its tougher to comprehend than the simpler one above."
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html#a-simple-solution",
    "href": "posts/algorithims/histogram_max_area.html#a-simple-solution",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "We can brute force this by generating the largest possible rectangle at every item in the array.\nFor each N in the arry:\n\nfind the left and right boundary of the largest possible rectangle\nthis gives us the width, exluding the width of N itself (adding 1 to ad back N)\nso now we have the width, and the height is just the value N in the array\n\nTo keep things simple, first up a helper function which returns the rectangle boundaries of a given point in an array:\n\ndef find_boundary(idx: int, arr: list[int]) -&gt; tuple[int, int]:\n    \"\"\"\n    Example:\n        idx 2 for [2, 1, 5, 6, 2, 3] returns (2, 3)\n    Returns:\n        (left, right)\n    \"\"\"\n\n    # find left boundary (can be itself)\n    left = 0\n    if idx == left:\n        pass  # deals with the left edge\n    else:\n        # march leftwards all the way to zero:\n        for j in range(idx - 1, -1, -1):\n            if arr[j] &lt; arr[idx]:\n                left = j + 1  # adding 1 to exlude boundary\n                break  # exit loop once the first boundary found\n\n    # find right boundary (can be itself)\n    right = len(arr) - 1  # deal with the right edge\n\n    if idx == right:\n        pass  # at right edge already\n    else:\n        # march rightwards\n        for j in range(idx, len(arr)):\n            if arr[j] &lt; arr[idx]:\n                right = j - 1  # subtracting 1 to exclude boundary\n                break\n\n    return left, right\n\nPhew! that should return the (left, right) boundaries for a given index and array. Using np.argmax should make this faster, but I wanted to see how it would look in pure python.\n\n# testing this for the first arrary in the tests\nfor arr, ans in tests:\n    print(\"array: \", arr)\n    for i in range(len(arr)):\n        print(f\"Index {i}: (val {arr[i]}) - Boundaries: {find_boundary(i, arr)}\")\n    break\n\narray:  [2, 1, 5, 6, 2, 3]\nIndex 0: (val 2) - Boundaries: (0, 0)\nIndex 1: (val 1) - Boundaries: (0, 5)\nIndex 2: (val 5) - Boundaries: (2, 3)\nIndex 3: (val 6) - Boundaries: (3, 3)\nIndex 4: (val 2) - Boundaries: (2, 5)\nIndex 5: (val 3) - Boundaries: (5, 5)\n\n\nI drew the boundaries by hand for the first array to test the algo, and checked that the left and right values at each index are correct.\nThe find_boundary func is working, so now its easy to get the area of the max rectangle.\n\n\nThe below function iterates through every item in the array, calculates the area of the largest rectangle at that point, and updates the max area function.\n\ndef max_area(arr: list[int], viz: bool = False) -&gt; int:\n    \"\"\"returns the area of the biggest rectangle,\n    and optionally returns its index\"\"\"\n    max_area = 0\n    max_idx = None\n\n    for i in range(len(arr)):\n        left, right = find_boundary(i, arr)\n        # adding 1 to width as when we calc (right - left) it excludes itself\n        width = 1 + right - left\n\n        new_area = arr[i] * width\n        if new_area &gt; max_area:\n            max_area = new_area\n            max_idx = i\n\n    if viz:\n        return max_idx, max_area\n    else:\n        return max_area\n\n\nfor arr, ans in tests:\n    assert max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nThat was pretty straight forward, though a bit verbose. Now an excercise in plotting this visually:\n\n\n\nUsing matplotlib to plot the largest rectangle - this helps explain why the answer better than a wall of text.\n\n\nCode\n# to use to show different rectangele with diff colors and fills\ncolors = list(mcolors.TABLEAU_COLORS.values())\nhatches = [\"/\", \"\\\\\", \"|\", \"-\", \"+\", \"x\", \"o\", \"O\", \".\", \"*\"]\n\nfig, axes = plt.subplots(2, 2, layout=\"tight\")\nfor ax, (arr, ans) in zip(axes.flatten(), tests):\n    ax.set_axis_off()\n    bar = ax.bar(\n        range(len(arr)),\n        arr,\n        width=0.95,\n        alpha=0.6,\n        edgecolor=\"yellow\",\n    )\n    ax.set_title(f\"Largest rectange: {ans:2}\", loc=\"left\", fontsize=10)\n    ax.bar_label(bar, fontsize=8)\n\n    idx, area = max_area(arr, True)\n\n    for i in range(len(arr)):\n        if i == idx:  # got too messy plotting all the rects\n            left, right = find_boundary(i, arr)\n            width = 1 + right - left\n            ax.add_patch(\n                Rectangle(\n                    (left - 0.5, 0),\n                    width,\n                    arr[i],\n                    alpha=0.35,\n                    facecolor=\"purple\",\n                    ls=\"--\",\n                    lw=2,\n                    hatch=hatches[5],\n                )\n            )\n\n\n\n\n\nEvery time I use matplotlib I’m both horrified and inmpressed by what you can do it… basically anything but so much code…"
  },
  {
    "objectID": "posts/algorithims/histogram_max_area.html#a-faster-stack-based-solution",
    "href": "posts/algorithims/histogram_max_area.html#a-faster-stack-based-solution",
    "title": "The largest rectangle in a histogram",
    "section": "",
    "text": "By using a stack, we can find the largest rect in O(n) time, as we don’t go through the array multiple times in this solution.\nInstead of relooping through the array, we can use a stack to only go through the array once.\nThis algo:\n\nwhile the stack isn’t empty and the height of the current bar is &lt;= to the top bar in the stack\n\ncalc rect area, update if bigger\n\nrect height is the bar at the top of the array\n\npush current bar onto stack\n\nthe above loops end but leaves bars in the stack which only have bigger items to their right. Deal with this by:\n\npop the stack, the height is that item, the width is the distance b/w that item and the right end of the array\ncalc rect area, update max area\n\n\n\ndef histogram_max_area(arr):\n    stack = [-1]  # the stop or sentinel value\n    max_area = 0\n\n    for i in range(len(arr)):\n        while stack[-1] != -1 and arr[stack[-1]] &gt;= arr[i]:\n            current_height = arr[stack.pop()]\n            current_width = i - stack[-1] - 1\n            max_area = max(max_area, current_height * current_width)\n        stack.append(i)\n\n    # the remaining stack items rectangles extend all the way to the end\n    while stack[-1] != -1:\n        current_height = arr[stack.pop()]\n        current_width = len(arr) - stack[-1] - 1\n        max_area = max(max_area, current_height * current_width)\n    return max_area\n\n\nfor arr, ans in tests:\n    assert histogram_max_area(arr) == ans\n    print(f\"Input: {str(arr):20} =&gt; Max Area: {ans:2} \")\n\nprint(f\"*All {len(tests)} tests passed!*\")\n\nInput: [2, 1, 5, 6, 2, 3]   =&gt; Max Area: 10 \nInput: [6, 3, 1, 4, 12, 4]  =&gt; Max Area: 12 \nInput: [5, 6, 7, 4, 1]      =&gt; Max Area: 16 \nInput: [2, 1, 3, 4, 1]      =&gt; Max Area:  6 \n*All 4 tests passed!*\n\n\nEven though the stack based solution is better, faster and smaller its tougher to comprehend than the simpler one above."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "KO",
    "section": "",
    "text": "The largest rectangle in a histogram\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nSQL 101 with duckdb and jupysql\n\n\n\n\n\n\n\npython\n\n\nsql\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2023\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nQuarto Jupyter notebook test\n\n\n\n\n\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2022\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nWindows 10/11 Setup\n\n\n\n\n\n\n\nwindows\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nAdvent of Code 2015\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\na verbose solve of AOC 2015\n\n\n\n\n\n\nAug 10, 2020\n\n\nKO\n\n\n\n\n\n\n  \n\n\n\n\nFloodfill algorithim\n\n\n\n\n\n\n\nalgorithims\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nAug 10, 2018\n\n\nKO\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/algorithims/flood_fill.html",
    "href": "posts/algorithims/flood_fill.html",
    "title": "Floodfill algorithim",
    "section": "",
    "text": "Flood Fill is a way to to visit every point in a bounded region. This makes it useful for many purposes. In this notebook I implement the “bucket fill” flood fill algo.\nFirst up generating a grid to flood fill:\nfill = np.random.randint(0, 2, size=(128,128), dtype=\"int\")\nfill\n\narray([[0, 1, 0, ..., 0, 1, 0],\n       [1, 1, 0, ..., 0, 0, 1],\n       [1, 1, 0, ..., 1, 0, 0],\n       ...,\n       [0, 1, 0, ..., 1, 1, 1],\n       [1, 1, 0, ..., 0, 1, 1],\n       [1, 0, 0, ..., 0, 1, 0]])\nEyeballing this grid visually:\nplt.imshow(fill);"
  },
  {
    "objectID": "posts/algorithims/flood_fill.html#the-floodfill-algo",
    "href": "posts/algorithims/flood_fill.html#the-floodfill-algo",
    "title": "Floodfill algorithim",
    "section": "the floodfill algo",
    "text": "the floodfill algo\nThe below function is a recursive implementation of flood fill - it will flood fill a a single region from 1 val to another:\n\ndef flood_fill(x, y, old, new):\n    \"\"\"takes in a x,y position from where to flood fill, the old val to change from and the new val\n    to change too, and then does so on a to_fill array\"\"\"\n    \n    if fill[x][y] != old or fill[x][y] == new:\n        return\n    \n    fill[x][y] = new\n    \n    max_x = len(fill) - 1\n    max_y = len(fill) - 1\n    \n    if x &gt; 0: # go left\n        flood_fill(x-1, y, old, new)\n    \n    if x &lt; max_x: # go right\n        flood_fill(x+1, y, old, new)\n        \n    if y &gt; 0: # go down\n        flood_fill(x, y-1, old, new)\n    if y &lt; max_y: # go up\n        flood_fill(x, y+1, old, new)\n\nHere, I flood fill the fill, converting all the 1’s to 8’s.\n\nregion_count = 0\n\nimagelist = list()\nimagelist.append(fill)\n\nfor i in range(len(fill)):\n    for j in range(len(fill[0])):\n        if fill[i][j] == 1:\n            flood_fill(i, j, 1, 8)\n            region_count += 1\n            imagelist.append(fill)\n            if region_count % 100 == 0:\n                plt.imshow(imagelist[region_count])\n                plt.title(f\"Flood Fill by region\")\n                plt.show()"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html",
    "href": "posts/code/quarto_notebook_test.html",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#polar-axis",
    "href": "posts/code/quarto_notebook_test.html#polar-axis",
    "title": "Quarto Jupyter notebook test",
    "section": "",
    "text": "For a demonstration of a line plot on a polar axis, see Figure 1.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\n\nfig, ax = plt.subplots(subplot_kw={'projection': 'polar'})\nax.plot(theta, r)\nplt.show()\n\n\n\n\nFigure 1: A line plot test with caption"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "href": "posts/code/quarto_notebook_test.html#lets-try-some-seaborn",
    "title": "Quarto Jupyter notebook test",
    "section": "Lets try some seaborn…",
    "text": "Lets try some seaborn…\n\nimport seaborn as sns\nsns.set_theme(style=\"ticks\", palette=\"pastel\")\n\n# Load the example tips dataset\ntips = sns.load_dataset(\"tips\")\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"day\", y=\"total_bill\",\n            hue=\"smoker\", palette=[\"m\", \"g\"],\n            data=tips)\nsns.despine(offset=10, trim=True)"
  },
  {
    "objectID": "posts/code/quarto_notebook_test.html#testing",
    "href": "posts/code/quarto_notebook_test.html#testing",
    "title": "Quarto Jupyter notebook test",
    "section": "testing…",
    "text": "testing…\nwhy isn’t quarto creating a _freeze directory? What stops it?"
  },
  {
    "objectID": "posts/windows.html",
    "href": "posts/windows.html",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC.\n\n\n\ninstall scoop to easily install programs by running this in powershell:\nSet-ExecutionPolicy RemoteSigned -Scope CurrentUser # Optional: Needed to run a remote script the first time\nirm get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable.\n\n\n\n\nFollow the real wsl instructions, my notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nInstall the latest linux kernel update package.\nNow install debian from the windows store.\nAnd presto, you have a VM running inside a linux container, and its easy to have multiple ones setup for different things."
  },
  {
    "objectID": "posts/windows.html#factory-reset",
    "href": "posts/windows.html#factory-reset",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Windows 10 gets slow and crufty over time. So once every few years, its good to start afresh.\nReset by:\n\nStart -&gt; Settings -&gt; Update & Security -&gt; Recovery -&gt; Reset this PC\n\nOR, if windows is pretty borked, restart the surface and hold the shift key down. This should boot into a screen with a: Troubleshoot -&gt; Reset this PC."
  },
  {
    "objectID": "posts/windows.html#setup",
    "href": "posts/windows.html#setup",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "install scoop to easily install programs by running this in powershell:\nSet-ExecutionPolicy RemoteSigned -Scope CurrentUser # Optional: Needed to run a remote script the first time\nirm get.scoop.sh | iex\nscoop bucket add extras\n\nRufus for making bootable disks. etcher is a decent alternative, but not as reliable."
  },
  {
    "objectID": "posts/windows.html#wsl-2",
    "href": "posts/windows.html#wsl-2",
    "title": "Windows 10/11 Setup",
    "section": "",
    "text": "Follow the real wsl instructions, my notes are:\nRun powershell as admin and do:\nActivate wsl by dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart\nEnable “virtual machine platform”, something which should have already been enabled by the command above.\nEnable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform -NoRestart\nRestart the pc now and set wsl2 as the default wsl: wsl --set-default-version 2\nInstall the latest linux kernel update package.\nNow install debian from the windows store.\nAnd presto, you have a VM running inside a linux container, and its easy to have multiple ones setup for different things."
  }
]